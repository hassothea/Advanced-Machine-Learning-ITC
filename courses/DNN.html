<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="DNN_files/libs/clipboard/clipboard.min.js"></script>
<script src="DNN_files/libs/quarto-html/tabby.min.js"></script>
<script src="DNN_files/libs/quarto-html/popper.min.js"></script>
<script src="DNN_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="DNN_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="DNN_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="DNN_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="DNN_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.56">

  <meta name="author" content="Lecturer: Dr.&nbsp;HAS Sothea   Code: AMSI61AML">
  <title>Advanced Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="DNN_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="DNN_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="DNN_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="styles.css">
  <link href="DNN_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="DNN_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="DNN_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="DNN_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
  <script type="text/javascript">
  window.PlotlyConfig = {MathJaxConfig: 'local'};
  if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
  if (typeof require !== 'undefined') {
  require.undef("plotly");
  requirejs.config({
      paths: {
          'plotly': ['https://cdn.plot.ly/plotly-2.34.0.min']
      }
  });
  require(['plotly'], function(Plotly) {
      window._Plotly = Plotly;
  });
  }
  </script>

  <style>
  #title-slide .title {
      font-size: 2em;
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Advanced Machine Learning</h1>
  <p class="subtitle"><br> <a href="https://itc.edu.kh/about-institute-of-technology-of-cambodia/" target="_blank"><img data-src="./img/itc.png" width="125"></a> &nbsp; &nbsp; &nbsp; <a href="https://itc.edu.kh/home-ams/" target="_blank"><img data-src="./img/AMS_logo.jpg" width="200"></a> <br></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Lecturer: Dr.&nbsp;HAS Sothea <br><br> Code: <code>AMSI61AML</code> 
</div>
</div>
</div>

</section>
<section>
<section id="deep-neural-network" class="title-slide slide level1 center" data-background-color="#15797A">
<h1><img data-src="./img/neural.png" style="position: relative; bottom: -10px" width="70"> Deep Neural Network</h1>

</section>
<section id="content" class="slide level2">
<h2><img data-src="./img/neural.png" style="position: relative; bottom: -28px" width="70"> Content</h2>
<ul>
<li><p><span class="section secbr">Introduction &amp; Brief History</span> <br></p></li>
<li><p><span class="section secbr">World of Approximation</span> <br></p></li>
<li><p><span class="section secbr">Neural Networks</span> <br></p></li>
<li><p><span class="section secbr">Optimization</span> <br></p></li>
<li><p><span class="section secbr">Applications</span> <br></p></li>
</ul>
</section>
<section id="introduction" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Introduction</h2>
<blockquote>
<p><a href="https://www.ibm.com/topics/neural-networks" target="_blank">Deep Neural Networks (DNN)</a> or <strong>Multilayer Perceptron (MLP)</strong> is a type of ML model built to simulate the complex decision-making power of the <strong>human brain</strong> 🧠.</p>
</blockquote>
<blockquote>
<p>It is a backbone that powers the recent development of <a href="https://www.ibm.com/topics/artificial-intelligence" target="_blank">Artificial Intelligence (AI)</a> applications in our lives today.</p>
</blockquote>

<img data-src="./img/Venn.jpg" class="quarto-figure quarto-figure-center r-stretch" width="320"></section>
<section id="history" class="slide level2 scrollable" data-background-image="https://www.nobelprize.org/uploads/2024/10/2-2_3-6704d5430e393-464x696.jpg" data-background-opacity="0.05" data-background-size="60%">
<h2>History</h2>
<div style="font-size: 60%">
<table class="hline caption-top">
<caption><span class="blue"><strong>Early Foundations</strong></span></caption>
<colgroup>
<col style="width: 5%">
<col style="width: 95%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><code>Year</code></th>
<th style="text-align: left;"><code>Development</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>1943</strong></td>
<td style="text-align: left;"><a href="https://www.historyofinformation.com/detail.php?entryid=782" target="_blank">Walter Pitts and Warren McCulloch</a> created the first computer model based on neural networks, using “threshold logic” to mimic the thought process.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>1960s</strong></td>
<td style="text-align: left;"><a href="https://aspace.lib.vt.edu/agents/people/185" target="_blank">Henry J. Kelley</a> developed the basics of a continuous backpropagation model, and Stuart Dreyfus simplified it using the chain rule.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table class="table-striped table-hover hline caption-top">
<caption><span class="blue"><strong>Development of Algorithms</strong></span></caption>
<colgroup>
<col style="width: 5%">
<col style="width: 95%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>1965</strong></td>
<td style="text-align: left;"><a href="https://en.wikipedia.org/wiki/Alexey_Ivakhnenko" target="_blank">Alexey Ivakhnenko</a> and Valentin Lapa developed early deep learning algorithms using polynomial activation functions.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>1980s</strong></td>
<td style="text-align: left;"><a href="https://scholar.google.com/citations?user=JicYPdAAAAAJ&amp;hl=en" target="_blank">Geoffrey Hinton</a><sup>1</sup> and colleagues revived neural networks by demonstrating effective training using backpropagation</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table class="table-striped table-hover hline caption-top">
<caption><span class="blue"><strong>AI Winters and Resurgence</strong></span></caption>
<colgroup>
<col style="width: 5%">
<col style="width: 95%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>1970s</strong></td>
<td style="text-align: left;">The first AI winter occurred due to unmet expectations, leading to reduced funding and research.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>1980s</strong></td>
<td style="text-align: left;">Despite the AI winter, research continued, leading to significant advancements in neural networks and deep learning.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table class="table-striped table-hover hline caption-top">
<caption><span class="blue"><strong>Modern Era</strong></span></caption>
<colgroup>
<col style="width: 5%">
<col style="width: 95%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>1990s</strong></td>
<td style="text-align: left;">Development of convolutional neural networks (CNNs) by Yann LeCun and others for image recognition.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>2006</strong></td>
<td style="text-align: left;">Geoffrey Hinton and colleagues introduced deep belief networks, which further advanced deep learning techniques.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>2012</strong></td>
<td style="text-align: left;">AlexNet, a deep convolutional neural network, won the ImageNet competition, showcasing the power of deep learning in computer vision.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>2016</strong></td>
<td style="text-align: left;">AlphaGo by DeepMind defeated a human Go champion, demonstrating the potential of deep learning in complex games.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Present</strong></td>
<td style="text-align: left;">Deep learning continues to evolve, with applications in natural language processing, speech recognition, autonomous vehicles, and more.</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table class="table-striped table-hover hline caption-top">
<caption><span class="blue"><strong>Key Milestones</strong></span></caption>
<colgroup>
<col style="width: 5%">
<col style="width: 95%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><code>Year</code></th>
<th style="text-align: left;"><code>Key Model Development</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>1943</strong></td>
<td style="text-align: left;">Pitts and McCulloch’s neural network model.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>1960s</strong></td>
<td style="text-align: left;">Kelley’s backpropagation model and Dreyfus’s chain rule simplification.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>1980s</strong></td>
<td style="text-align: left;">Hinton’s backpropagation revival &amp; Recurrent Neural Networks (RNNs).</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>1990s</strong></td>
<td style="text-align: left;">LeCun’s Convolutional Neural Networks (CNNs).</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>2006</strong></td>
<td style="text-align: left;">Deep belief networks.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>2012</strong></td>
<td style="text-align: left;">AlexNet’s ImageNet win.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>2016</strong></td>
<td style="text-align: left;">AlphaGo’s victory.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>2017</strong></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention is all you need</a> (key models of ChatGPT)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<aside><ol class="aside-footnotes"><li id="fn1"><p> Novbel Prize in Physics 2024: https://www.nobelprize.org/prizes/physics/2024/hinton/facts/</p></li></ol></aside></section></section>
<section>
<section id="world-of-approximations" class="title-slide slide level1 center" data-background-color="#15797A">
<h1>World of Approximations</h1>

</section>
<section id="approximation" class="slide level2">
<h2>Approximation</h2>
<div style="font-size:75%">
<ul>
<li><code>Approximation</code> is the process of finding a value that is close to the true value of a quantity, but not exactly equal to it. It is often used when an exact value is difficult to obtain or not necessary.</li>
<li>Ex: In 1683, <a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli" target="_blank">Jacob Bernoulli</a> discovered <span class="math inline">\(e=2.718...\)</span> from compound interest.</li>
</ul>
<div class="columns">
<div class="column fragment" style="font-size: 67%">
<p>Suppose I put <span class="math inline">\(\$ 1\)</span> into a saving account:</p>
<table class="caption-top">
<thead>
<tr class="header">
<th style="text-align: center;"><code>Interest Per Year</code></th>
<th style="text-align: center;"><code>N Compound</code></th>
<th style="text-align: center;"><code>Total</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(100\%\)</span></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"><span class="math inline">\(1+1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(100\%\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"><span class="math inline">\((1+1/2)^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(100\%\)</span></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"><span class="math inline">\((1+1/3)^3\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\vdots\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\vdots\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(100\%\)</span></td>
<td style="text-align: center;">n</td>
<td style="text-align: center;"><span class="math inline">\((1+1/n)^n\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>The compounded interest <span class="math inline">\(\to e\)</span> as <span class="math inline">\(n\)</span> becomes very large i.e., <span class="math display">\[\lim_{n\to \infty}\Big(1+\frac{1}{n}\Big)^n=e=2.71828182...\]</span> With <span class="math inline">\(100\%\)</span> interest per year calculated every second, my <span class="math inline">\(\$ 1\)</span> yields nearly <span class="math inline">\(\$ e=\$ 2.71828...\)</span> at the end of the year.</p>
</div><div class="column fragment" style="width:50%;">
<div id="c47f8680" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>                            <div id="625359df-5cf4-4780-967c-e671e66ee0cf" class="plotly-graph-div" style="height:430px; width:350px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("625359df-5cf4-4780-967c-e671e66ee0cf")) {                    Plotly.newPlot(                        "625359df-5cf4-4780-967c-e671e66ee0cf",                        [{"name":"$n=10$","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],"y":[2.0,2.25,2.37037037037037,2.44140625,2.4883199999999994,2.5216263717421135,2.546499697040712,2.565784513950348,2.5811747917131984,2.5937424601000023],"type":"scatter"},{"name":"$n=100$","visible":"legendonly","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],"y":[2.0,2.25,2.37037037037037,2.44140625,2.4883199999999994,2.5216263717421135,2.546499697040712,2.565784513950348,2.5811747917131984,2.5937424601000023,2.6041990118975287,2.613035290224676,2.6206008878857308,2.6271515563008685,2.6328787177279187,2.6379284973666,2.64241437518311,2.6464258210976865,2.650034326640442,2.653297705144422,2.656263213926108,2.658969858537786,2.6614501186387796,2.663731258068599,2.665836331487422,2.6677849665337465,2.6695939778125704,2.6712778534408463,2.6728491439808066,2.6743187758703026,2.6756963059146854,2.676990129378183,2.678207651253779,2.6793554280957674,2.6804392861534603,2.6814644203008586,2.6824354773085255,2.6833566262745787,2.6842316184670922,2.685063838389963,2.6858563475377526,2.686611922032571,2.687333085118294,2.6880221353133043,2.688681170884324,2.689312111189782,2.6899167153502597,2.6904965986289264,2.691053246842418,2.691588029073608,2.6921022089150086,2.692596954437168,2.6930733470476085,2.6935323893818506,2.6939750123475794,2.6944020814263596,2.694814402322114,2.695212726034574,2.6955977534247255,2.6959701393302162,2.696330496282266,2.6966793978678543,2.697017381776468,2.697344952565099,2.6976625841715243,2.6979707222024034,2.6982697860189915,2.698560170641394,2.698842248489487,2.6991163709761854,2.699382869968298,2.6996420591266403,2.699894235137786,2.7001396788468326,2.700378656300911,2.7006114197110898,2.700838208340397,2.7010592493240737,2.701274758428429,2.7014849407533275,2.701689991383461,2.701890095991989,2.7020854314015796,2.7022761661054715,2.702462460752151,2.7026444685967603,2.702822335921502,2.702996202427776,2.7031662016021554,2.703332461058186,2.703495102855847,2.7036542438003983,2.703809995722089,2.703962465738423,2.704111756499579,2.7042579664191284,2.7044011898901603,2.7045415174887277,2.704679036164753,2.7048138294215285],"type":"scatter"},{"name":"$n=500$","visible":"legendonly","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500],"y":[2.0,2.25,2.37037037037037,2.44140625,2.4883199999999994,2.5216263717421135,2.546499697040712,2.565784513950348,2.5811747917131984,2.5937424601000023,2.6041990118975287,2.613035290224676,2.6206008878857308,2.6271515563008685,2.6328787177279187,2.6379284973666,2.64241437518311,2.6464258210976865,2.650034326640442,2.653297705144422,2.656263213926108,2.658969858537786,2.6614501186387796,2.663731258068599,2.665836331487422,2.6677849665337465,2.6695939778125704,2.6712778534408463,2.6728491439808066,2.6743187758703026,2.6756963059146854,2.676990129378183,2.678207651253779,2.6793554280957674,2.6804392861534603,2.6814644203008586,2.6824354773085255,2.6833566262745787,2.6842316184670922,2.685063838389963,2.6858563475377526,2.686611922032571,2.687333085118294,2.6880221353133043,2.688681170884324,2.689312111189782,2.6899167153502597,2.6904965986289264,2.691053246842418,2.691588029073608,2.6921022089150086,2.692596954437168,2.6930733470476085,2.6935323893818506,2.6939750123475794,2.6944020814263596,2.694814402322114,2.695212726034574,2.6955977534247255,2.6959701393302162,2.696330496282266,2.6966793978678543,2.697017381776468,2.697344952565099,2.6976625841715243,2.6979707222024034,2.6982697860189915,2.698560170641394,2.698842248489487,2.6991163709761854,2.699382869968298,2.6996420591266403,2.699894235137786,2.7001396788468326,2.700378656300911,2.7006114197110898,2.700838208340397,2.7010592493240737,2.701274758428429,2.7014849407533275,2.701689991383461,2.701890095991989,2.7020854314015796,2.7022761661054715,2.702462460752151,2.7026444685967603,2.702822335921502,2.702996202427776,2.7031662016021554,2.703332461058186,2.703495102855847,2.7036542438003983,2.703809995722089,2.703962465738423,2.704111756499579,2.7042579664191284,2.7044011898901603,2.7045415174887277,2.704679036164753,2.7048138294215285,2.7049459774851603,2.705075557463504,2.7052026434963605,2.7053273068967987,2.705449616284729,2.705569637712799,2.7056874347851623,2.7058030687704,2.7059165987070446,2.7060280815047544,2.7061375720390206,2.706245123241461,2.706350786184983,2.7064546101647613,2.706556642774879,2.706656929981162,2.7067555161900714,2.706852444314376,2.7069477558354027,2.7070414908622435,2.707133688188016,2.7072243853433786,2.7073136186475804,2.707401423256779,2.7074878332103167,2.7075728814747357,2.7076565999857247,2.7077390196880207,2.7078201705736085,2.7079000817180776,2.7079787813153984,2.7080562967111232,2.708132654433869,2.7082078802257286,2.7082819990713864,2.7083550352254617,2.708427012239114,2.7084979529852773,2.7085678796831387,2.7086368139211445,2.7087047766791463,2.7087717883499662,2.708837868759473,2.7089030371862597,2.7089673123806697,2.7090307125823254,2.7090932555376694,2.709154958516498,2.7092158383277742,2.709275911334851,2.7093351934705265,2.7093937002504522,2.7094514467873294,2.7095084478035383,2.709564717643635,2.709620270286827,2.7096751193579265,2.7097292781389424,2.7097827595795927,2.7098355763078152,2.709887740639328,2.7099392645878924,2.709990159873631,2.710040437932739,2.710090109925527,2.7101391867449083,2.7101876790242274,2.7102355971452643,2.710282951245167,2.7103297512238647,2.710376006751046,2.710421727272701,2.7104669220174693,2.7105116000029934,2.710555770041915,2.7105994407475107,2.7106426205396774,2.7106853176496353,2.7107275401261237,2.7107692958394067,2.7108105924871326,2.7108514375982247,2.71089183853824,2.710931802513102,2.710971336573943,2.7110104476208314,2.711049142406988,2.7110874275426164,2.711125309498924,2.711162794611157,2.711199889082793,2.7112365989886458,2.7112729302782266,2.711308888778786,2.7113444801988122,2.711379710130714,2.711414584053919,2.7114491073377014,2.7114832852438147,2.711517122929317,2.711550625449212,2.7115837977584714,2.711616644715385,2.711649171082937,2.7116813815320095,2.711713280642897,2.7117448729078997,2.7117761627332237,2.711807154441183,2.711837852272195,2.711868260386403,2.7118983828660914,2.7119282237169045,2.7119577868701272,2.711987076184215,2.7120160954462653,2.7120448483743687,2.712073338618013,2.712101569760657,2.7121295453210887,2.7121572687541216,2.712184743453065,2.7122119727504406,2.712238959919452,2.7122657081752832,2.7122922206765927,2.7123185005267625,2.7123445507744126,2.712370374415694,2.712395974394588,2.7124213536042254,2.712446514888293,2.7124714610415417,2.7124961948113246,2.7125207188984595,2.712545035958207,2.7125691486008763,2.7125930593934067,2.712616770859716,2.7126402854819887,2.712663605701229,2.7126867339183964,2.7127096724948347,2.7127324237534554,2.7127549899796515,2.712777373421339,2.7127995762902963,2.7128216007631707,2.7128434489810562,2.7128651230514547,2.7128866250482604,2.71290795701252,2.712929120953159,2.712950118847592,2.7129709526422783,2.7129916242534344,2.7130121355674954,2.713032488441829,2.7130526847051266,2.713072726158125,2.7130926145740597,2.71311235169903,2.7131319392529245,2.7131513789296537,2.7131706723971694,2.713189821299013,2.713208827254072,2.7132276918568965,2.713246416678417,2.713265003266904,2.7132834531470245,2.713301767821834,2.7133199487720017,2.713337997456778,2.7133559153143993,2.713373703762031,2.713391364196705,2.713408897995204,2.713426306514845,2.7134435910936014,2.713460753050332,2.7134777936850667,2.7134947142801233,2.713511516099284,2.7135282003886747,2.7135447683774245,2.7135612212771623,2.7135775602831758,2.7135937865737256,2.7136099013112065,2.713625905642323,2.7136418006975065,2.713657587592458,2.713673267426999,2.7136888412869173,2.7137043102427274,2.7137196753510575,2.7137349376537863,2.713750098179433,2.7137651579427837,2.7137801179447605,2.7137949791737017,2.713809742604503,2.7138244091994714,2.713838979907877,2.7138534556673557,2.71386783740284,2.713882126027457,2.7138963224425243,2.7139104275376127,2.7139244421912836,2.713938367270513,2.7139522036312633,2.7139659521189663,2.7139796135676244,2.7139931888015503,2.714006678634123,2.7140200838688986,2.714033405298759,2.7140466437077153,2.7140597998689064,2.714072874546881,2.7140858684961446,2.7140987824619627,2.71411161718058,2.7141243733794735,2.714137051776686,2.7141496530820253,2.714162177996638,2.7141746272125515,2.714187001414602,2.714199301278247,2.714211527471612,2.7142236806544746,2.714235761478973,2.714247770589491,2.7142597086225337,2.714271576207239,2.7142833739655483,2.7142951025118482,2.714306762453333,2.714318354390311,2.714329878915852,2.7143413366162483,2.714352728071284,2.7143640538535085,2.7143753145296423,2.714386510658958,2.7143976427953054,2.7144087114855213,2.714419717270497,2.7144306606849624,2.7144415422574664,2.714452362511006,2.7144631219624156,2.714473821122653,2.714484460496947,2.7144950405852537,2.714505561881537,2.7145160248746802,2.714526430047828,2.7145367778789686,2.714547068840845,2.7145573034010604,2.7145674820219727,2.714577605160607,2.714587673269921,2.7145976867972696,2.7146076461849074,2.7146175518712523,2.714627404289141,2.7146372038673046,2.7146469510295317,2.714656646195305,2.7146662897794847,2.714675882192835,2.714685423841387,2.7146949151271262,2.7147043564477666,2.7147137481967514,2.714723090763718,2.714732384533623,2.7147416298877274,2.7147508272039547,2.7147599768550004,2.7147690792111256,2.7147781346377267,2.7147871434966917,2.7147961061464856,2.7148050229418796,2.7148138942335738,2.714822720369363,2.714831501692637,2.7148402385443298,2.7148489312610375,2.7148575801766133,2.7148661856211165,2.71487474792138,2.714883267401313,2.7148917443812293,2.7149001791783514,2.71490857210684,2.7149169234773396,2.7149252335982306,2.7149335027739987,2.7149417313067823,2.7149499194952873,2.7149580676354463,2.7149661760206523,2.7149742449408145,2.714982274683702,2.71499026553365,2.7149982177725653,2.7150061316797953,2.7150140075318356,2.715021845602393,2.715029646162556,2.715037409481338,2.7150451358243366,2.715052825455536,2.7150604786357246,2.7150680956235766,2.7150756766748603,2.7150832220438317,2.7150907319817463,2.7150982067370517,2.715105646557045,2.7151130516856132,2.7151204223652594,2.715127758835469,2.7151350613338416,2.715142330096149,2.715149565355294,2.715156767342303,2.7151639362865043,2.7151710724146216,2.7151781759516647,2.7151852471199627,2.7151922861405353,2.7151992932318194,2.7152062686110194,2.715213212492803,2.7152201250898447,2.715227006613376,2.715233857272137,2.715240677273201,2.7152474668225577,2.715254226123019,2.7152609553767113,2.7152676547832946,2.71527432454117,2.715280964846557,2.7152875758939596,2.715294157876404,2.715300710985448,2.715307235410414,2.715313731339038,2.715320198957904,2.7153266384515615,2.7153330500033515,2.715339433794091,2.715345790004418,2.715352118811948,2.715358420394328,2.7153646949262553,2.7153709425815817,2.7153771635330033,2.7153833579509845,2.71538952600502,2.7153956678633064,2.7154017836919397,2.715407873656474,2.7154139379201863,2.7154199766458467,2.7154259899941904,2.7154319781247707,2.7154379411960763,2.715443879365191,2.7154497927875663,2.715455681617785,2.7154615460084237,2.715467386111939,2.7154732020786296,2.715478994057622,2.7154847621974185,2.7154905066445436,2.7154962275449495,2.715501925042938,2.715507599281984,2.7155132504039408,2.7155188785498683,2.7155244838598356,2.7155300664725828,2.71553562652533,2.715541164154643,2.715546679495975,2.7155521726836644,2.7155576438511027,2.715563093129868,2.715568520651728],"type":"scatter"},{"name":"$n=1000$","visible":"legendonly","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],"y":[2.0,2.25,2.37037037037037,2.44140625,2.4883199999999994,2.5216263717421135,2.546499697040712,2.565784513950348,2.5811747917131984,2.5937424601000023,2.6041990118975287,2.613035290224676,2.6206008878857308,2.6271515563008685,2.6328787177279187,2.6379284973666,2.64241437518311,2.6464258210976865,2.650034326640442,2.653297705144422,2.656263213926108,2.658969858537786,2.6614501186387796,2.663731258068599,2.665836331487422,2.6677849665337465,2.6695939778125704,2.6712778534408463,2.6728491439808066,2.6743187758703026,2.6756963059146854,2.676990129378183,2.678207651253779,2.6793554280957674,2.6804392861534603,2.6814644203008586,2.6824354773085255,2.6833566262745787,2.6842316184670922,2.685063838389963,2.6858563475377526,2.686611922032571,2.687333085118294,2.6880221353133043,2.688681170884324,2.689312111189782,2.6899167153502597,2.6904965986289264,2.691053246842418,2.691588029073608,2.6921022089150086,2.692596954437168,2.6930733470476085,2.6935323893818506,2.6939750123475794,2.6944020814263596,2.694814402322114,2.695212726034574,2.6955977534247255,2.6959701393302162,2.696330496282266,2.6966793978678543,2.697017381776468,2.697344952565099,2.6976625841715243,2.6979707222024034,2.6982697860189915,2.698560170641394,2.698842248489487,2.6991163709761854,2.699382869968298,2.6996420591266403,2.699894235137786,2.7001396788468326,2.700378656300911,2.7006114197110898,2.700838208340397,2.7010592493240737,2.701274758428429,2.7014849407533275,2.701689991383461,2.701890095991989,2.7020854314015796,2.7022761661054715,2.702462460752151,2.7026444685967603,2.702822335921502,2.702996202427776,2.7031662016021554,2.703332461058186,2.703495102855847,2.7036542438003983,2.703809995722089,2.703962465738423,2.704111756499579,2.7042579664191284,2.7044011898901603,2.7045415174887277,2.704679036164753,2.7048138294215285,2.7049459774851603,2.705075557463504,2.7052026434963605,2.7053273068967987,2.705449616284729,2.705569637712799,2.7056874347851623,2.7058030687704,2.7059165987070446,2.7060280815047544,2.7061375720390206,2.706245123241461,2.706350786184983,2.7064546101647613,2.706556642774879,2.706656929981162,2.7067555161900714,2.706852444314376,2.7069477558354027,2.7070414908622435,2.707133688188016,2.7072243853433786,2.7073136186475804,2.707401423256779,2.7074878332103167,2.7075728814747357,2.7076565999857247,2.7077390196880207,2.7078201705736085,2.7079000817180776,2.7079787813153984,2.7080562967111232,2.708132654433869,2.7082078802257286,2.7082819990713864,2.7083550352254617,2.708427012239114,2.7084979529852773,2.7085678796831387,2.7086368139211445,2.7087047766791463,2.7087717883499662,2.708837868759473,2.7089030371862597,2.7089673123806697,2.7090307125823254,2.7090932555376694,2.709154958516498,2.7092158383277742,2.709275911334851,2.7093351934705265,2.7093937002504522,2.7094514467873294,2.7095084478035383,2.709564717643635,2.709620270286827,2.7096751193579265,2.7097292781389424,2.7097827595795927,2.7098355763078152,2.709887740639328,2.7099392645878924,2.709990159873631,2.710040437932739,2.710090109925527,2.7101391867449083,2.7101876790242274,2.7102355971452643,2.710282951245167,2.7103297512238647,2.710376006751046,2.710421727272701,2.7104669220174693,2.7105116000029934,2.710555770041915,2.7105994407475107,2.7106426205396774,2.7106853176496353,2.7107275401261237,2.7107692958394067,2.7108105924871326,2.7108514375982247,2.71089183853824,2.710931802513102,2.710971336573943,2.7110104476208314,2.711049142406988,2.7110874275426164,2.711125309498924,2.711162794611157,2.711199889082793,2.7112365989886458,2.7112729302782266,2.711308888778786,2.7113444801988122,2.711379710130714,2.711414584053919,2.7114491073377014,2.7114832852438147,2.711517122929317,2.711550625449212,2.7115837977584714,2.711616644715385,2.711649171082937,2.7116813815320095,2.711713280642897,2.7117448729078997,2.7117761627332237,2.711807154441183,2.711837852272195,2.711868260386403,2.7118983828660914,2.7119282237169045,2.7119577868701272,2.711987076184215,2.7120160954462653,2.7120448483743687,2.712073338618013,2.712101569760657,2.7121295453210887,2.7121572687541216,2.712184743453065,2.7122119727504406,2.712238959919452,2.7122657081752832,2.7122922206765927,2.7123185005267625,2.7123445507744126,2.712370374415694,2.712395974394588,2.7124213536042254,2.712446514888293,2.7124714610415417,2.7124961948113246,2.7125207188984595,2.712545035958207,2.7125691486008763,2.7125930593934067,2.712616770859716,2.7126402854819887,2.712663605701229,2.7126867339183964,2.7127096724948347,2.7127324237534554,2.7127549899796515,2.712777373421339,2.7127995762902963,2.7128216007631707,2.7128434489810562,2.7128651230514547,2.7128866250482604,2.71290795701252,2.712929120953159,2.712950118847592,2.7129709526422783,2.7129916242534344,2.7130121355674954,2.713032488441829,2.7130526847051266,2.713072726158125,2.7130926145740597,2.71311235169903,2.7131319392529245,2.7131513789296537,2.7131706723971694,2.713189821299013,2.713208827254072,2.7132276918568965,2.713246416678417,2.713265003266904,2.7132834531470245,2.713301767821834,2.7133199487720017,2.713337997456778,2.7133559153143993,2.713373703762031,2.713391364196705,2.713408897995204,2.713426306514845,2.7134435910936014,2.713460753050332,2.7134777936850667,2.7134947142801233,2.713511516099284,2.7135282003886747,2.7135447683774245,2.7135612212771623,2.7135775602831758,2.7135937865737256,2.7136099013112065,2.713625905642323,2.7136418006975065,2.713657587592458,2.713673267426999,2.7136888412869173,2.7137043102427274,2.7137196753510575,2.7137349376537863,2.713750098179433,2.7137651579427837,2.7137801179447605,2.7137949791737017,2.713809742604503,2.7138244091994714,2.713838979907877,2.7138534556673557,2.71386783740284,2.713882126027457,2.7138963224425243,2.7139104275376127,2.7139244421912836,2.713938367270513,2.7139522036312633,2.7139659521189663,2.7139796135676244,2.7139931888015503,2.714006678634123,2.7140200838688986,2.714033405298759,2.7140466437077153,2.7140597998689064,2.714072874546881,2.7140858684961446,2.7140987824619627,2.71411161718058,2.7141243733794735,2.714137051776686,2.7141496530820253,2.714162177996638,2.7141746272125515,2.714187001414602,2.714199301278247,2.714211527471612,2.7142236806544746,2.714235761478973,2.714247770589491,2.7142597086225337,2.714271576207239,2.7142833739655483,2.7142951025118482,2.714306762453333,2.714318354390311,2.714329878915852,2.7143413366162483,2.714352728071284,2.7143640538535085,2.7143753145296423,2.714386510658958,2.7143976427953054,2.7144087114855213,2.714419717270497,2.7144306606849624,2.7144415422574664,2.714452362511006,2.7144631219624156,2.714473821122653,2.714484460496947,2.7144950405852537,2.714505561881537,2.7145160248746802,2.714526430047828,2.7145367778789686,2.714547068840845,2.7145573034010604,2.7145674820219727,2.714577605160607,2.714587673269921,2.7145976867972696,2.7146076461849074,2.7146175518712523,2.714627404289141,2.7146372038673046,2.7146469510295317,2.714656646195305,2.7146662897794847,2.714675882192835,2.714685423841387,2.7146949151271262,2.7147043564477666,2.7147137481967514,2.714723090763718,2.714732384533623,2.7147416298877274,2.7147508272039547,2.7147599768550004,2.7147690792111256,2.7147781346377267,2.7147871434966917,2.7147961061464856,2.7148050229418796,2.7148138942335738,2.714822720369363,2.714831501692637,2.7148402385443298,2.7148489312610375,2.7148575801766133,2.7148661856211165,2.71487474792138,2.714883267401313,2.7148917443812293,2.7149001791783514,2.71490857210684,2.7149169234773396,2.7149252335982306,2.7149335027739987,2.7149417313067823,2.7149499194952873,2.7149580676354463,2.7149661760206523,2.7149742449408145,2.714982274683702,2.71499026553365,2.7149982177725653,2.7150061316797953,2.7150140075318356,2.715021845602393,2.715029646162556,2.715037409481338,2.7150451358243366,2.715052825455536,2.7150604786357246,2.7150680956235766,2.7150756766748603,2.7150832220438317,2.7150907319817463,2.7150982067370517,2.715105646557045,2.7151130516856132,2.7151204223652594,2.715127758835469,2.7151350613338416,2.715142330096149,2.715149565355294,2.715156767342303,2.7151639362865043,2.7151710724146216,2.7151781759516647,2.7151852471199627,2.7151922861405353,2.7151992932318194,2.7152062686110194,2.715213212492803,2.7152201250898447,2.715227006613376,2.715233857272137,2.715240677273201,2.7152474668225577,2.715254226123019,2.7152609553767113,2.7152676547832946,2.71527432454117,2.715280964846557,2.7152875758939596,2.715294157876404,2.715300710985448,2.715307235410414,2.715313731339038,2.715320198957904,2.7153266384515615,2.7153330500033515,2.715339433794091,2.715345790004418,2.715352118811948,2.715358420394328,2.7153646949262553,2.7153709425815817,2.7153771635330033,2.7153833579509845,2.71538952600502,2.7153956678633064,2.7154017836919397,2.715407873656474,2.7154139379201863,2.7154199766458467,2.7154259899941904,2.7154319781247707,2.7154379411960763,2.715443879365191,2.7154497927875663,2.715455681617785,2.7154615460084237,2.715467386111939,2.7154732020786296,2.715478994057622,2.7154847621974185,2.7154905066445436,2.7154962275449495,2.715501925042938,2.715507599281984,2.7155132504039408,2.7155188785498683,2.7155244838598356,2.7155300664725828,2.71553562652533,2.715541164154643,2.715546679495975,2.7155521726836644,2.7155576438511027,2.715563093129868,2.715568520651728,2.715573926546316,2.7155793109425796,2.7155846739686083,2.715590015751637,2.7155953364173175,2.7156006360906897,2.715605914895822,2.7156111729556347,2.715616410392217,2.7156216273267257,2.715626823879168,2.7156320001689913,2.715637156314335,2.7156422924324017,2.7156474086400295,2.715652505052303,2.715657581784153,2.7156626389496847,2.7156676766610843,2.7156726950308503,2.715677694170188,2.715682674189167,2.7156876351974146,2.715692577303769,2.7156975006155184,2.7157024052403806,2.715707291283974,2.71571215885193,2.7157170080487543,2.715721838978494,2.7157266517437284,2.7157314464470175,2.7157362231900315,2.7157409820729073,2.715745723196406,2.7157504466591718,2.7157551525597223,2.715759840996112,2.715764512065182,2.715769165863347,2.715773802486334,2.7157784220288326,2.7157830245850496,2.7157876102485305,2.7157921791119057,2.7157967312678695,2.71580126680746,2.7158057858218747,2.7158102884005966,2.715814774633728,2.7158192446099685,2.7158236984174886,2.7158281361440757,2.7158325578762095,2.7158369637003994,2.715841353702458,2.71584572796737,2.7158500865797603,2.7158544296229636,2.7158587571810417,2.7158630693360357,2.715867366170536,2.7158716477654705,2.7158759142022215,2.7158801655609586,2.7158844019216204,2.715888623363138,2.715892829964642,2.7158970218038805,2.7159011989583095,2.7159053615054445,2.715909509521096,2.7159136430820108,2.715917762263301,2.7159218671396017,2.7159259577854833,2.715930034274781,2.7159340966809005,2.7159381450770725,2.715942179534817,2.7159462001268784,2.715950206923837,2.7159541999969496,2.715958179416751,2.715962145252995,2.715966097574904,2.715970036451719,2.715973961952148,2.715977874143404,2.715981773094092,2.715985658870319,2.715989531539558,2.7159933911673635,2.715997237820086,2.7160010715625584,2.716004892460013,2.7160087005767415,2.7160124959768592,2.7160162787235866,2.7160200488806514,2.7160238065104014,2.716027551675484,2.716031284437593,2.7160350048579827,2.7160387129983055,2.7160424089192325,2.7160460926804797,2.7160497643426225,2.7160534239648606,2.7160570716060217,2.716060707325538,2.716064331181079,2.716067943230919,2.716071543532963,2.716075132144098,2.7160787091209815,2.716082274520432,2.7160858283982963,2.716089370810882,2.716092901813006,2.716096421459726,2.716099929806188,2.7161034269063045,2.7161069128140207,2.71611038758343,2.7161138512673446,2.7161173039192246,2.716120745591303,2.7161241763356956,2.7161275962051077,2.7161310052504777,2.71613440352351,2.716137791075012,2.7161411679554717,2.716144534215613,2.716147889905007,2.716151235073711,2.716154569771205,2.7161578940464386,2.7161612079478537,2.7161645115245316,2.7161678048242353,2.7161710878952685,2.7161743607844544,2.7161776235397226,2.7161808762080257,2.7161841188359377,2.716187351469448,2.7161905741556818,2.716193786939522,2.716196989867228,2.716200182983779,2.716203366333711,2.716206539962715,2.716209703914893,2.7162128582339102,2.7162160029645284,2.716219138149772,2.7162222638331976,2.716225380057989,2.7162284868672035,2.7162315843032068,2.716234672408273,2.716237751224769,2.7162408207945137,2.7162438811585115,2.71624693235907,2.7162499744366833,2.716253007432506,2.7162560313867945,2.7162590463401353,2.7162620523326435,2.716265049403942,2.716268037594338,2.7162710169423248,2.7162739874881527,2.7162769492700707,2.7162799023266357,2.7162828466969806,2.716285782419015,2.716288709530871,2.716291628070287,2.716294538074792,2.71629743958228,2.7163003326290984,2.716303217253054,2.7163060934902226,2.716308961377099,2.716311820950669,2.716314672246387,2.7163175153006986,2.71632035014856,2.7163231768259277,2.7163259953680767,2.7163288058099724,2.7163316081866316,2.7163344025326293,2.7163371888824237,2.716339967269827,2.7163427377295566,2.7163455002954144,2.7163482550009608,2.716351001879761,2.7163537409651903,2.7163564722897964,2.7163591958876183,2.716361911790118,2.716364620031023,2.7163673206417904,2.7163700136551503,2.7163726991030916,2.716375377016961,2.7163780474292647,2.716380710370741,2.7163833658729897,2.716386013967229,2.716388654684322,2.7163912880547882,2.716393914109667,2.716396532879207,2.7163991443937,2.7164017486834364,2.716404345777779,2.7164069357072655,2.7164095185009103,2.716412094188281,2.716414662798952,2.716417224361941,2.716419778906001,2.7164223264603464,2.7164248670531665,2.716427400713204,2.716429927468679,2.716432447347966,2.71643496037893,2.7164374665891358,2.716439966007372,2.7164424586598503,2.7164449445752537,2.7164474237800134,2.7164498963011225,2.7164523621667245,2.716454821402219,2.7164572740357356,2.7164597200924216,2.7164621595994576,2.71646459258345,2.7164670190695945,2.7164694390848836,2.7164718526545495,2.7164742598043494,2.7164766605596866,2.71647905494653,2.716481442990054,2.7164838247150827,2.716486200146976,2.7164885693102363,2.7164909322302875,2.7164932889316002,2.7164956394380826,2.716497983774837,2.716500321965789,2.7165026540354265,2.7165049800073175,2.7165072999053743,2.716509613753732,2.7165119215756914,2.716514223394496,2.716516519234565,2.7165188091180954,2.7165210930685855,2.7165233711092673,2.7165256432629117,2.716527909552089,2.71653016999989,2.7165324246287734,2.716534673460799,2.7165369165187903,2.7165391538246833,2.716541385400671,2.716543611268466,2.7165458314504383,2.7165480459679108,2.716550254842754,2.716552458096607,2.7165546557505684,2.7165568478262885,2.716559034344696,2.716561215327079,2.716563390794263,2.7165655607672057,2.7165677252671077,2.7165698843140866,2.7165720379287004,2.7165741861320387,2.7165763289438876,2.716578466384717,2.716580598474785,2.716582725234197,2.716584846682471,2.716586962840025,2.7165890737262828,2.7165911793611484,2.716593279764059,2.7165953749544802,2.716597464951911,2.7165995497753954,2.716601629444,2.716603703977767,2.716605773394774,2.7166078377140206,2.716609896954418,2.7166119511347433,2.716614000273589,2.7166160443897915,2.7166180835013876,2.7166201176270093,2.716622146784335,2.7166241709924615,2.716626190268857,2.716628204631725,2.716630214098914,2.7166322186882965,2.716634218417155,2.7166362133040414,2.7166382033656666,2.7166401886202385,2.7166421690848326,2.7166441447767316,2.7166461157127704,2.7166480819111714,2.716650043387874,2.7166520001603613,2.716653952245474,2.716655899660228,2.716657842421114,2.7166597805449046,2.7166617140479428,2.7166636429474447,2.716665567258754,2.716667486999162,2.7166694021841353,2.716671312831133,2.7166732189548,2.7166751205717943,2.7166770176985033,2.716678910350276,2.716680798542683,2.7166826822922685,2.7166845616142345,2.7166864365241987,2.716688307037398,2.7166901731699338,2.71669203493642,2.7166938923532773,2.7166957454343073,2.7166975941955265,2.716699438652272,2.7167012788188747,2.716703114710393,2.7167049463422814,2.7167067737290105,2.7167085968847577,2.716710415825133,2.716712230564224,2.7167140411163135,2.716715847496556,2.7167176497190413,2.7167194477979826,2.716721241747857,2.716723031583019,2.7167248173167815,2.7167265989641702,2.716728376538894,2.7167301500545133,2.7167319195256097,2.7167336849652592,2.716735446387778,2.7167372038067104,2.716738957235985,2.716740706688418,2.7167424521779213,2.7167441937180006,2.716745931322364,2.7167476650039375,2.7167493947760004,2.7167511206517214,2.716752842644225,2.7167545607666512,2.716756275032214,2.716757985454256,2.716759692044684,2.7167613948166993,2.716763093783769,2.716764788958047,2.71676648035202,2.716768167979083,2.7167698518508625,2.7167715319804797,2.716773208380411,2.716774881063041,2.7167765500406746,2.716778215324999,2.716779876929264,2.7167815348650213,2.716783189144293,2.7167848397801158,2.716786486783253,2.7167881301665524,2.7167897699416454,2.716791406120573,2.7167930387146586,2.7167946677361314,2.7167962931964458,2.7167979151073443,2.716799533480824,2.716801148327988,2.7168027596606574,2.7168043674896656,2.716805971827017,2.7168075726841763,2.716809170072004,2.7168107640018078,2.7168123544852762,2.716813941533301,2.7168155251570147,2.7168171053671624,2.7168186821751363,2.7168202555917813,2.7168218256278673,2.7168233922951233,2.7168249556033444,2.7168265155636613,2.716828072186762,2.716829625483353,2.7168311754646206,2.716832722140433,2.7168342655220523,2.716835805619198,2.716837342442759,2.716838876002977,2.716840406311028,2.716841933376061,2.7168434572090416,2.7168449778197847,2.7168464952190967,2.716848009416932,2.716849520423392,2.716851028248585,2.7168525329024824,2.71685403439477,2.716855532735843,2.716857027935508,2.7168585200034028,2.7168600089499826,2.7168614947840615,2.7168629775161,2.7168644571557365,2.716865933712769,2.716867407195829,2.716868877615672,2.7168703449815332,2.716871809302681,2.716873270588815,2.7168747288492994,2.716876184092974,2.7168776363302913,2.7168790855696305,2.716880531820595,2.716881975092648,2.716883415394329,2.7168848527359706,2.7168862871253987,2.716887718572404,2.7168891470856034,2.716890572674573,2.716891995347279,2.716893415113979,2.716894831982879,2.7168962459628525,2.716897657063218,2.7168990652917344,2.7169004706579183,2.7169018731704457,2.7169032728380977,2.7169046696689287,2.7169060636718045,2.7169074548555696,2.7169088432287984,2.7169102287995464,2.7169116115768883,2.7169129915688766,2.7169143687840753,2.716915743230707,2.716917114916988,2.7169184838514693,2.7169198500421694,2.716921213498111,2.7169225742266474],"type":"scatter"},{"line":{"color":"red","dash":"dash"},"mode":"lines","name":"y=e=2.718281...","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],"y":[2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045,2.718281828459045],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"width":350,"height":430,"title":{"text":"Compound interest converges to e"},"xaxis":{"type":"log"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('625359df-5cf4-4780-967c-e671e66ee0cf');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</div></div>
</div>
</section>
<section id="approximation-1" class="slide level2">
<h2>Approximation</h2>
<h3 id="taylor-expansion">Taylor expansion</h3>
<div style="font-size: 70%">
<p>If <span class="math inline">\(f:\mathbb{R}\to\mathbb{R}\)</span> is infinitely differentiable <span class="math inline">\(f\in C^{\infty}\)</span>, i.e., <span class="math inline">\(f',f'',f''',...\)</span> exist, then</p>
<p><span class="math display">\[\forall x,a\in\mathbb{R}: f(x)=\sum_{n=0}^{\infty}\frac{f^{(n)}(a)}{n!}(x-a)^n.\]</span></p>
<ul>
<li class="fragment">Example:
<ul>
<li class="fragment"><span class="math inline">\(e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\dots\)</span></li>
<li class="fragment"><span class="math inline">\(\sin(x)=x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\dots\)</span></li>
<li class="fragment"><span class="math inline">\(\cos(x)=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\dots\)</span></li>
</ul></li>
<li class="fragment">Why does it matter?</li>
<li class="fragment">In reality, even though interested in <span class="math inline">\(e^x\)</span> we only compute <span class="math inline">\(x,x^2,x^3,...,x^p\)</span> for some large enough degree <span class="math inline">\(p\in\mathbb{N}\)</span>.</li>
</ul>
</div>
</section>
<section id="approximation-2" class="slide level2">
<h2>Approximation</h2>
<h3 id="the-role-of-models">The Role of Models</h3>
<div style="font-size: 80%">
<ul>
<li>As data/ML practitioners, we are interested in the <code>relationship</code> between <strong>input</strong> <span class="math inline">\(X\)</span> and the <strong>target</strong> <span class="math inline">\(y\)</span>, called <span class="math inline">\(\color{red}{f}\)</span>.</li>
<li>Models approximate this relationship <span class="math inline">\(\color{red}{f}\)</span>.</li>
</ul>
<div style="font-style: 80%">
<div class="columns">
<div class="column" style="width:60%;">
<p><span class="math display">\[\underbrace{\begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1d}\\
x_{21} &amp; x_{22} &amp; \dots &amp; x_{2d}\\
x_{31} &amp; x_{32} &amp; \dots &amp; x_{3d}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{nd}\\
\end{bmatrix}}_{\text{Input }X}\xrightarrow[]{\color{red}{f}} \underbrace{\begin{bmatrix}y_1\\
y_2\\
y_3\\
\vdots\\
y_n
\end{bmatrix}}_{\text{target }y}\]</span></p>
</div><div class="column" style="width:40%;">
<div id="68bc7a6d" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="DNN_files/figure-revealjs/cell-3-output-1.png" width="515" height="381"></p>
</figure>
</div>
</div>
</div>
</div></div>
</div>
</div>
</section>
<section id="model-multilayer-perceptron" class="slide level2">
<h2>Model: Multilayer Perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn0.jpg" height="320"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 300px; left: 97px; width: 0px; height: 200px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/brain.jpg" height="320"></p>
</div>
</div></div>
</section>
<section id="model-multilayer-perceptron-1" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="320"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 300px; left: 97px; width: 50px; height: 200px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\text{x}_i\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/brain.jpg" height="320"></p>
</div>
</div></div>
</section>
<section id="model-multilayer-perceptron-2" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="320"></p>
<p><img data-src="./img/big_rec.png" class="absolute" style="top: 240px; left: 149px; width: 180px; height: 305px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-3" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="320"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 310px; left: 336px; width: 50px; height: 180px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
<li><span class="blue"><strong>Output layer</strong></span>: Returns the predictions: <span class="math display">\[\color{blue}{\hat{y}}=\sigma_L(\color{blue}{W_L}\color{green}{z_{L-1}}+\color{blue}{b_L}).\]</span></li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-4" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/loss.jpg" height="320"></p>
<p><img data-src="./img/big_rec.png" class="absolute" style="top: 305px; left: 335px; width: 100px; height: 180px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
<li><span class="blue"><strong>Output layer</strong></span>: Returns the predictions: <span class="math display">\[\color{blue}{\hat{y}}=\sigma_L(\color{blue}{W_L}\color{green}{z_{L-1}}+\color{blue}{b_L}).\]</span></li>
<li><span class="blue"><strong>Loss function</strong></span>: measures the difference between predictions and the real targets.</li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-5" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/ffNN.gif" height="320"></p>
<p><img data-src="./img/rec.png" class="absolute" style="top: 310px; left: 340px; width: 0px; height: 180px; "></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
<li><span class="blue"><strong>Output layer</strong></span>: Returns the predictions: <span class="math display">\[\color{blue}{\hat{y}}=\sigma_L(\color{blue}{W_L}\color{green}{z_{L-1}}+\color{blue}{b_L}).\]</span></li>
<li><span class="blue"><strong>Loss function</strong></span>: measures the difference between predictions and the real targets.</li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-6" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="input-layer-sensory-organs-of-the-network">Input Layer: sensory organs of the network</h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>It plays a role as senses: 👀, 👂, 👃, 👅, 👊 …</li>
<li>The input data are directly fitted into <span class="blue"><strong>input layer</strong></span>.</li>
<li>Let’s use our kaggle <a href="https://www.kaggle.com/datasets/rodolfomendes/abalone-dataset" target="_blank"><code>Abalone</code></a> dataset.</li>
</ul>
<div id="dedad650" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="266">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe caption-top" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">age</th>
<th data-quarto-table-cell-role="th">sex</th>
<th data-quarto-table-cell-role="th">cp</th>
<th data-quarto-table-cell-role="th">trestbps</th>
<th data-quarto-table-cell-role="th">chol</th>
<th data-quarto-table-cell-role="th">fbs</th>
<th data-quarto-table-cell-role="th">restecg</th>
<th data-quarto-table-cell-role="th">thalach</th>
<th data-quarto-table-cell-role="th">exang</th>
<th data-quarto-table-cell-role="th">oldpeak</th>
<th data-quarto-table-cell-role="th">slope</th>
<th data-quarto-table-cell-role="th">ca</th>
<th data-quarto-table-cell-role="th">thal</th>
<th data-quarto-table-cell-role="th">target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">595</td>
<td>61</td>
<td>1</td>
<td>0</td>
<td>148</td>
<td>203</td>
<td>0</td>
<td>1</td>
<td>161</td>
<td>0</td>
<td>0.0</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">145</td>
<td>40</td>
<td>1</td>
<td>0</td>
<td>110</td>
<td>167</td>
<td>0</td>
<td>0</td>
<td>114</td>
<td>1</td>
<td>2.0</td>
<td>1</td>
<td>0</td>
<td>3</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">36</td>
<td>51</td>
<td>1</td>
<td>3</td>
<td>125</td>
<td>213</td>
<td>0</td>
<td>0</td>
<td>125</td>
<td>1</td>
<td>1.4</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">10</td>
<td>71</td>
<td>0</td>
<td>0</td>
<td>112</td>
<td>149</td>
<td>0</td>
<td>1</td>
<td>125</td>
<td>0</td>
<td>1.6</td>
<td>1</td>
<td>0</td>
<td>2</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">914</td>
<td>66</td>
<td>1</td>
<td>0</td>
<td>112</td>
<td>212</td>
<td>0</td>
<td>0</td>
<td>132</td>
<td>1</td>
<td>0.1</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<ul>
<li>Input: <span class="math inline">\(\text{x}_1=\)</span> [52.0, 1.0, 0.0, 125.0, 212.0, 0.0, 1.0, 168.0, 0.0, 1.0, 2.0, 2.0, 3.0].</li>
<li>Target: <span class="math inline">\(y_1=\)</span> 0.</li>
</ul>
<div>
<ul>
<li class="fragment"><span class="light-red"><strong>Q1</strong></span>: What should be done in preprocessing step?
<ul>
<li class="fragment">Remove missing values if there are any.</li>
<li class="fragment">Encode cat. variables: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank"><code>OneHotEncoder</code></a>.</li>
</ul></li>
</ul>
</div>
</div><div class="column fragment" style="width:50%;">
<ul>
<li>Scale input: <a href="https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MinMaxScaler.html" target="_blank"><code>MinMaxScaler</code></a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" target="_blank"><span class="green"><code>StandardScaler</code></span></a>.</li>
</ul>
<div id="0c47081e" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split </span>
<span id="cb1-2"><a href=""></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler </span>
<span id="cb1-3"><a href=""></a>data <span class="op">=</span> data   <span class="co"># drop missing values</span></span>
<span id="cb1-4"><a href=""></a>quan_vars <span class="op">=</span> [<span class="st">'age'</span>,<span class="st">'trestbps'</span>,<span class="st">'chol'</span>,<span class="st">'thalach'</span>,<span class="st">'oldpeak'</span>]</span>
<span id="cb1-5"><a href=""></a>qual_vars <span class="op">=</span> [<span class="st">'sex'</span>,<span class="st">'cp'</span>,<span class="st">'fbs'</span>,<span class="st">'restecg'</span>,<span class="st">'exang'</span>,<span class="st">'slope'</span>,<span class="st">'ca'</span>,<span class="st">'thal'</span>]</span>
<span id="cb1-6"><a href=""></a><span class="cf">for</span> i <span class="kw">in</span> quan_vars:</span>
<span id="cb1-7"><a href=""></a>  data[i] <span class="op">=</span> data[i].astype(<span class="st">'float'</span>)</span>
<span id="cb1-8"><a href=""></a><span class="cf">for</span> i <span class="kw">in</span> qual_vars:</span>
<span id="cb1-9"><a href=""></a>  data[i] <span class="op">=</span> data[i].astype(<span class="st">'category'</span>)</span>
<span id="cb1-10"><a href=""></a>data <span class="op">=</span> pd.get_dummies(data, columns<span class="op">=</span>qual_vars, drop_first<span class="op">=</span><span class="va">True</span>)  <span class="co"># One-hot encoding</span></span>
<span id="cb1-11"><a href=""></a>y <span class="op">=</span> data[<span class="st">'target'</span>]</span>
<span id="cb1-12"><a href=""></a>X <span class="op">=</span> data.drop(<span class="st">'target'</span>, axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb1-13"><a href=""></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>) <span class="co"># Train-test split</span></span>
<span id="cb1-14"><a href=""></a>scaler <span class="op">=</span> StandardScaler() <span class="co"># Scaling inputs</span></span>
<span id="cb1-15"><a href=""></a>X_train <span class="op">=</span> scaler.fit_transform(X_train) </span>
<span id="cb1-16"><a href=""></a>X_test <span class="op">=</span> scaler.transform(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div>
<ul>
<li class="fragment">Input: <span class="math inline">\(\text{x}_1=\)</span> [-0.586, -0.779, -1.935, -1.019, -0.211, 0.655, -0.43, 1.606, -0.293, -0.414, -0.981, -0.122, -0.726, -0.959, 1.095, -0.523, -0.383, 3.625, -0.132, -0.263, 0.934, -0.814].</li>
<li class="fragment">Target: <span class="math inline">\(y_1=\)</span> 0.</li>
<li class="fragment"><span class="light-red"><strong>Q2</strong></span>: What’s the size of the <strong>input layer</strong>?</li>
</ul>
</div>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-7" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="input-layer-sensory-organs-of-the-network-1">Input Layer: sensory organs of the network</h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:55%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="450"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 300px; left: 77px; width: 70px; height: 250px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
</div><div class="column" style="width:45%;">
<ul>
<li>Let’s build an <code>MLP</code> using <a href="https://keras.io/" target="_blank"><code>Keras</code></a>.</li>
<li>We first create <span class="blue"><strong>Input Layer</strong></span> of size <span class="math inline">\(d=9\)</span>.</li>
</ul>
<div id="de76f35e" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy" data-id="quarto-animate-code"><code class="sourceCode python hljs"><span id="cb2-1" class="hljs-ln-code"><a href=""></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error </span>
<span id="cb2-2" class="hljs-ln-code"><a href=""></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential </span>
<span id="cb2-3" class="hljs-ln-code"><a href=""></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Input</span>
<span id="cb2-4" class="hljs-ln-code"><a href=""></a></span>
<span id="cb2-5" class="hljs-ln-code"><a href=""></a><span class="co"># Dimension of the data</span></span>
<span id="cb2-6" class="hljs-ln-code"><a href=""></a>n, d <span class="op">=</span> X_train.shape   <span class="co"># rows &amp; columns</span></span>
<span id="cb2-7" class="hljs-ln-code"><a href=""></a></span>
<span id="cb2-8" class="hljs-ln-code"><a href=""></a><span class="co"># Initiate the MLP model</span></span>
<span id="cb2-9" class="hljs-ln-code"><a href=""></a>model <span class="op">=</span> Sequential()</span>
<span id="cb2-10" class="hljs-ln-code"><a href=""></a></span>
<span id="cb2-11" class="hljs-ln-code"><a href=""></a><span class="co"># Add an input layer</span></span>
<span id="cb2-12" class="hljs-ln-code"><a href=""></a>model.add(Input(shape<span class="op">=</span>(d,)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment" style="font-size: 85%">
<ul>
<li>Given <strong>trainable</strong> weights <span class="math inline">\(\color{blue}{W_1}\)</span> of size <span class="math inline">\(\ell_1\times d\)</span> and bias <span class="math inline">\(\color{blue}{b_1}\in\mathbb{R}^d\)</span>, the input <span class="math inline">\(\color{green}{\text{x}}\in\mathbb{R}^d\)</span> is converted at the <span class="blue"><strong>input layer</strong></span> by <span class="math display">\[\begin{align*}
\color{green}{z_1}&amp;=\sigma_1(\color{blue}{W_1}\color{green}{\text{x}} + \color{blue}{b_1})\\
&amp;=\sigma_1\begin{pmatrix}
\color{blue}{\begin{bmatrix}
w_{11} &amp; w_{12} &amp; \dots &amp; w_{1d}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
w_{\ell_11} &amp; w_{\ell_12} &amp; \dots &amp; w_{\ell_1d}\\
\end{bmatrix}}\color{green}{\begin{bmatrix}
x_1\\
\vdots\\
x_d
\end{bmatrix}}+
\color{blue}{\begin{bmatrix}
b_1\\
\vdots\\
b_{\ell_1}
\end{bmatrix}}
\end{pmatrix}
\end{align*}\]</span></li>
</ul>
</div>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-8" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="hiddenoutput-layer-brain-action">Hidden/output Layer: brain 🧠/Action 🏃🏻‍♂️‍➡️</h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:55%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="450"></p>
<p><img data-src="./img/big_rec.png" class="absolute" style="top: 200px; left: 140px; width: 325px; height: 430px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
</div><div class="column" style="width:45%;">
<ul>
<li>Let’s add two <span class="blue"><strong>hidden layers</strong></span> of sizes <span class="math inline">\(32\)</span> to our existing network.</li>
<li>Then add an <span class="blue"><strong>output layer</strong></span> to make real-valued prediction <span class="math inline">\(\color{blue}{\hat{y}}\)</span> of <code>Rings.</code></li>
</ul>
<div id="81b22c83" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy" data-id="quarto-animate-code"><code class="sourceCode python hljs"><span id="cb3-1" class="hljs-ln-code"><a href=""></a><span class="co"># Add hidden layer of size 32</span></span>
<span id="cb3-2" class="hljs-ln-code"><a href=""></a>model.add(Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb3-3" class="hljs-ln-code"><a href=""></a></span>
<span id="cb3-4" class="hljs-ln-code"><a href=""></a><span class="co"># Add another hidden layer of size 32</span></span>
<span id="cb3-5" class="hljs-ln-code"><a href=""></a>model.add(Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb3-6" class="hljs-ln-code"><a href=""></a></span>
<span id="cb3-7" class="hljs-ln-code"><a href=""></a><span class="co"># Add one last layer (output) of size 1</span></span>
<span id="cb3-8" class="hljs-ln-code"><a href=""></a>model.add(Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment">
<ul>
<li>With <strong>trainable</strong> weights <span class="math inline">\(\color{blue}{W_2, W_3}\)</span> and biases <span class="math inline">\(\color{blue}{b_2,b_3}\)</span>, the feedforward path: <span class="math display">\[\begin{align*}
\color{green}{z_2}&amp;=\sigma_2(\color{blue}{W_2}\color{green}{z_1} + \color{blue}{b_2})\in\mathbb{R}^{32}\\
\color{blue}{\hat{y}}&amp;=\sigma_4(\color{blue}{W_3}\color{green}{z_2} + \color{blue}{b_2})\in\mathbb{R}
\end{align*}\]</span></li>
<li>What is the dimension of each <span class="light-blue"><strong>parameter</strong></span>?</li>
</ul>
</div>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-9" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="activation-functions-sigma.">Activation functions: <span class="math inline">\(\sigma(.)\)</span> ╭╯</h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>In feedforward path, we use matrix <strong>multiplications</strong> (<span class="math inline">\(\color{blue}{W_j}\)</span>’s) and <strong>additions</strong> (<span class="math inline">\(\color{blue}{b_j}\)</span>’s).</li>
<li>These <strong>operations</strong> are <strong>linear</strong>.</li>
<li>Without <strong>non-linear</strong> components, the network is just a <strong>linear regression</strong>.</li>
<li>These non-linear functions are called <span class="light-blue"><strong>activation functions</strong></span>.</li>
<li>It’s an important component that makes the networks powerful!</li>
<li>Types of activation functions <span class="math inline">\(\sigma_j(.)\)</span>:</li>
</ul>
<div class="fragment" style="font-size: 80%">
<p><span class="math display">\[\begin{align*}
\text{Sigmoid}(z)&amp;=1/(1+e^{-z})\text{ for }z\in\mathbb{R}\\
\text{Softmax}(z)&amp;=(e^{z_1},\dots,e^{z_d})/\sum_{k=1}^de^{z_k},\text{ for }z\in\mathbb{R}^d\\
\color{red}{\text{ReLU}(z)}&amp;\color{red}{=\max(0,z)\text{ for }z\in\mathbb{R}}\\
\text{Tanh}(z)&amp;=\tanh(z)\text{ for }z\in\mathbb{R}\\
\text{Leaky ReLU}(z)&amp;=\begin{cases}z,&amp;\mbox{if} z&gt;0\\ \alpha z,&amp;\mbox{if }z\leq 0\end{cases}.
\end{align*}\]</span></p>
</div>
</div><div class="column fragment" style="width:40%;">

</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-10" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="loss-function-true-y-vs-prediction-colorbluehaty">Loss function: true <span class="math inline">\(y\)</span> vs prediction <span class="math inline">\(\color{blue}{\hat{y}}\)</span></h3>
<div style="font-size: 57%">
<div class="columns">
<div class="column" style="width:65%;">
<ul>
<li>Given weights <span class="math inline">\(\color{blue}{W_j}\)</span>’s and biases <span class="math inline">\(\color{blue}{b_j}\)</span>’s of the network, the feedforward network can produce prediction <span class="math inline">\(\hat{y}\)</span>.</li>
<li>To measure how good the network is, we compare the prediction <span class="math inline">\(\color{blue}{\hat{y}}\)</span> to the real target <span class="math inline">\(y\)</span>.</li>
<li><strong>Loss function</strong> quantifies the difference between the predicted output and the actual target.</li>
<li><strong>Regression</strong> losses:
<ul>
<li><span class="math inline">\(\ell_2(y_i,\color{blue}{\hat{y}_i})=(y_i-\color{blue}{\hat{y}_i})^2\)</span>: Squared loss.</li>
<li><span class="math inline">\(\ell_1(y_i,\color{blue}{\hat{y}_i})=|y_i-\color{blue}{\hat{y}_i}|\)</span>: Absolute loss.</li>
<li><span class="math inline">\(\ell_{\text{rel}}(y_i,\color{blue}{\hat{y}_i})=|\frac{y_i-\color{blue}{\hat{y}_i}}{y_i}|\)</span>: Relative loss.</li>
</ul></li>
<li><strong>Classification</strong> losses:
<ul>
<li><span class="math inline">\(\text{CEn}(y_i,\color{blue}{\hat{y}_i})=-\sum_{j=1}^My_{ij}\log(\color{blue}{\hat{y}_{ij}})\)</span>: Cross-Entropy.</li>
<li><span class="math inline">\(\text{Hinge}(y_i,\color{blue}{\hat{y}_i})=\max\{0,1-\sum_{j=1}^My_{ij}\color{blue}{\hat{y}_{ij}}\}\)</span>: Hinge loss.</li>
<li><span class="math inline">\(\text{KL}(y_i,\color{blue}{\hat{y}_i})=\sum{j=1}^My_{ij}\log(y_{ij}/\color{blue}{\hat{y}_{ij}})\)</span>: Kullback-Leibler (KL) Divergence.</li>
</ul></li>
</ul>
</div><div class="column" style="width:35%;">
<ul>
<li class="fragment"><span class="light-red"><strong>Q3</strong></span>: What are the <span class="light-blue">key parameters</span> of the network? <img data-src="./img/answer.jpg"></li>
<li class="fragment"><span class="green"><strong>A3</strong></span>: All weights <span class="math inline">\(\color{blue}{W_j}\)</span>’s and biases <span class="math inline">\(\color{blue}{b_j}\)</span>’s.</li>
<li class="fragment"><span class="light-red"><strong>Q4</strong></span>: How to find the suitable values of these <span class="light-blue">parameters</span>?</li>
<li class="fragment"><span class="green"><strong>A4</strong></span>: <strong>Loss function</strong> can guide the network to its better and better state! In other words, we can use the <span class="light-red"><strong>loss/mistake</strong></span> to adjust all <span class="light-blue">key parameters</span>, leading to a better state of the network.</li>
</ul>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-11" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="feedforward-neural-networks-by-hand">Feedforward Neural Networks By Hand</h3>
<p>👉 <a href="https://colab.research.google.com/drive/1UlLjXnFHUKZbs2jH31eVLe5CuoQd5kXR?usp=sharing" target="_blank">Jupyter notebook: Feedforward NN by hand</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/ffNN.gif" class="quarto-figure quarto-figure-center" height="450"></p>
</figure>
</div>
</section>
<section id="model-multilayer-perceptron-12" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="why-is-it-powerful">Why is it powerful?</h3>
<iframe src="https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/" width="1000" height="450" frameborder="0" marginwidth="0" marginheight="0" style="border: none">
</iframe>
<div class="fragment" style="font-size: 70%">
<ul>
<li><span class="blue">Roughly speaking, it can approximate any <strong>reasonably complex input-output relationship</strong> to any desired <strong>level of precision</strong>! (For more, read <a href="https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/" target="_blank">UAT, Deepmind</a>)</span>.</li>
</ul>
</div>
</section>
<section id="model-multilayer-perceptron-13" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="why-is-it-powerful-1">Why is it powerful?</h3>
<iframe src="https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/" width="1000" height="450" frameborder="0" marginwidth="0" marginheight="0" style="border: none">
</iframe>
<div style="font-size: 70%">
<p>Let’s see what it means: 👉 <a href="https://colab.research.google.com/drive/1oydA7p62mXIfQab5MYgAj2VhHRsPp7Ds?usp=sharing" target="_blank">Jupyter notebook: Universal Approximation Theorem</a>.</p>
</div>
</section>
<section id="backpropagation-gradient-based" class="slide level2">
<h2>Backpropagation: <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/courses/LogisticReg.html#/binary-logistic-regression-14" data-taret="_blank">Gradient-based</a></h2>
<div style="font-size: 70%">
<ul>
<li><a href="https://www.3blue1brown.com/about">Grant Sanderson</a> of <a href="https://www.3blue1brown.com/">3B1B</a> did a really amazing job on this 👇</li>
<li>Source: <a href="https://www.3blue1brown.com/lessons/backpropagation">Backpropagation, 3Blue1Brown</a>.</li>
<li>More readings:
<ul>
<li><a href="https://colah.github.io/posts/2015-08-Backprop/" target="_blank">Graphs: Backpropagation by Colah</a></li>
<li><a href="https://www.cs.columbia.edu/~mcollins/ff2.pdf" target="_blank">Computational Graphs, and Backpropagation by Michael Collins</a>.</li>
</ul></li>
</ul>
</div>
<iframe src="https://www.3blue1brown.com/lessons/backpropagation" width="100%" height="400px" frameborder="0">
</iframe>
</section>
<section id="optimization-in-keras" class="slide level2">
<h2>Optimization in Keras</h2>
<div style="font-size: 75%">
<ul>
<li>We set up optimization method for our existing network as follow:</li>
</ul>
<div id="15619d22" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href=""></a><span class="co"># We use Adam optimizer</span></span>
<span id="cb4-2"><a href=""></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam, SGD</span>
<span id="cb4-3"><a href=""></a></span>
<span id="cb4-4"><a href=""></a><span class="co"># Set up optimizer for our model</span></span>
<span id="cb4-5"><a href=""></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>), loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Let’s have a look at your model:</li>
</ul>
<div id="63b25e66" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href=""></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_22"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense_66 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">736</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_67 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">1,056</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_68 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">33</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,825</span> (7.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,825</span> (7.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</div>
</section>
<section id="training-learning-curves" class="slide level2">
<h2>Training &amp; Learning Curves</h2>
<div style="font-size: 70%">
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>A few important hyperparameters:
<ul>
<li><code>batch_size</code>: number of minibatch <span class="math inline">\(b\)</span>.</li>
<li><code>epochs</code>: number of times that the network passes through the entire training dataset.</li>
<li><code>validation_split</code>: a fraction of the training data for validation during model training. We can keep track of the model state during training by measuring the loss on this validation data, especially for preventing overfitting.</li>
</ul></li>
<li>The network yields Test MSE <span class="math inline">\(=\)</span> 0.937.</li>
<li>This is better than what we could achieve in <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/courses/LogisticReg.html" target="_blank">Logistic Regression</a> (see our <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/TP2_LogisticReg.html" target="_blank">TP2</a>).</li>
<li>Tuning the hyperparameters would push its performance even further.</li>
</ul>
</div><div class="column" style="width:50%;">
<div id="c1832010" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href=""></a><span class="co"># Training the network</span></span>
<span id="cb6-2"><a href=""></a>history <span class="op">=</span> model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">200</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.1</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-3"><a href=""></a></span>
<span id="cb6-4"><a href=""></a><span class="co"># Extract loss values </span></span>
<span id="cb6-5"><a href=""></a>train_loss <span class="op">=</span> history.history[<span class="st">'loss'</span>]</span>
<span id="cb6-6"><a href=""></a>val_loss <span class="op">=</span> history.history[<span class="st">'val_loss'</span>] </span>
<span id="cb6-7"><a href=""></a></span>
<span id="cb6-8"><a href=""></a><span class="co"># Plot the learning curves </span></span>
<span id="cb6-9"><a href=""></a>epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(train_loss) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb6-10"><a href=""></a>fig1 <span class="op">=</span> go.Figure(go.Scatter(x<span class="op">=</span>epochs, y<span class="op">=</span>train_loss, name<span class="op">=</span><span class="st">"Training loss"</span>))</span>
<span id="cb6-11"><a href=""></a>fig1.add_trace(go.Scatter(x<span class="op">=</span>epochs, y<span class="op">=</span>val_loss, name<span class="op">=</span><span class="st">"Training loss"</span>))</span>
<span id="cb6-12"><a href=""></a>fig1.update_layout(title<span class="op">=</span><span class="st">"Training and Validation Loss"</span>, </span>
<span id="cb6-13"><a href=""></a>                   width<span class="op">=</span><span class="dv">510</span>, height<span class="op">=</span><span class="dv">250</span>,</span>
<span id="cb6-14"><a href=""></a>                   xaxis<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Epoch"</span>, <span class="bu">type</span><span class="op">=</span><span class="st">"log"</span>),</span>
<span id="cb6-15"><a href=""></a>                   yaxis<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Loss"</span>))</span>
<span id="cb6-16"><a href=""></a>fig1.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7cbc2375" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div>                            <div id="f363d306-83e8-4894-8621-58dc3736d721" class="plotly-graph-div" style="height:250px; width:510px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("f363d306-83e8-4894-8621-58dc3736d721")) {                    Plotly.newPlot(                        "f363d306-83e8-4894-8621-58dc3736d721",                        [{"name":"Training loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[0.736224353313446,0.6679815053939819,0.6205049157142639,0.5818037390708923,0.5494926571846008,0.5231379866600037,0.4994407892227173,0.4760996103286743,0.4564615488052368,0.438658744096756,0.4219273626804352,0.40782883763313293,0.3948037922382355,0.3839680850505829,0.3736967444419861,0.36462026834487915,0.3569087088108063,0.3493592441082001,0.3426031172275543,0.3360121250152588,0.33024588227272034,0.3247321844100952,0.31940245628356934,0.3146979510784149,0.30978572368621826,0.30586180090904236,0.3014202117919922,0.2971222400665283,0.29351192712783813,0.2899765074253082,0.28614506125450134,0.28249436616897583,0.2791661322116852,0.2759912610054016,0.2728816866874695,0.26991087198257446,0.267147034406662,0.2641313076019287,0.2615174651145935,0.258882999420166,0.2562546133995056,0.2538161873817444,0.25140413641929626,0.24899189174175262,0.2465168982744217,0.244339257478714,0.24183937907218933,0.23951278626918793,0.2375655323266983,0.2351950705051422,0.23291456699371338,0.23105758428573608,0.2285996675491333,0.22665047645568848,0.22503988444805145,0.22297972440719604,0.22072245180606842,0.21872557699680328,0.2170863300561905,0.2151995450258255,0.21341294050216675,0.21162176132202148,0.21005131304264069,0.20837214589118958,0.20673507452011108,0.2050495147705078,0.20362378656864166,0.2020951509475708,0.2006334364414215,0.19953928887844086,0.19796709716320038,0.1973322629928589,0.1958514004945755,0.1941612958908081,0.19280900061130524,0.19151277840137482,0.19000522792339325,0.18916548788547516,0.18762394785881042,0.18629011511802673,0.18497444689273834,0.1837104707956314,0.1824260950088501,0.18112534284591675,0.17990049719810486,0.17878536880016327,0.1774892359972,0.17616774141788483,0.1750747412443161,0.1740351915359497,0.17296360433101654,0.17195996642112732,0.17075923085212708,0.16956546902656555,0.16848403215408325,0.16734947264194489,0.1662134975194931,0.1652260422706604,0.1640426367521286,0.16292016208171844,0.16178882122039795,0.16083094477653503,0.1601678431034088,0.15909092128276825,0.15778721868991852,0.15680794417858124,0.15575675666332245,0.15474076569080353,0.15368613600730896,0.1526782363653183,0.15169815719127655,0.15074026584625244,0.14990174770355225,0.14916092157363892,0.14797750115394592,0.14701908826828003,0.14607389271259308,0.14508499205112457,0.14410367608070374,0.14319181442260742,0.142173171043396,0.1413203328847885,0.14034287631511688,0.13958235085010529,0.13876868784427643,0.13768644630908966,0.13682512938976288,0.13577720522880554,0.13484324514865875,0.13411039113998413,0.13301807641983032,0.13205519318580627,0.1310034990310669,0.13009177148342133,0.12916596233844757,0.12811298668384552,0.12724335491657257,0.12656433880329132,0.12616541981697083,0.12486075609922409,0.12370489537715912,0.12286056578159332,0.12243913859128952,0.12111872434616089,0.1201336681842804,0.119753398001194,0.11858365684747696,0.11760929971933365,0.11668311059474945,0.11589247733354568,0.11508653312921524,0.1144152581691742,0.11341702938079834,0.11258149892091751,0.11180144548416138,0.11178778111934662,0.11066997051239014,0.10944090783596039,0.10863390564918518,0.1076773852109909,0.10687621682882309,0.10608528554439545,0.10541077703237534,0.10467839241027832,0.1040288656949997,0.10321439057588577,0.10278134047985077,0.10211075842380524,0.10131726413965225,0.10052258521318436,0.0997609943151474,0.0990849956870079,0.09830749034881592,0.09790986031293869,0.09711635857820511,0.09656170755624771,0.0957426130771637,0.09561333805322647,0.09458280354738235,0.09391608089208603,0.09319443255662918,0.09246844053268433,0.09182103723287582,0.09118988364934921,0.09043825417757034,0.08984875679016113,0.08939127624034882,0.08879895508289337,0.08801845461130142,0.08736217767000198,0.08689864724874496,0.08617103099822998,0.0856139212846756,0.08499359339475632,0.08512844890356064,0.08420931547880173,0.08345767110586166,0.08284720033407211,0.08233875036239624,0.08168929070234299],"type":"scatter"},{"name":"Validation loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[0.6904744505882263,0.6519880294799805,0.6214662790298462,0.5966789722442627,0.5770989656448364,0.5594030022621155,0.5429432392120361,0.5277062654495239,0.5152453184127808,0.5049222707748413,0.49656230211257935,0.4888279139995575,0.486076295375824,0.47797802090644836,0.472346693277359,0.4682522416114807,0.46401822566986084,0.46074798703193665,0.45795077085494995,0.4551563858985901,0.45212024450302124,0.4510067403316498,0.447465717792511,0.44579753279685974,0.445745050907135,0.4442490339279175,0.4458123743534088,0.4432486593723297,0.4410964250564575,0.4398176670074463,0.4388664960861206,0.43815869092941284,0.4393874406814575,0.43835771083831787,0.43786802887916565,0.44062358140945435,0.4390825927257538,0.4382137060165405,0.4373086094856262,0.43637898564338684,0.4356209933757782,0.43494465947151184,0.4367261230945587,0.43600407242774963,0.43519681692123413,0.43350470066070557,0.43302106857299805,0.4347979426383972,0.43288129568099976,0.4331984221935272,0.43187466263771057,0.4306751489639282,0.4303494989871979,0.4312344491481781,0.4257836639881134,0.425819456577301,0.42601990699768066,0.4267526865005493,0.42668625712394714,0.4269105792045593,0.4306417405605316,0.4319249093532562,0.43209993839263916,0.42893186211586,0.42936772108078003,0.431328147649765,0.4312288165092468,0.4307125508785248,0.43051743507385254,0.4312300384044647,0.4337000846862793,0.43328359723091125,0.4347475469112396,0.43598711490631104,0.4351063668727875,0.43718621134757996,0.4416036307811737,0.44110971689224243,0.43480002880096436,0.43546950817108154,0.43593165278434753,0.43662452697753906,0.43680089712142944,0.4371538460254669,0.4367310106754303,0.436727374792099,0.43683409690856934,0.4370289742946625,0.43713247776031494,0.4373827278614044,0.4333423376083374,0.4343614876270294,0.43523117899894714,0.4359513223171234,0.4366050362586975,0.4361884891986847,0.43864625692367554,0.43783602118492126,0.4383118450641632,0.4386499524116516,0.4421287178993225,0.44468969106674194,0.44451388716697693,0.4432707130908966,0.4427657127380371,0.4429519474506378,0.44349581003189087,0.4435858428478241,0.44442126154899597,0.4436147212982178,0.4443337023258209,0.4447210431098938,0.45094048976898193,0.4490317404270172,0.44823580980300903,0.44765743613243103,0.4474498927593231,0.4471849203109741,0.4466600716114044,0.44668638706207275,0.4472429156303406,0.4475047290325165,0.4480312168598175,0.445148229598999,0.44598257541656494,0.44659173488616943,0.44704949855804443,0.44753238558769226,0.4469441771507263,0.4477960169315338,0.44772952795028687,0.4476509988307953,0.44768086075782776,0.45259085297584534,0.4513891637325287,0.45141926407814026,0.4466882646083832,0.44143685698509216,0.44507473707199097,0.4476085901260376,0.4493585526943207,0.45725637674331665,0.45472562313079834,0.4542319178581238,0.4574359357357025,0.4573676288127899,0.45686545968055725,0.4582013785839081,0.45840761065483093,0.4589911699295044,0.45625007152557373,0.45345208048820496,0.45384931564331055,0.45399418473243713,0.4494260549545288,0.4511915445327759,0.4522179365158081,0.45390209555625916,0.4557015895843506,0.45571011304855347,0.4561489522457123,0.45697394013404846,0.4571091830730438,0.45733627676963806,0.4579383134841919,0.457242488861084,0.4579208791255951,0.4580320417881012,0.45857155323028564,0.4562295973300934,0.45818501710891724,0.4593644440174103,0.4549814462661743,0.45560306310653687,0.4545842111110687,0.45596206188201904,0.4611425995826721,0.4602338671684265,0.46153849363327026,0.4615429639816284,0.4623090624809265,0.46252158284187317,0.4632423222064972,0.46396395564079285,0.46462181210517883,0.46601536870002747,0.46750178933143616,0.4677387475967407,0.46748510003089905,0.4673054814338684,0.4677738547325134,0.46735504269599915,0.46726614236831665,0.47648751735687256,0.4733276963233948,0.4712979793548584,0.46997201442718506,0.46894142031669617,0.46933412551879883,0.4703291952610016],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"title":{"text":"Epoch"},"type":"log"},"title":{"text":"Training and Validation Loss"},"width":510,"height":250,"yaxis":{"title":{"text":"Loss"}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('f363d306-83e8-4894-8621-58dc3736d721');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</div></div>
</div>
</section>
<section id="diagnostics-with-learning-curves" class="slide level2">
<h2>Diagnostics with Learning Curves</h2>
<div style="font-size: 70%">
<div class="columns">
<div class="column" style="width:50%;">
<div id="0bf63c98" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div>                            <div id="1a555e0a-6e1c-4d62-ab28-498a3876261a" class="plotly-graph-div" style="height:250px; width:510px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1a555e0a-6e1c-4d62-ab28-498a3876261a")) {                    Plotly.newPlot(                        "1a555e0a-6e1c-4d62-ab28-498a3876261a",                        [{"name":"Training loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[0.736224353313446,0.6679815053939819,0.6205049157142639,0.5818037390708923,0.5494926571846008,0.5231379866600037,0.4994407892227173,0.4760996103286743,0.4564615488052368,0.438658744096756,0.4219273626804352,0.40782883763313293,0.3948037922382355,0.3839680850505829,0.3736967444419861,0.36462026834487915,0.3569087088108063,0.3493592441082001,0.3426031172275543,0.3360121250152588,0.33024588227272034,0.3247321844100952,0.31940245628356934,0.3146979510784149,0.30978572368621826,0.30586180090904236,0.3014202117919922,0.2971222400665283,0.29351192712783813,0.2899765074253082,0.28614506125450134,0.28249436616897583,0.2791661322116852,0.2759912610054016,0.2728816866874695,0.26991087198257446,0.267147034406662,0.2641313076019287,0.2615174651145935,0.258882999420166,0.2562546133995056,0.2538161873817444,0.25140413641929626,0.24899189174175262,0.2465168982744217,0.244339257478714,0.24183937907218933,0.23951278626918793,0.2375655323266983,0.2351950705051422,0.23291456699371338,0.23105758428573608,0.2285996675491333,0.22665047645568848,0.22503988444805145,0.22297972440719604,0.22072245180606842,0.21872557699680328,0.2170863300561905,0.2151995450258255,0.21341294050216675,0.21162176132202148,0.21005131304264069,0.20837214589118958,0.20673507452011108,0.2050495147705078,0.20362378656864166,0.2020951509475708,0.2006334364414215,0.19953928887844086,0.19796709716320038,0.1973322629928589,0.1958514004945755,0.1941612958908081,0.19280900061130524,0.19151277840137482,0.19000522792339325,0.18916548788547516,0.18762394785881042,0.18629011511802673,0.18497444689273834,0.1837104707956314,0.1824260950088501,0.18112534284591675,0.17990049719810486,0.17878536880016327,0.1774892359972,0.17616774141788483,0.1750747412443161,0.1740351915359497,0.17296360433101654,0.17195996642112732,0.17075923085212708,0.16956546902656555,0.16848403215408325,0.16734947264194489,0.1662134975194931,0.1652260422706604,0.1640426367521286,0.16292016208171844,0.16178882122039795,0.16083094477653503,0.1601678431034088,0.15909092128276825,0.15778721868991852,0.15680794417858124,0.15575675666332245,0.15474076569080353,0.15368613600730896,0.1526782363653183,0.15169815719127655,0.15074026584625244,0.14990174770355225,0.14916092157363892,0.14797750115394592,0.14701908826828003,0.14607389271259308,0.14508499205112457,0.14410367608070374,0.14319181442260742,0.142173171043396,0.1413203328847885,0.14034287631511688,0.13958235085010529,0.13876868784427643,0.13768644630908966,0.13682512938976288,0.13577720522880554,0.13484324514865875,0.13411039113998413,0.13301807641983032,0.13205519318580627,0.1310034990310669,0.13009177148342133,0.12916596233844757,0.12811298668384552,0.12724335491657257,0.12656433880329132,0.12616541981697083,0.12486075609922409,0.12370489537715912,0.12286056578159332,0.12243913859128952,0.12111872434616089,0.1201336681842804,0.119753398001194,0.11858365684747696,0.11760929971933365,0.11668311059474945,0.11589247733354568,0.11508653312921524,0.1144152581691742,0.11341702938079834,0.11258149892091751,0.11180144548416138,0.11178778111934662,0.11066997051239014,0.10944090783596039,0.10863390564918518,0.1076773852109909,0.10687621682882309,0.10608528554439545,0.10541077703237534,0.10467839241027832,0.1040288656949997,0.10321439057588577,0.10278134047985077,0.10211075842380524,0.10131726413965225,0.10052258521318436,0.0997609943151474,0.0990849956870079,0.09830749034881592,0.09790986031293869,0.09711635857820511,0.09656170755624771,0.0957426130771637,0.09561333805322647,0.09458280354738235,0.09391608089208603,0.09319443255662918,0.09246844053268433,0.09182103723287582,0.09118988364934921,0.09043825417757034,0.08984875679016113,0.08939127624034882,0.08879895508289337,0.08801845461130142,0.08736217767000198,0.08689864724874496,0.08617103099822998,0.0856139212846756,0.08499359339475632,0.08512844890356064,0.08420931547880173,0.08345767110586166,0.08284720033407211,0.08233875036239624,0.08168929070234299],"type":"scatter"},{"name":"Validation loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200],"y":[0.6904744505882263,0.6519880294799805,0.6214662790298462,0.5966789722442627,0.5770989656448364,0.5594030022621155,0.5429432392120361,0.5277062654495239,0.5152453184127808,0.5049222707748413,0.49656230211257935,0.4888279139995575,0.486076295375824,0.47797802090644836,0.472346693277359,0.4682522416114807,0.46401822566986084,0.46074798703193665,0.45795077085494995,0.4551563858985901,0.45212024450302124,0.4510067403316498,0.447465717792511,0.44579753279685974,0.445745050907135,0.4442490339279175,0.4458123743534088,0.4432486593723297,0.4410964250564575,0.4398176670074463,0.4388664960861206,0.43815869092941284,0.4393874406814575,0.43835771083831787,0.43786802887916565,0.44062358140945435,0.4390825927257538,0.4382137060165405,0.4373086094856262,0.43637898564338684,0.4356209933757782,0.43494465947151184,0.4367261230945587,0.43600407242774963,0.43519681692123413,0.43350470066070557,0.43302106857299805,0.4347979426383972,0.43288129568099976,0.4331984221935272,0.43187466263771057,0.4306751489639282,0.4303494989871979,0.4312344491481781,0.4257836639881134,0.425819456577301,0.42601990699768066,0.4267526865005493,0.42668625712394714,0.4269105792045593,0.4306417405605316,0.4319249093532562,0.43209993839263916,0.42893186211586,0.42936772108078003,0.431328147649765,0.4312288165092468,0.4307125508785248,0.43051743507385254,0.4312300384044647,0.4337000846862793,0.43328359723091125,0.4347475469112396,0.43598711490631104,0.4351063668727875,0.43718621134757996,0.4416036307811737,0.44110971689224243,0.43480002880096436,0.43546950817108154,0.43593165278434753,0.43662452697753906,0.43680089712142944,0.4371538460254669,0.4367310106754303,0.436727374792099,0.43683409690856934,0.4370289742946625,0.43713247776031494,0.4373827278614044,0.4333423376083374,0.4343614876270294,0.43523117899894714,0.4359513223171234,0.4366050362586975,0.4361884891986847,0.43864625692367554,0.43783602118492126,0.4383118450641632,0.4386499524116516,0.4421287178993225,0.44468969106674194,0.44451388716697693,0.4432707130908966,0.4427657127380371,0.4429519474506378,0.44349581003189087,0.4435858428478241,0.44442126154899597,0.4436147212982178,0.4443337023258209,0.4447210431098938,0.45094048976898193,0.4490317404270172,0.44823580980300903,0.44765743613243103,0.4474498927593231,0.4471849203109741,0.4466600716114044,0.44668638706207275,0.4472429156303406,0.4475047290325165,0.4480312168598175,0.445148229598999,0.44598257541656494,0.44659173488616943,0.44704949855804443,0.44753238558769226,0.4469441771507263,0.4477960169315338,0.44772952795028687,0.4476509988307953,0.44768086075782776,0.45259085297584534,0.4513891637325287,0.45141926407814026,0.4466882646083832,0.44143685698509216,0.44507473707199097,0.4476085901260376,0.4493585526943207,0.45725637674331665,0.45472562313079834,0.4542319178581238,0.4574359357357025,0.4573676288127899,0.45686545968055725,0.4582013785839081,0.45840761065483093,0.4589911699295044,0.45625007152557373,0.45345208048820496,0.45384931564331055,0.45399418473243713,0.4494260549545288,0.4511915445327759,0.4522179365158081,0.45390209555625916,0.4557015895843506,0.45571011304855347,0.4561489522457123,0.45697394013404846,0.4571091830730438,0.45733627676963806,0.4579383134841919,0.457242488861084,0.4579208791255951,0.4580320417881012,0.45857155323028564,0.4562295973300934,0.45818501710891724,0.4593644440174103,0.4549814462661743,0.45560306310653687,0.4545842111110687,0.45596206188201904,0.4611425995826721,0.4602338671684265,0.46153849363327026,0.4615429639816284,0.4623090624809265,0.46252158284187317,0.4632423222064972,0.46396395564079285,0.46462181210517883,0.46601536870002747,0.46750178933143616,0.4677387475967407,0.46748510003089905,0.4673054814338684,0.4677738547325134,0.46735504269599915,0.46726614236831665,0.47648751735687256,0.4733276963233948,0.4712979793548584,0.46997201442718506,0.46894142031669617,0.46933412551879883,0.4703291952610016],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"title":{"text":"Epoch"},"type":"log"},"title":{"text":"Training and Validation Loss"},"width":510,"height":250,"yaxis":{"title":{"text":"Loss"}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('1a555e0a-6e1c-4d62-ab28-498a3876261a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<div style="font-size: 85%">
<ul>
<li>The above learning curve can be used to access the state of our model during and after training.
<ul>
<li>The <span class="light-blue"><strong>training loss</strong></span> always decreases as it’s measured using the training data.</li>
<li>The drop of <span class="light-red"><strong>validation loss</strong></span> indicates the generalization capability of the model at that state.</li>
<li>The model starts to overfit the training data when the validation curve starts to increase.</li>
<li>We should stop the training process when we observe this change in validation curve.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 90%">
<ul>
<li>The learning curves can also reveal other aspects of the network and the data including:
<ul>
<li>When the model underfit the data or requires more training epochs</li>
<li>When the learning rate (<span class="math inline">\(\eta\)</span>) is too large</li>
<li>When the model cannot generalize well to validation set</li>
<li>When it converges properly</li>
<li>When the validation data is not representative enough</li>
<li>When the validation data is too easy too predict…</li>
</ul></li>
<li>These are helpful resources for understanding the above properties:
<ul>
<li><a href="https://wandb.ai/mostafaibrahim17/ml-articles/reports/A-Deep-Dive-Into-Learning-Curves-in-Machine-Learning--Vmlldzo0NjA1ODY0" target="_blank">A deep Dive Into Learning Curves in ML, Mostafa Ibrahim</a></li>
<li><a href="https://rstudio-conf-2020.github.io/dl-keras-tf/notebooks/learning-curve-diagnostics.nb.html#:~:text=Learning%20curves%20can%20also%20be,train%20and%20a%20validation%20dataset." target="_blank">Diagnosing Model Performance with Learning Curves</a>.</li>
</ul></li>
</ul>
</div></div>
</div>
</section>
<section id="neural-network-playground" class="slide level2">
<h2><a href="https://playground.tensorflow.org/" target="_blank">Neural Network Playground</a></h2>
<iframe src="https://playground.tensorflow.org/" width="100%" height="630px" frameborder="0">
</iframe>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<div style="font-size:80%">
<div class="columns">
<div class="column" style="width:50%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pros</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Versatility</strong>: DNNs can be used for a wide range of tasks including classification, regression, and even function approximation.</li>
<li><strong>Non-linear Problem Solving</strong>: They can model complex relationships and capture non-linear patterns in data, thanks to their non-linear activation functions.</li>
<li><strong>Flexibility</strong>: MLPs can have multiple layers and neurons, making them highly adaptable to various problem complexities.</li>
<li><strong>Training Efficiency</strong>: With advancements like backpropagation, training MLPs has become efficient and effective.</li>
<li><strong>Feature Learning</strong>: MLPs can automatically learn features from raw data, reducing the need for manual feature extraction.</li>
</ul>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/neural.png" class="quarto-figure quarto-figure-center" style="position: relative; bottom: 10px" width="100"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Cons</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Computational Complexity</strong>: They can be computationally intensive, especially with large datasets and complex architectures, requiring significant processing power and memory.</li>
<li><strong>Overfitting</strong>: MLPs can easily overfit to training data, especially if they have too many parameters relative to the amount of training data.</li>
<li><strong>Black Box Nature</strong>: The internal workings of an MLP are not easily interpretable, making it difficult to understand how specific decisions are made.</li>
<li><strong>Requires Large Datasets</strong>: Effective training of MLPs often requires large amounts of data, which might not always be available.</li>
<li><strong>Hyperparameter Tuning</strong>: MLPs have several hyperparameters (e.g., learning rate, number of hidden layers, number of neurons per layer) that need careful tuning, which can be time-consuming and challenging.</li>
<li><strong>Architecture</strong>: Designing right architecture can be challenging as well.</li>
</ul>
</div>
</div>
</div>
</div></div>
</div>
</section>
<section id="its-party-time" class="slide level2 center" data-background-image="./img/end_page.jpg" data-background-opacity="0.3">
<h2>🥳 It’s party time 🥂</h2>
<p><br> <br> <br> <br> <br> <br> <br> <br></p>
<h4 id="view-party-menu-here-party-3-menu.">📋 View party menu here: <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/TP3_DNN.html" target="_blank">Party 3 Menu</a>.</h4>
<h4 id="download-party-invitation-here-party-3-invitation-letter.">🫠 Download party invitation here: <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/TP3_DNN.ipynb" target="_blank">Party 3 Invitation Letter</a>.</h4>
</section>
<div class="quarto-auto-generated-content">
<p><img src="./img/AMS_logo.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div></section>

    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="DNN_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="DNN_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="DNN_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="DNN_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="DNN_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="DNN_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="DNN_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="DNN_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="DNN_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="DNN_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>