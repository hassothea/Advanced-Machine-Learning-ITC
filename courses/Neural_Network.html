<!DOCTYPE html>
<html lang="en"><head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="Neural_Network_files/libs/clipboard/clipboard.min.js"></script>
<script src="Neural_Network_files/libs/quarto-html/tabby.min.js"></script>
<script src="Neural_Network_files/libs/quarto-html/popper.min.js"></script>
<script src="Neural_Network_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Neural_Network_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Neural_Network_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="Neural_Network_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="Neural_Network_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.56">

  <meta name="author" content="Lecturer: Dr.&nbsp;HAS Sothea">
  <title> Deep Neural Network</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="Neural_Network_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="Neural_Network_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="Neural_Network_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="styles.css">
  <link href="Neural_Network_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="Neural_Network_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="Neural_Network_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="Neural_Network_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
  <script type="text/javascript">
  window.PlotlyConfig = {MathJaxConfig: 'local'};
  if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
  if (typeof require !== 'undefined') {
  require.undef("plotly");
  requirejs.config({
      paths: {
          'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']
      }
  });
  require(['plotly'], function(Plotly) {
      window._Plotly = Plotly;
  });
  }
  </script>

  <style>
  #title-slide .title {
      font-size: 2em;
  }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><img data-src="./img/neural.png" style="position: relative; bottom: -13px" width="70"> Deep Neural Network</h1>
  <p class="subtitle"><br> <a href="https://itc.edu.kh/about-institute-of-technology-of-cambodia/" target="_blank"><img data-src="./img/itc.png" width="125"></a> &nbsp; &nbsp; &nbsp; <a href="https://itc.edu.kh/home-ams/" target="_blank"><img data-src="./img/AMS_logo.jpg" width="200"></a> <br></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Lecturer: Dr.&nbsp;HAS Sothea 
</div>
</div>
</div>

</section>
<section id="content" class="slide level2">
<h2><img data-src="./img/neural.png" style="position: relative; bottom: -28px" width="70"> Content</h2>
<ul>
<li><p><span class="section secbr">Introduction</span> <br></p></li>
<li><p><span class="section secbr">Multilayer Perceptrons</span> <br></p></li>
<li><p><span class="section secbr">Training and Learning Curves</span> <br></p></li>
<li><p><span class="section secbr">Applications</span> <br></p></li>
</ul>
</section>
<section id="introduction" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Introduction</h2>
<blockquote>
<p><a href="https://www.ibm.com/topics/neural-networks" target="_blank">Deep Neural Networks (DNN)</a> or <strong>Multilayer Perceptron (MLP)</strong> is a type of ML model built to simulate the complex decision-making power of the <strong>human brain</strong> 🧠.</p>
</blockquote>
<blockquote>
<p>It is a backbone that powers the recent development of <a href="https://www.ibm.com/topics/artificial-intelligence" target="_blank">Artificial Intelligence (AI)</a> applications in our lives today.</p>
</blockquote>

<img data-src="./img/Venn.jpg" class="quarto-figure quarto-figure-center r-stretch" width="320"></section>
<section id="model-multilayer-perceptron-mp" class="slide level2">
<h2>Model: Multilayer Perceptron (MP)</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn0.jpg" height="320"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 300px; left: 97px; width: 0px; height: 200px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/brain.jpg" height="320"></p>
</div>
</div></div>
</section>
<section id="model-multilayer-perceptron" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="320"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 300px; left: 97px; width: 50px; height: 200px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\text{x}_i\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/brain.jpg" height="320"></p>
</div>
</div></div>
</section>
<section id="model-multilayer-perceptron-1" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="320"></p>
<p><img data-src="./img/big_rec.png" class="absolute" style="top: 240px; left: 149px; width: 180px; height: 305px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-2" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="320"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 310px; left: 336px; width: 50px; height: 180px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
<li><span class="blue"><strong>Output layer</strong></span>: Returns the predictions: <span class="math display">\[\color{blue}{\hat{y}}=\sigma_L(\color{blue}{W_L}\color{green}{z_{L-1}}+\color{blue}{b_L}).\]</span></li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-3" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/loss.jpg" height="320"></p>
<p><img data-src="./img/big_rec.png" class="absolute" style="top: 305px; left: 335px; width: 100px; height: 180px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
<li><span class="blue"><strong>Output layer</strong></span>: Returns the predictions: <span class="math display">\[\color{blue}{\hat{y}}=\sigma_L(\color{blue}{W_L}\color{green}{z_{L-1}}+\color{blue}{b_L}).\]</span></li>
<li><span class="blue"><strong>Loss function</strong></span>: measures the difference between predictions and the real targets.</li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-4" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<ul>
<li>Deep Neural Networks (DNNs)/Multilayer Perceptrons (MLP) are computational models inspired by the human brain.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<p><img data-src="./img/ffNN.gif" height="320"></p>
<p><img data-src="./img/rec.png" class="absolute" style="top: 310px; left: 340px; width: 0px; height: 180px; "></p>
</div>
<div style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Input layer</strong></span>: vector of individual inputs <span class="math inline">\(\color{green}{\text{x}_i}\in\mathbb{R}^d\)</span>.
<ul>
<li>It takes the inputs from the dataset.</li>
<li>The inputs should be preprocessed: scaled, encoded, transformed, etc, before passing to this layer.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 60%">
<ul>
<li><span class="blue"><strong>Hidden layer</strong></span>: Governed by the equations:<br> <span class="math display">\[\begin{align*}\color{green}{z_0}&amp;=\color{green}{\text{x}}\in\mathbb{R}^d\\
\color{green}{z_k}&amp;=\sigma_k(\color{blue}{W_k}\color{green}{z_{k-1}}+\color{blue}{b_k})\text{ for }k=1,...,L-1.
\end{align*}\]</span> where,
<ul>
<li><span class="math inline">\(\color{blue}{W_k}\)</span> is a matrix of size <span class="math inline">\(\ell_{k}\times\ell_{k-1}\)</span></li>
<li><span class="math inline">\(\color{blue}{b_k}\)</span> is a bias vector of size <span class="math inline">\(\ell_k\)</span></li>
<li><span class="math inline">\(\sigma_k\)</span>: is a point-wise <code>nonlinear activation function</code>.</li>
</ul></li>
<li><span class="blue"><strong>Output layer</strong></span>: Returns the predictions: <span class="math display">\[\color{blue}{\hat{y}}=\sigma_L(\color{blue}{W_L}\color{green}{z_{L-1}}+\color{blue}{b_L}).\]</span></li>
<li><span class="blue"><strong>Loss function</strong></span>: measures the difference between predictions and the real targets.</li>
</ul>
</div></div>
</section>
<section id="model-multilayer-perceptron-5" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="input-layer-sensory-organs-of-the-network">Input Layer: sensory organs of the network</h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>It plays a role as senses: 👀, 👂, 👃, 👅, 👊 …</li>
<li>The input data are directly fitted into <span class="blue"><strong>input layer</strong></span>.</li>
<li>Let’s take a look at <a href="https://www.digitalocean.com/community/tutorials/mnist-dataset-in-python" target="_blank"><code>Mnist</code></a> dataset.</li>
</ul>
<div id="8e27f600" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" data-code-line-numbers="|2,3|1,4-11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href=""></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href=""></a><span class="im">from</span> keras.datasets <span class="im">import</span> mnist</span>
<span id="cb1-3"><a href=""></a>(X_train, y_train), (X_test, y_test) <span class="op">=</span> mnist.load_data()</span>
<span id="cb1-4"><a href=""></a>_, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">2</span>))</span>
<span id="cb1-5"><a href=""></a><span class="bu">print</span>(<span class="ss">f"Train image dimension: </span><span class="sc">{</span>X_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-6"><a href=""></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb1-7"><a href=""></a>    axs[i].imshow(X_train[i,:,:])</span>
<span id="cb1-8"><a href=""></a>    axs[i].set_title(<span class="ss">f"Number </span><span class="sc">{</span>y_test[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-9"><a href=""></a>    axs[i].axis(<span class="st">"off"</span>)</span>
<span id="cb1-10"><a href=""></a>plt.tight_layout()</span>
<span id="cb1-11"><a href=""></a>plt.axis(<span class="st">"off"</span>)</span>
<span id="cb1-12"><a href=""></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train image dimension: (60000, 28, 28)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure>
<p><img data-src="Neural_Network_files/figure-revealjs/cell-3-output-2.png" width="545" height="191"></p>
</figure>
</div>
</div>
</div>
</div><div class="column fragment" style="width:50%;">
<!-- -->
<h3 id="preprocessing">Preprocessing:</h3>
<ul>
<li class="fragment">Scaling: pixel <span class="math inline">\(\in [0,1]\)</span></li>
<li class="fragment">Reshaping: image dim: <span class="math inline">\(28\times 28\to 784\)</span>.</li>
<li class="fragment">Target <strong>one-hot encoding</strong>: <span class="math display">\[y=2\to y_{\text{one-hot}}=[0,0,\color{red}{1},0,0,0,0,0,0,0,0].\]</span></li>
</ul>
<div class="fragment" data-id="code1">
<div id="14e270df" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" data-code-line-numbers="1,2|3,4,5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href=""></a>X_train <span class="op">=</span> X_train.reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)).astype(<span class="st">"float32"</span>)<span class="op">/</span><span class="dv">255</span></span>
<span id="cb3-2"><a href=""></a>X_test <span class="op">=</span> X_test.reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)).astype(<span class="st">"float32"</span>)<span class="op">/</span><span class="dv">255</span></span>
<span id="cb3-3"><a href=""></a><span class="im">from</span> tensorflow.keras.utils <span class="im">import</span> to_categorical</span>
<span id="cb3-4"><a href=""></a>train_labels <span class="op">=</span> to_categorical(y_train)</span>
<span id="cb3-5"><a href=""></a>test_labels <span class="op">=</span> to_categorical(y_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/digit.gif" class="fragment quarto-figure quarto-figure-center" style="width:75.0%"></p>
</figure>
</div>
</div>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-6" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="input-layer-sensory-organs-of-the-network-1">Input Layer: sensory organs of the network</h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:55%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="450"></p>
<p><img data-src="./img/small_rec.png" class="absolute" style="top: 300px; left: 77px; width: 70px; height: 250px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
</div><div class="column" style="width:45%;">
<ul>
<li>Let’s build an <code>MLP</code> using <a href="https://keras.io/" target="_blank"><code>Keras</code></a>.</li>
<li>We first create <span class="blue"><strong>Input Layer</strong></span> of size <span class="math inline">\(d=9\)</span>.</li>
</ul>
<div id="e8019cf0" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy" data-id="quarto-animate-code"><code class="sourceCode python hljs"><span id="cb4-1" class="hljs-ln-code"><a href=""></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error </span>
<span id="cb4-2" class="hljs-ln-code"><a href=""></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential </span>
<span id="cb4-3" class="hljs-ln-code"><a href=""></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Input</span>
<span id="cb4-4" class="hljs-ln-code"><a href=""></a></span>
<span id="cb4-5" class="hljs-ln-code"><a href=""></a><span class="co"># Dimension of the data</span></span>
<span id="cb4-6" class="hljs-ln-code"><a href=""></a>n, d <span class="op">=</span> X_train.shape   <span class="co"># rows &amp; columns</span></span>
<span id="cb4-7" class="hljs-ln-code"><a href=""></a></span>
<span id="cb4-8" class="hljs-ln-code"><a href=""></a><span class="co"># Initiate the MLP model</span></span>
<span id="cb4-9" class="hljs-ln-code"><a href=""></a>model <span class="op">=</span> Sequential()</span>
<span id="cb4-10" class="hljs-ln-code"><a href=""></a><span class="co"># Add an input layer</span></span>
<span id="cb4-11" class="hljs-ln-code"><a href=""></a>model.add(Input(shape<span class="op">=</span>(d,)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment" style="font-size: 85%">
<ul>
<li>Given <strong>trainable</strong> weights <span class="math inline">\(\color{blue}{W_1}\)</span> of size <span class="math inline">\(\ell_1\times d\)</span> and bias <span class="math inline">\(\color{blue}{b_1}\in\mathbb{R}^d\)</span>, the input <span class="math inline">\(\color{green}{\text{x}}\in\mathbb{R}^d\)</span> is converted at the <span class="blue"><strong>input layer</strong></span> by <span class="math display">\[\begin{align*}
\color{green}{z_1}&amp;=\sigma_1(\color{blue}{W_1}\color{green}{\text{x}} + \color{blue}{b_1})\\
&amp;=\sigma_1\begin{pmatrix}
\color{blue}{\begin{bmatrix}
w_{11} &amp; w_{12} &amp; \dots &amp; w_{1d}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
w_{\ell_11} &amp; w_{\ell_12} &amp; \dots &amp; w_{\ell_1d}\\
\end{bmatrix}}\color{green}{\begin{bmatrix}
x_1\\
\vdots\\
x_d
\end{bmatrix}}+
\color{blue}{\begin{bmatrix}
b_1\\
\vdots\\
b_{\ell_1}
\end{bmatrix}}
\end{pmatrix}
\end{align*}\]</span></li>
</ul>
</div>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-7" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="hiddenoutput-layer-brain-action">Hidden/output Layer: brain 🧠/Action 🏃🏻‍♂️‍➡️</h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:55%;">
<div class="r-stack">
<p><img data-src="./img/nn.jpg" height="450"></p>
<p><img data-src="./img/big_rec.png" class="absolute" style="top: 200px; left: 140px; width: 325px; height: 430px; " data-id="box" data-auto-animate-delay="0.01"></p>
</div>
</div><div class="column" style="width:45%;">
<ul>
<li>Let’s add two <span class="blue"><strong>hidden layers</strong></span> of sizes <span class="math inline">\(32\)</span> to our existing network.</li>
<li>Then add an <span class="blue"><strong>output layer</strong></span> to make real-valued prediction <span class="math inline">\(\color{blue}{\hat{y}}\)</span> of <code>Rings.</code></li>
</ul>
<div id="52422001" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy" data-id="quarto-animate-code"><code class="sourceCode python hljs"><span id="cb5-1" class="hljs-ln-code"><a href=""></a><span class="co"># Add hidden layer of size 128</span></span>
<span id="cb5-2" class="hljs-ln-code"><a href=""></a>model.add(Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">"relu"</span>))</span>
<span id="cb5-3" class="hljs-ln-code"><a href=""></a></span>
<span id="cb5-4" class="hljs-ln-code"><a href=""></a><span class="co"># Add another hidden layer of size 128</span></span>
<span id="cb5-5" class="hljs-ln-code"><a href=""></a>model.add(Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">"relu"</span>))</span>
<span id="cb5-6" class="hljs-ln-code"><a href=""></a></span>
<span id="cb5-7" class="hljs-ln-code"><a href=""></a><span class="co"># Add one last layer (output) of size 10</span></span>
<span id="cb5-8" class="hljs-ln-code"><a href=""></a>model.add(Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment">
<ul>
<li>With <strong>trainable</strong> weights <span class="math inline">\(\color{blue}{W_2, W_3}\)</span> and biases <span class="math inline">\(\color{blue}{b_2,b_3}\)</span>, the feedforward path: <span class="math display">\[\begin{align*}
\color{green}{z_2}&amp;=\sigma_2(\color{blue}{W_2}\color{green}{z_1} + \color{blue}{b_2})\in\mathbb{R}^{128}\\
\color{blue}{\hat{y}}&amp;=\sigma_3(\color{blue}{W_3}\color{green}{z_2} + \color{blue}{b_3})\in\mathbb{R}
\end{align*}\]</span></li>
<li>What is the dimension of each <span class="light-blue"><strong>parameter</strong></span>?</li>
</ul>
</div>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-8" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="activation-functions-sigma.">Activation functions: <span class="math inline">\(\sigma(.)\)</span></h3>
<div style="font-size: 60%">
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li class="fragment">In feedforward path, we use matrix <strong>multiplications</strong> (<span class="math inline">\(\color{blue}{W_j}\)</span>’s) and <strong>additions</strong> (<span class="math inline">\(\color{blue}{b_j}\)</span>’s).</li>
<li class="fragment">These <strong>operations</strong> are <strong>linear</strong>.</li>
<li class="fragment">Without <strong>non-linear</strong> components, the network is just a <strong>linear regression</strong>.</li>
<li class="fragment">These non-linear functions are called <span class="light-blue"><strong>activation functions</strong></span>.</li>
<li class="fragment">It’s an important component that makes the networks powerful!</li>
<li class="fragment">Types of activation functions <span class="math inline">\(\sigma_j(.)\)</span>:</li>
</ul>
<div class="fragment" style="font-size: 80%">
<p><span class="math display">\[\begin{align*}
\text{Sigmoid}(z)&amp;=1/(1+e^{-z})\text{ for }z\in\mathbb{R}\\
\text{Softmax}(z)&amp;=(e^{z_1},\dots,e^{z_d})/\sum_{k=1}^de^{z_k},\text{ for }z\in\mathbb{R}^d\\
\color{red}{\text{ReLU}(z)}&amp;\color{red}{=\max(0,z)\text{ for }z\in\mathbb{R}}\\
\text{Tanh}(z)&amp;=\tanh(z)\text{ for }z\in\mathbb{R}\\
\text{Leaky ReLU}(z)&amp;=\begin{cases}z,&amp;\mbox{if} z&gt;0\\ \alpha z,&amp;\mbox{if }z\leq 0\end{cases}.
\end{align*}\]</span></p>
</div>
</div><div class="column fragment r-fit-text" style="width:40%;">
<div id="5d2bf2e0" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>                            <div id="d8758cb9-7c27-4678-827a-054d39836bd3" class="plotly-graph-div" style="height:300px; width:400px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("d8758cb9-7c27-4678-827a-054d39836bd3")) {                    Plotly.newPlot(                        "d8758cb9-7c27-4678-827a-054d39836bd3",                        [{"mode":"lines","name":"Sigmoid","x":[-5.0,-4.974937343358396,-4.949874686716792,-4.924812030075188,-4.899749373433584,-4.87468671679198,-4.849624060150376,-4.824561403508772,-4.799498746867168,-4.774436090225564,-4.74937343358396,-4.724310776942356,-4.6992481203007515,-4.674185463659148,-4.649122807017544,-4.62406015037594,-4.598997493734336,-4.5739348370927315,-4.548872180451128,-4.523809523809524,-4.49874686716792,-4.473684210526316,-4.448621553884712,-4.423558897243108,-4.398496240601504,-4.3734335839598995,-4.348370927318296,-4.323308270676692,-4.298245614035087,-4.273182957393484,-4.2481203007518795,-4.223057644110276,-4.197994987468672,-4.172932330827067,-4.147869674185464,-4.12280701754386,-4.097744360902256,-4.072681704260652,-4.0476190476190474,-4.022556390977444,-3.9974937343358397,-3.9724310776942358,-3.947368421052632,-3.9223057644110275,-3.8972431077694236,-3.8721804511278197,-3.8471177944862154,-3.8220551378446115,-3.7969924812030076,-3.7719298245614037,-3.7468671679198,-3.7218045112781954,-3.6967418546365916,-3.6716791979949877,-3.6466165413533833,-3.6215538847117794,-3.5964912280701755,-3.5714285714285716,-3.5463659147869677,-3.5213032581453634,-3.4962406015037595,-3.4711779448621556,-3.4461152882205512,-3.4210526315789473,-3.3959899749373434,-3.3709273182957395,-3.3458646616541357,-3.3208020050125313,-3.2957393483709274,-3.2706766917293235,-3.245614035087719,-3.2205513784461153,-3.1954887218045114,-3.1704260651629075,-3.1453634085213036,-3.1203007518796992,-3.0952380952380953,-3.0701754385964914,-3.045112781954887,-3.020050125313283,-2.9949874686716793,-2.9699248120300754,-2.9448621553884715,-2.919799498746867,-2.8947368421052633,-2.8696741854636594,-2.844611528822055,-2.819548872180451,-2.7944862155388472,-2.7694235588972433,-2.7443609022556394,-2.719298245614035,-2.694235588972431,-2.6691729323308273,-2.644110275689223,-2.619047619047619,-2.593984962406015,-2.5689223057644113,-2.5438596491228074,-2.518796992481203,-2.493734335839599,-2.4686716791979952,-2.443609022556391,-2.418546365914787,-2.393483709273183,-2.368421052631579,-2.3433583959899753,-2.318295739348371,-2.293233082706767,-2.268170426065163,-2.243107769423559,-2.218045112781955,-2.192982456140351,-2.167919799498747,-2.1428571428571432,-2.117794486215539,-2.092731829573935,-2.067669172932331,-2.0426065162907268,-2.017543859649123,-1.992481203007519,-1.967418546365915,-1.9423558897243112,-1.9172932330827068,-1.892230576441103,-1.867167919799499,-1.8421052631578947,-1.8170426065162908,-1.791979949874687,-1.766917293233083,-1.741854636591479,-1.7167919799498748,-1.6917293233082709,-1.666666666666667,-1.6416040100250626,-1.6165413533834587,-1.5914786967418548,-1.566416040100251,-1.541353383458647,-1.5162907268170427,-1.4912280701754388,-1.466165413533835,-1.4411027568922306,-1.4160401002506267,-1.3909774436090228,-1.3659147869674189,-1.340852130325815,-1.3157894736842106,-1.2907268170426067,-1.2656641604010028,-1.2406015037593985,-1.2155388471177946,-1.1904761904761907,-1.1654135338345868,-1.140350877192983,-1.1152882205513786,-1.0902255639097747,-1.0651629072681708,-1.0401002506265664,-1.0150375939849625,-0.9899749373433586,-0.9649122807017543,-0.9398496240601508,-0.9147869674185465,-0.889724310776943,-0.8646616541353387,-0.8395989974937343,-0.8145363408521309,-0.7894736842105265,-0.7644110275689222,-0.7393483709273188,-0.7142857142857144,-0.6892230576441101,-0.6641604010025066,-0.6390977443609023,-0.6140350877192988,-0.5889724310776945,-0.5639097744360901,-0.5388471177944867,-0.5137844611528823,-0.4887218045112789,-0.46365914786967455,-0.4385964912280702,-0.41353383458646675,-0.3884711779448624,-0.36340852130325807,-0.3383458646616546,-0.3132832080200503,-0.28822055137844593,-0.2631578947368425,-0.23809523809523814,-0.21303258145363468,-0.18796992481203034,-0.162907268170426,-0.13784461152882255,-0.1127819548872182,-0.08771929824561475,-0.06265664160401041,-0.03759398496240607,-0.012531328320802615,0.012531328320801727,0.03759398496240607,0.06265664160400952,0.08771929824561386,0.1127819548872182,0.13784461152882166,0.162907268170426,0.18796992481202945,0.2130325814536338,0.23809523809523814,0.2631578947368416,0.28822055137844593,0.3132832080200494,0.33834586466165373,0.36340852130325807,0.3884711779448615,0.41353383458646586,0.4385964912280702,0.46365914786967366,0.488721804511278,0.5137844611528823,0.5388471177944858,0.5639097744360901,0.5889724310776936,0.6140350877192979,0.6390977443609023,0.6641604010025057,0.6892230576441101,0.7142857142857135,0.7393483709273179,0.7644110275689222,0.7894736842105257,0.81453634085213,0.8395989974937343,0.8646616541353378,0.8897243107769421,0.9147869674185465,0.9398496240601499,0.9649122807017543,0.9899749373433577,1.015037593984962,1.0401002506265664,1.0651629072681699,1.0902255639097742,1.1152882205513777,1.140350877192982,1.1654135338345863,1.1904761904761898,1.2155388471177941,1.2406015037593985,1.265664160401002,1.2907268170426063,1.3157894736842106,1.340852130325814,1.3659147869674184,1.3909774436090219,1.4160401002506262,1.4411027568922306,1.466165413533834,1.4912280701754383,1.5162907268170418,1.5413533834586461,1.5664160401002505,1.591478696741854,1.6165413533834583,1.6416040100250626,1.666666666666666,1.6917293233082704,1.7167919799498748,1.7418546365914782,1.7669172932330826,1.791979949874686,1.8170426065162903,1.8421052631578947,1.8671679197994981,1.8922305764411025,1.917293233082706,1.9423558897243103,1.9674185463659146,1.992481203007518,2.0175438596491224,2.0426065162907268,2.06766917293233,2.0927318295739346,2.117794486215539,2.1428571428571423,2.1679197994987467,2.19298245614035,2.2180451127819545,2.243107769423559,2.2681704260651623,2.2932330827067666,2.31829573934837,2.3433583959899744,2.3684210526315788,2.393483709273182,2.4185463659147866,2.443609022556391,2.4686716791979944,2.4937343358395987,2.518796992481203,2.5438596491228065,2.568922305764411,2.5939849624060143,2.6190476190476186,2.644110275689223,2.6691729323308264,2.6942355889724308,2.719298245614034,2.7443609022556386,2.769423558897243,2.7944862155388464,2.8195488721804507,2.844611528822055,2.8696741854636585,2.894736842105263,2.919799498746867,2.9448621553884706,2.969924812030075,2.9949874686716784,3.0200501253132828,3.045112781954886,3.0701754385964914,3.095238095238095,3.1203007518796984,3.1453634085213036,3.170426065162907,3.1954887218045105,3.220551378446114,3.245614035087719,3.2706766917293226,3.295739348370926,3.3208020050125313,3.3458646616541348,3.370927318295738,3.3959899749373434,3.421052631578947,3.4461152882205504,3.4711779448621556,3.496240601503759,3.5213032581453625,3.5463659147869677,3.571428571428571,3.5964912280701746,3.62155388471178,3.6466165413533833,3.6716791979949868,3.69674185463659,3.7218045112781954,3.746867167919799,3.7719298245614024,3.7969924812030076,3.822055137844611,3.8471177944862145,3.8721804511278197,3.897243107769423,3.9223057644110266,3.947368421052632,3.9724310776942353,3.9974937343358388,4.022556390977442,4.0476190476190474,4.072681704260651,4.097744360902254,4.12280701754386,4.147869674185463,4.1729323308270665,4.197994987468672,4.223057644110275,4.248120300751879,4.273182957393484,4.298245614035087,4.323308270676691,4.348370927318296,4.3734335839598995,4.398496240601503,4.423558897243108,4.448621553884712,4.473684210526315,4.4987468671679185,4.523809523809524,4.548872180451127,4.573934837092731,4.598997493734336,4.624060150375939,4.649122807017543,4.674185463659148,4.6992481203007515,4.724310776942355,4.74937343358396,4.774436090225564,4.799498746867167,4.8245614035087705,4.849624060150376,4.874686716791979,4.899749373433583,4.924812030075188,4.949874686716791,4.974937343358395,5.0],"y":[0.0066928509242848554,0.006861545728776488,0.007034462411545243,0.007211705095960342,0.00739338039499735,0.007579597466754003,0.0077704680709956165,0.007966106626738023,0.008166630270875082,0.008372158917857113,0.00858281532042599,0.008798725131411623,0.009020016966593694,0.009246822468631675,0.009479276372065014,0.009717516569384148,0.009961684178172238,0.010211923609315665,0.010468382636280558,0.010731212465450798,0.011000567807521502,0.011276606949940583,0.011559491830388837,0.011849388111287662,0.012146465255321197,0.012450896601957705,0.012762859444953029,0.013082535110816454,0.013410109038216815,0.013745770858304448,0.01408971447592155,0.014442138151670702,0.014803244584808624,0.01517324099692852,0.015552339216391596,0.0159407557634645,0.016338711936115753,0.016746433896420707,0.017164152757520028,0.017592104671073,0.018030530915142125,0.018479677982440875,0.018939797668871963,0.019411147162277917,0.019893989131320778,0.0203885918144022,0.020895229108529242,0.021414180658025354,0.02194573194297974,0.022490174367321675,0.023047805346399743,0.023618928393938747,0.02420385320823984,0.024802895757481985,0.025416378363974573,0.026044629787203365,0.026687985305503297,0.027346786796182875,0.028021382813916106,0.028712128667208507,0.02941938649273421,0.030143525327331364,0.03088492117743277,0.0316439570856984,0.03242102319460587,0.03321651680674356,0.03403084244154034,0.03486441188815408,0.035717644254229464,0.03659096601022379,0.03748481102898708,0.038399620620270414,0.039335843559824356,0.04029393611273575,0.04127436205063904,0.042277592662425,0.04330410675805652,0.044354390665088485,0.04542893821747485,0.04652825073623367,0.047652837001527316,0.04880321321570217,0.049979902956819304,0.05118343712219517,0.052414353861458464,0.05367319849861814,0.05496052344262457,0.056276888085895996,0.05762285869027072,0.05899900825983564,0.06040591640007241,0.061844169162753794,0.06331435887601455,0.06481708395901527,0.0663529487206104,0.06792256314142832,0.06952654263876744,0.07116550781370984,0.07284008417985444,0.07455090187307208,0.07629859534168806,0.07808380301650299,0.07990716696006857,0.08176933249464481,0.08367094780827695,0.08561266353844257,0.08759513233273755,0.08961900838608775,0.09168494695399544,0.09379360384135549,0.09594563486640341,0.09814169529938997,0.10038243927561277,0.10266851918247229,0.10500058502026478,0.10737928373646931,0.1098052585333371,0.11227914814864749,0.11480158610955099,0.11737319995948577,0.11999461045822074,0.1226664307551492,0.12538926553603585,0.1281637101434991,0.13099034967159726,0.13386975803497725,0.13680249701313807,0.139789115270461,0.14283014735276175,0.14592611266122563,0.14907751440469943,0.1522848385314278,0.15554855264143996,0.15886910488091513,0.16224692281997802,0.16568241231550365,0.16917595636063992,0.17272791392288578,0.17633861877269605,0.18000837830471575,0.18373747235387927,0.18752615200874237,0.1913746384245444,0.19528312163862888,0.19925175939097486,0.20328067595271593,0.20736996096564048,0.21151966829578214,0.21572981490431492,0.22000037973907013,0.22433130265008283,0.22872248333266088,0.23317378030154554,0.23768500989979338,0.24225594534606423,0.2468863158240396,0.25157580561772247,0.2563240532963824,0.2611306509529074,0.26599514349930614,0.2709170280230717,0.2758957532080624,0.2809307188234902,0.28602127528452015,0.29116672328787274,0.29636631352570625,0.30161924648090166,0.3069246723067189,0.31228169079360685,0.3176893514257478,0.32314665352970073,0.32865254651727,0.3342059302244654,0.3398056553481556,0.3454505239817236,0.351139290250728,0.3568706610492651,0.362643296877386,0.368455812779592,0.37430677938407764,0.38019472404202725,0.38611813206591883,0.3920754480654063,0.3980650773789908,0.40408538759931706,0.41013471018955705,0.4162113421879859,0.42231354799748994,0.4284395612563948,0.43458758678666687,0.44075580261520725,0.4469423620636446,0.4531453959017431,0.4593630145592515,0.4655933103907736,0.471834359988,0.47808422653342164,0.4843409621894782,0.4906026105169095,0.4968672089159678,0.503132791084032,0.5093973894830904,0.5156590378105215,0.521915773466578,0.5281656400120001,0.5344066896092261,0.5406369854407486,0.5468546040982567,0.5530576379363552,0.5592441973847928,0.5654124132133329,0.5715604387436052,0.5776864520025098,0.5837886578120138,0.589865289810443,0.5959146124006828,0.601934922621009,0.6079245519345937,0.613881867934081,0.6198052759579725,0.6256932206159224,0.6315441872204078,0.637356703122614,0.6431293389507347,0.6488607097492718,0.6545494760182764,0.6601943446518442,0.6657940697755346,0.6713474534827298,0.676853346470299,0.6823106485742523,0.6877183092063929,0.693075327693281,0.6983807535190983,0.7036336864742935,0.7088332767121271,0.7139787247154799,0.7190692811765097,0.7241042467919375,0.7290829719769281,0.7340048565006938,0.7388693490470926,0.7436759467036175,0.7484241943822775,0.7531136841759603,0.7577440546539357,0.7623149901002064,0.7668262196984542,0.7712775166673391,0.7756686973499172,0.7799996202609296,0.784270185095685,0.7884803317042179,0.7926300390343595,0.796719324047284,0.8007482406090249,0.8047168783613711,0.8086253615754556,0.8124738479912575,0.8162625276461206,0.8199916216952841,0.8236613812273038,0.8272720860771141,0.8308240436393599,0.8343175876844964,0.837753077180022,0.8411308951190848,0.8444514473585599,0.8477151614685722,0.8509224855953005,0.8540738873387743,0.8571698526472381,0.860210884729539,0.8631975029868619,0.8661302419650226,0.8690096503284028,0.8718362898565009,0.874610734463964,0.8773335692448508,0.8800053895417792,0.8826268000405142,0.8851984138904491,0.8877208518513525,0.8901947414666628,0.8926207162635308,0.8949994149797352,0.8973314808175277,0.8996175607243871,0.9018583047006099,0.9040543651335967,0.9062063961586444,0.9083150530460046,0.9103809916139122,0.9124048676672625,0.9143873364615573,0.9163290521917231,0.9182306675053552,0.9200928330399314,0.921916196983497,0.9237014046583119,0.9254490981269279,0.9271599158201455,0.9288344921862901,0.9304734573612324,0.9320774368585716,0.9336470512793896,0.9351829160409846,0.9366856411239853,0.9381558308372462,0.9395940835999276,0.9410009917401643,0.9423771413097293,0.9437231119141041,0.9450394765573754,0.9463268015013818,0.9475856461385416,0.9488165628778047,0.9500200970431806,0.9511967867842978,0.9523471629984727,0.9534717492637662,0.9545710617825252,0.9556456093349116,0.9566958932419435,0.957722407337575,0.958725637949361,0.9597060638872643,0.9606641564401756,0.9616003793797296,0.962515188971013,0.9634090339897762,0.9642823557457705,0.9651355881118459,0.9659691575584597,0.9667834831932564,0.9675789768053942,0.9683560429143017,0.9691150788225672,0.9698564746726686,0.9705806135072657,0.9712878713327914,0.9719786171860839,0.9726532132038171,0.9733120146944967,0.9739553702127968,0.9745836216360254,0.9751971042425179,0.9757961467917601,0.9763810716060612,0.9769521946536002,0.9775098256326783,0.9780542680570202,0.9785858193419747,0.9791047708914709,0.9796114081855978,0.9801060108686791,0.980588852837722,0.981060202331128,0.981520322017559,0.9819694690848578,0.9824078953289269,0.9828358472424801,0.9832535661035792,0.9836612880638843,0.9840592442365355,0.9844476607836083,0.9848267590030715,0.9851967554151914,0.9855578618483293,0.9859102855240786,0.9862542291416956,0.9865898909617833,0.9869174648891834,0.987237140555047,0.9875491033980424,0.9878535347446787,0.9881506118887122,0.988440508169611,0.9887233930500593,0.9889994321924785,0.9892687875345492,0.9895316173637195,0.9897880763906843,0.9900383158218277,0.9902824834306158,0.9905207236279351,0.9907531775313684,0.9909799830334063,0.9912012748685884,0.991417184679574,0.9916278410821427,0.991833369729125,0.9920338933732619,0.9922295319290044,0.992420402533246,0.9926066196050027,0.9927882949040396,0.9929655375884547,0.9931384542712236,0.9933071490757153],"type":"scatter"},{"mode":"lines","name":"ReLU","x":[-5.0,-4.974937343358396,-4.949874686716792,-4.924812030075188,-4.899749373433584,-4.87468671679198,-4.849624060150376,-4.824561403508772,-4.799498746867168,-4.774436090225564,-4.74937343358396,-4.724310776942356,-4.6992481203007515,-4.674185463659148,-4.649122807017544,-4.62406015037594,-4.598997493734336,-4.5739348370927315,-4.548872180451128,-4.523809523809524,-4.49874686716792,-4.473684210526316,-4.448621553884712,-4.423558897243108,-4.398496240601504,-4.3734335839598995,-4.348370927318296,-4.323308270676692,-4.298245614035087,-4.273182957393484,-4.2481203007518795,-4.223057644110276,-4.197994987468672,-4.172932330827067,-4.147869674185464,-4.12280701754386,-4.097744360902256,-4.072681704260652,-4.0476190476190474,-4.022556390977444,-3.9974937343358397,-3.9724310776942358,-3.947368421052632,-3.9223057644110275,-3.8972431077694236,-3.8721804511278197,-3.8471177944862154,-3.8220551378446115,-3.7969924812030076,-3.7719298245614037,-3.7468671679198,-3.7218045112781954,-3.6967418546365916,-3.6716791979949877,-3.6466165413533833,-3.6215538847117794,-3.5964912280701755,-3.5714285714285716,-3.5463659147869677,-3.5213032581453634,-3.4962406015037595,-3.4711779448621556,-3.4461152882205512,-3.4210526315789473,-3.3959899749373434,-3.3709273182957395,-3.3458646616541357,-3.3208020050125313,-3.2957393483709274,-3.2706766917293235,-3.245614035087719,-3.2205513784461153,-3.1954887218045114,-3.1704260651629075,-3.1453634085213036,-3.1203007518796992,-3.0952380952380953,-3.0701754385964914,-3.045112781954887,-3.020050125313283,-2.9949874686716793,-2.9699248120300754,-2.9448621553884715,-2.919799498746867,-2.8947368421052633,-2.8696741854636594,-2.844611528822055,-2.819548872180451,-2.7944862155388472,-2.7694235588972433,-2.7443609022556394,-2.719298245614035,-2.694235588972431,-2.6691729323308273,-2.644110275689223,-2.619047619047619,-2.593984962406015,-2.5689223057644113,-2.5438596491228074,-2.518796992481203,-2.493734335839599,-2.4686716791979952,-2.443609022556391,-2.418546365914787,-2.393483709273183,-2.368421052631579,-2.3433583959899753,-2.318295739348371,-2.293233082706767,-2.268170426065163,-2.243107769423559,-2.218045112781955,-2.192982456140351,-2.167919799498747,-2.1428571428571432,-2.117794486215539,-2.092731829573935,-2.067669172932331,-2.0426065162907268,-2.017543859649123,-1.992481203007519,-1.967418546365915,-1.9423558897243112,-1.9172932330827068,-1.892230576441103,-1.867167919799499,-1.8421052631578947,-1.8170426065162908,-1.791979949874687,-1.766917293233083,-1.741854636591479,-1.7167919799498748,-1.6917293233082709,-1.666666666666667,-1.6416040100250626,-1.6165413533834587,-1.5914786967418548,-1.566416040100251,-1.541353383458647,-1.5162907268170427,-1.4912280701754388,-1.466165413533835,-1.4411027568922306,-1.4160401002506267,-1.3909774436090228,-1.3659147869674189,-1.340852130325815,-1.3157894736842106,-1.2907268170426067,-1.2656641604010028,-1.2406015037593985,-1.2155388471177946,-1.1904761904761907,-1.1654135338345868,-1.140350877192983,-1.1152882205513786,-1.0902255639097747,-1.0651629072681708,-1.0401002506265664,-1.0150375939849625,-0.9899749373433586,-0.9649122807017543,-0.9398496240601508,-0.9147869674185465,-0.889724310776943,-0.8646616541353387,-0.8395989974937343,-0.8145363408521309,-0.7894736842105265,-0.7644110275689222,-0.7393483709273188,-0.7142857142857144,-0.6892230576441101,-0.6641604010025066,-0.6390977443609023,-0.6140350877192988,-0.5889724310776945,-0.5639097744360901,-0.5388471177944867,-0.5137844611528823,-0.4887218045112789,-0.46365914786967455,-0.4385964912280702,-0.41353383458646675,-0.3884711779448624,-0.36340852130325807,-0.3383458646616546,-0.3132832080200503,-0.28822055137844593,-0.2631578947368425,-0.23809523809523814,-0.21303258145363468,-0.18796992481203034,-0.162907268170426,-0.13784461152882255,-0.1127819548872182,-0.08771929824561475,-0.06265664160401041,-0.03759398496240607,-0.012531328320802615,0.012531328320801727,0.03759398496240607,0.06265664160400952,0.08771929824561386,0.1127819548872182,0.13784461152882166,0.162907268170426,0.18796992481202945,0.2130325814536338,0.23809523809523814,0.2631578947368416,0.28822055137844593,0.3132832080200494,0.33834586466165373,0.36340852130325807,0.3884711779448615,0.41353383458646586,0.4385964912280702,0.46365914786967366,0.488721804511278,0.5137844611528823,0.5388471177944858,0.5639097744360901,0.5889724310776936,0.6140350877192979,0.6390977443609023,0.6641604010025057,0.6892230576441101,0.7142857142857135,0.7393483709273179,0.7644110275689222,0.7894736842105257,0.81453634085213,0.8395989974937343,0.8646616541353378,0.8897243107769421,0.9147869674185465,0.9398496240601499,0.9649122807017543,0.9899749373433577,1.015037593984962,1.0401002506265664,1.0651629072681699,1.0902255639097742,1.1152882205513777,1.140350877192982,1.1654135338345863,1.1904761904761898,1.2155388471177941,1.2406015037593985,1.265664160401002,1.2907268170426063,1.3157894736842106,1.340852130325814,1.3659147869674184,1.3909774436090219,1.4160401002506262,1.4411027568922306,1.466165413533834,1.4912280701754383,1.5162907268170418,1.5413533834586461,1.5664160401002505,1.591478696741854,1.6165413533834583,1.6416040100250626,1.666666666666666,1.6917293233082704,1.7167919799498748,1.7418546365914782,1.7669172932330826,1.791979949874686,1.8170426065162903,1.8421052631578947,1.8671679197994981,1.8922305764411025,1.917293233082706,1.9423558897243103,1.9674185463659146,1.992481203007518,2.0175438596491224,2.0426065162907268,2.06766917293233,2.0927318295739346,2.117794486215539,2.1428571428571423,2.1679197994987467,2.19298245614035,2.2180451127819545,2.243107769423559,2.2681704260651623,2.2932330827067666,2.31829573934837,2.3433583959899744,2.3684210526315788,2.393483709273182,2.4185463659147866,2.443609022556391,2.4686716791979944,2.4937343358395987,2.518796992481203,2.5438596491228065,2.568922305764411,2.5939849624060143,2.6190476190476186,2.644110275689223,2.6691729323308264,2.6942355889724308,2.719298245614034,2.7443609022556386,2.769423558897243,2.7944862155388464,2.8195488721804507,2.844611528822055,2.8696741854636585,2.894736842105263,2.919799498746867,2.9448621553884706,2.969924812030075,2.9949874686716784,3.0200501253132828,3.045112781954886,3.0701754385964914,3.095238095238095,3.1203007518796984,3.1453634085213036,3.170426065162907,3.1954887218045105,3.220551378446114,3.245614035087719,3.2706766917293226,3.295739348370926,3.3208020050125313,3.3458646616541348,3.370927318295738,3.3959899749373434,3.421052631578947,3.4461152882205504,3.4711779448621556,3.496240601503759,3.5213032581453625,3.5463659147869677,3.571428571428571,3.5964912280701746,3.62155388471178,3.6466165413533833,3.6716791979949868,3.69674185463659,3.7218045112781954,3.746867167919799,3.7719298245614024,3.7969924812030076,3.822055137844611,3.8471177944862145,3.8721804511278197,3.897243107769423,3.9223057644110266,3.947368421052632,3.9724310776942353,3.9974937343358388,4.022556390977442,4.0476190476190474,4.072681704260651,4.097744360902254,4.12280701754386,4.147869674185463,4.1729323308270665,4.197994987468672,4.223057644110275,4.248120300751879,4.273182957393484,4.298245614035087,4.323308270676691,4.348370927318296,4.3734335839598995,4.398496240601503,4.423558897243108,4.448621553884712,4.473684210526315,4.4987468671679185,4.523809523809524,4.548872180451127,4.573934837092731,4.598997493734336,4.624060150375939,4.649122807017543,4.674185463659148,4.6992481203007515,4.724310776942355,4.74937343358396,4.774436090225564,4.799498746867167,4.8245614035087705,4.849624060150376,4.874686716791979,4.899749373433583,4.924812030075188,4.949874686716791,4.974937343358395,5.0],"y":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.012531328320801727,0.03759398496240607,0.06265664160400952,0.08771929824561386,0.1127819548872182,0.13784461152882166,0.162907268170426,0.18796992481202945,0.2130325814536338,0.23809523809523814,0.2631578947368416,0.28822055137844593,0.3132832080200494,0.33834586466165373,0.36340852130325807,0.3884711779448615,0.41353383458646586,0.4385964912280702,0.46365914786967366,0.488721804511278,0.5137844611528823,0.5388471177944858,0.5639097744360901,0.5889724310776936,0.6140350877192979,0.6390977443609023,0.6641604010025057,0.6892230576441101,0.7142857142857135,0.7393483709273179,0.7644110275689222,0.7894736842105257,0.81453634085213,0.8395989974937343,0.8646616541353378,0.8897243107769421,0.9147869674185465,0.9398496240601499,0.9649122807017543,0.9899749373433577,1.015037593984962,1.0401002506265664,1.0651629072681699,1.0902255639097742,1.1152882205513777,1.140350877192982,1.1654135338345863,1.1904761904761898,1.2155388471177941,1.2406015037593985,1.265664160401002,1.2907268170426063,1.3157894736842106,1.340852130325814,1.3659147869674184,1.3909774436090219,1.4160401002506262,1.4411027568922306,1.466165413533834,1.4912280701754383,1.5162907268170418,1.5413533834586461,1.5664160401002505,1.591478696741854,1.6165413533834583,1.6416040100250626,1.666666666666666,1.6917293233082704,1.7167919799498748,1.7418546365914782,1.7669172932330826,1.791979949874686,1.8170426065162903,1.8421052631578947,1.8671679197994981,1.8922305764411025,1.917293233082706,1.9423558897243103,1.9674185463659146,1.992481203007518,2.0175438596491224,2.0426065162907268,2.06766917293233,2.0927318295739346,2.117794486215539,2.1428571428571423,2.1679197994987467,2.19298245614035,2.2180451127819545,2.243107769423559,2.2681704260651623,2.2932330827067666,2.31829573934837,2.3433583959899744,2.3684210526315788,2.393483709273182,2.4185463659147866,2.443609022556391,2.4686716791979944,2.4937343358395987,2.518796992481203,2.5438596491228065,2.568922305764411,2.5939849624060143,2.6190476190476186,2.644110275689223,2.6691729323308264,2.6942355889724308,2.719298245614034,2.7443609022556386,2.769423558897243,2.7944862155388464,2.8195488721804507,2.844611528822055,2.8696741854636585,2.894736842105263,2.919799498746867,2.9448621553884706,2.969924812030075,2.9949874686716784,3.0200501253132828,3.045112781954886,3.0701754385964914,3.095238095238095,3.1203007518796984,3.1453634085213036,3.170426065162907,3.1954887218045105,3.220551378446114,3.245614035087719,3.2706766917293226,3.295739348370926,3.3208020050125313,3.3458646616541348,3.370927318295738,3.3959899749373434,3.421052631578947,3.4461152882205504,3.4711779448621556,3.496240601503759,3.5213032581453625,3.5463659147869677,3.571428571428571,3.5964912280701746,3.62155388471178,3.6466165413533833,3.6716791979949868,3.69674185463659,3.7218045112781954,3.746867167919799,3.7719298245614024,3.7969924812030076,3.822055137844611,3.8471177944862145,3.8721804511278197,3.897243107769423,3.9223057644110266,3.947368421052632,3.9724310776942353,3.9974937343358388,4.022556390977442,4.0476190476190474,4.072681704260651,4.097744360902254,4.12280701754386,4.147869674185463,4.1729323308270665,4.197994987468672,4.223057644110275,4.248120300751879,4.273182957393484,4.298245614035087,4.323308270676691,4.348370927318296,4.3734335839598995,4.398496240601503,4.423558897243108,4.448621553884712,4.473684210526315,4.4987468671679185,4.523809523809524,4.548872180451127,4.573934837092731,4.598997493734336,4.624060150375939,4.649122807017543,4.674185463659148,4.6992481203007515,4.724310776942355,4.74937343358396,4.774436090225564,4.799498746867167,4.8245614035087705,4.849624060150376,4.874686716791979,4.899749373433583,4.924812030075188,4.949874686716791,4.974937343358395,5.0],"type":"scatter"},{"mode":"lines","name":"Tanh","x":[-5.0,-4.974937343358396,-4.949874686716792,-4.924812030075188,-4.899749373433584,-4.87468671679198,-4.849624060150376,-4.824561403508772,-4.799498746867168,-4.774436090225564,-4.74937343358396,-4.724310776942356,-4.6992481203007515,-4.674185463659148,-4.649122807017544,-4.62406015037594,-4.598997493734336,-4.5739348370927315,-4.548872180451128,-4.523809523809524,-4.49874686716792,-4.473684210526316,-4.448621553884712,-4.423558897243108,-4.398496240601504,-4.3734335839598995,-4.348370927318296,-4.323308270676692,-4.298245614035087,-4.273182957393484,-4.2481203007518795,-4.223057644110276,-4.197994987468672,-4.172932330827067,-4.147869674185464,-4.12280701754386,-4.097744360902256,-4.072681704260652,-4.0476190476190474,-4.022556390977444,-3.9974937343358397,-3.9724310776942358,-3.947368421052632,-3.9223057644110275,-3.8972431077694236,-3.8721804511278197,-3.8471177944862154,-3.8220551378446115,-3.7969924812030076,-3.7719298245614037,-3.7468671679198,-3.7218045112781954,-3.6967418546365916,-3.6716791979949877,-3.6466165413533833,-3.6215538847117794,-3.5964912280701755,-3.5714285714285716,-3.5463659147869677,-3.5213032581453634,-3.4962406015037595,-3.4711779448621556,-3.4461152882205512,-3.4210526315789473,-3.3959899749373434,-3.3709273182957395,-3.3458646616541357,-3.3208020050125313,-3.2957393483709274,-3.2706766917293235,-3.245614035087719,-3.2205513784461153,-3.1954887218045114,-3.1704260651629075,-3.1453634085213036,-3.1203007518796992,-3.0952380952380953,-3.0701754385964914,-3.045112781954887,-3.020050125313283,-2.9949874686716793,-2.9699248120300754,-2.9448621553884715,-2.919799498746867,-2.8947368421052633,-2.8696741854636594,-2.844611528822055,-2.819548872180451,-2.7944862155388472,-2.7694235588972433,-2.7443609022556394,-2.719298245614035,-2.694235588972431,-2.6691729323308273,-2.644110275689223,-2.619047619047619,-2.593984962406015,-2.5689223057644113,-2.5438596491228074,-2.518796992481203,-2.493734335839599,-2.4686716791979952,-2.443609022556391,-2.418546365914787,-2.393483709273183,-2.368421052631579,-2.3433583959899753,-2.318295739348371,-2.293233082706767,-2.268170426065163,-2.243107769423559,-2.218045112781955,-2.192982456140351,-2.167919799498747,-2.1428571428571432,-2.117794486215539,-2.092731829573935,-2.067669172932331,-2.0426065162907268,-2.017543859649123,-1.992481203007519,-1.967418546365915,-1.9423558897243112,-1.9172932330827068,-1.892230576441103,-1.867167919799499,-1.8421052631578947,-1.8170426065162908,-1.791979949874687,-1.766917293233083,-1.741854636591479,-1.7167919799498748,-1.6917293233082709,-1.666666666666667,-1.6416040100250626,-1.6165413533834587,-1.5914786967418548,-1.566416040100251,-1.541353383458647,-1.5162907268170427,-1.4912280701754388,-1.466165413533835,-1.4411027568922306,-1.4160401002506267,-1.3909774436090228,-1.3659147869674189,-1.340852130325815,-1.3157894736842106,-1.2907268170426067,-1.2656641604010028,-1.2406015037593985,-1.2155388471177946,-1.1904761904761907,-1.1654135338345868,-1.140350877192983,-1.1152882205513786,-1.0902255639097747,-1.0651629072681708,-1.0401002506265664,-1.0150375939849625,-0.9899749373433586,-0.9649122807017543,-0.9398496240601508,-0.9147869674185465,-0.889724310776943,-0.8646616541353387,-0.8395989974937343,-0.8145363408521309,-0.7894736842105265,-0.7644110275689222,-0.7393483709273188,-0.7142857142857144,-0.6892230576441101,-0.6641604010025066,-0.6390977443609023,-0.6140350877192988,-0.5889724310776945,-0.5639097744360901,-0.5388471177944867,-0.5137844611528823,-0.4887218045112789,-0.46365914786967455,-0.4385964912280702,-0.41353383458646675,-0.3884711779448624,-0.36340852130325807,-0.3383458646616546,-0.3132832080200503,-0.28822055137844593,-0.2631578947368425,-0.23809523809523814,-0.21303258145363468,-0.18796992481203034,-0.162907268170426,-0.13784461152882255,-0.1127819548872182,-0.08771929824561475,-0.06265664160401041,-0.03759398496240607,-0.012531328320802615,0.012531328320801727,0.03759398496240607,0.06265664160400952,0.08771929824561386,0.1127819548872182,0.13784461152882166,0.162907268170426,0.18796992481202945,0.2130325814536338,0.23809523809523814,0.2631578947368416,0.28822055137844593,0.3132832080200494,0.33834586466165373,0.36340852130325807,0.3884711779448615,0.41353383458646586,0.4385964912280702,0.46365914786967366,0.488721804511278,0.5137844611528823,0.5388471177944858,0.5639097744360901,0.5889724310776936,0.6140350877192979,0.6390977443609023,0.6641604010025057,0.6892230576441101,0.7142857142857135,0.7393483709273179,0.7644110275689222,0.7894736842105257,0.81453634085213,0.8395989974937343,0.8646616541353378,0.8897243107769421,0.9147869674185465,0.9398496240601499,0.9649122807017543,0.9899749373433577,1.015037593984962,1.0401002506265664,1.0651629072681699,1.0902255639097742,1.1152882205513777,1.140350877192982,1.1654135338345863,1.1904761904761898,1.2155388471177941,1.2406015037593985,1.265664160401002,1.2907268170426063,1.3157894736842106,1.340852130325814,1.3659147869674184,1.3909774436090219,1.4160401002506262,1.4411027568922306,1.466165413533834,1.4912280701754383,1.5162907268170418,1.5413533834586461,1.5664160401002505,1.591478696741854,1.6165413533834583,1.6416040100250626,1.666666666666666,1.6917293233082704,1.7167919799498748,1.7418546365914782,1.7669172932330826,1.791979949874686,1.8170426065162903,1.8421052631578947,1.8671679197994981,1.8922305764411025,1.917293233082706,1.9423558897243103,1.9674185463659146,1.992481203007518,2.0175438596491224,2.0426065162907268,2.06766917293233,2.0927318295739346,2.117794486215539,2.1428571428571423,2.1679197994987467,2.19298245614035,2.2180451127819545,2.243107769423559,2.2681704260651623,2.2932330827067666,2.31829573934837,2.3433583959899744,2.3684210526315788,2.393483709273182,2.4185463659147866,2.443609022556391,2.4686716791979944,2.4937343358395987,2.518796992481203,2.5438596491228065,2.568922305764411,2.5939849624060143,2.6190476190476186,2.644110275689223,2.6691729323308264,2.6942355889724308,2.719298245614034,2.7443609022556386,2.769423558897243,2.7944862155388464,2.8195488721804507,2.844611528822055,2.8696741854636585,2.894736842105263,2.919799498746867,2.9448621553884706,2.969924812030075,2.9949874686716784,3.0200501253132828,3.045112781954886,3.0701754385964914,3.095238095238095,3.1203007518796984,3.1453634085213036,3.170426065162907,3.1954887218045105,3.220551378446114,3.245614035087719,3.2706766917293226,3.295739348370926,3.3208020050125313,3.3458646616541348,3.370927318295738,3.3959899749373434,3.421052631578947,3.4461152882205504,3.4711779448621556,3.496240601503759,3.5213032581453625,3.5463659147869677,3.571428571428571,3.5964912280701746,3.62155388471178,3.6466165413533833,3.6716791979949868,3.69674185463659,3.7218045112781954,3.746867167919799,3.7719298245614024,3.7969924812030076,3.822055137844611,3.8471177944862145,3.8721804511278197,3.897243107769423,3.9223057644110266,3.947368421052632,3.9724310776942353,3.9974937343358388,4.022556390977442,4.0476190476190474,4.072681704260651,4.097744360902254,4.12280701754386,4.147869674185463,4.1729323308270665,4.197994987468672,4.223057644110275,4.248120300751879,4.273182957393484,4.298245614035087,4.323308270676691,4.348370927318296,4.3734335839598995,4.398496240601503,4.423558897243108,4.448621553884712,4.473684210526315,4.4987468671679185,4.523809523809524,4.548872180451127,4.573934837092731,4.598997493734336,4.624060150375939,4.649122807017543,4.674185463659148,4.6992481203007515,4.724310776942355,4.74937343358396,4.774436090225564,4.799498746867167,4.8245614035087705,4.849624060150376,4.874686716791979,4.899749373433583,4.924812030075188,4.949874686716791,4.974937343358395,5.0],"y":[-0.9999092042625951,-0.9999045373263423,-0.9998996305197883,-0.9998944715153939,-0.9998890473522115,-0.9998833444033518,-0.9998773483417841,-0.9998710441043804,-0.9998644158541149,-0.9998574469403246,-0.9998501198569318,-0.9998424161985231,-0.9998343166141768,-0.9998258007589227,-0.9998168472427124,-0.9998074335767748,-0.9997975361172214,-0.9997871300057618,-0.9997761891073809,-0.9997646859448236,-0.9997525916297244,-0.9997398757902081,-0.9997265064947849,-0.9997124501723494,-0.9996976715280835,-0.9996821334550579,-0.9996657969413094,-0.999648620972167,-0.9996305624275826,-0.9996115759742135,-0.9995916139519898,-0.9995706262548869,-0.9995485602056082,-0.9995253604238691,-0.9995009686879595,-0.9994753237892423,-0.9994483613792318,-0.9994200138088754,-0.9993902099596468,-0.9993588750660347,-0.9993259305289952,-0.999291293719911,-0.999254877774581,-0.9992165913767376,-0.9991763385305674,-0.9991340183216821,-0.9990895246659622,-0.9990427460456668,-0.9989935652321724,-0.9989418589946758,-0.9988874977941592,-0.9988303454618885,-0.9987702588616745,-0.9987070875350931,-0.9986406733288249,-0.9985708500032279,-0.9984974428212235,-0.9984202681165271,-0.9983391328402118,-0.9982538340845482,-0.9981641585830109,-0.998069882185297,-0.9979707693061433,-0.9978665723466811,-0.9977570310870052,-0.9976418720485791,-0.9975208078250365,-0.9973935363798765,-0.9972597403094857,-0.9971190860698556,-0.9969712231652907,-0.9968157832973388,-0.9966523794720956,-0.9964806050639681,-0.9963000328339014,-0.9961102138999985,-0.995910676658381,-0.9957009256520644,-0.9954804403855342,-0.9952486740826332,-0.9950050523852831,-0.9947489719904884,-0.9944797992229802,-0.9941968685407854,-0.9938994809709213,-0.9935869024723402,-0.9932583622231747,-0.9929130508292613,-0.9925501184508541,-0.9921686728443769,-0.991767777316011,-0.9913464485838605,-0.9909036545454089,-0.9904383119469413,-0.9899492839516016,-0.9894353776027437,-0.9888953411792566,-0.9883278614395746,-0.9877315607511391,-0.9871049941021544,-0.9864466459925932,-0.9857549272015366,-0.9850281714281066,-0.9842646318034644,-0.983462477271589,-0.9826197888368629,-0.9817345556768361,-0.9808046711189558,-0.9798279284805251,-0.9788020167717046,-0.9777245162619942,-0.9765928939113506,-0.9754044986679044,-0.97415655663515,-0.9728461661125113,-0.9714702925143321,-0.9700257631736225,-0.9685092620383182,-0.9669173242693933,-0.965246330751913,-0.9634925025320469,-0.9616518951951791,-0.9597203932025843,-0.9576937042066789,-0.955567353367634,-0.9533366776971537,-0.9509968204584933,-0.948542725655333,-0.9459691326459295,-0.9432705709230678,-0.9404413551047165,-0.9374755801849691,-0.9343671170998189,-0.9311096086675777,-0.9276964659692857,-0.9241208652402677,-0.9203757453500478,-0.9164538059541101,-0.9123475064074584,-0.9080490655365332,-0.9035504623727352,-0.8988434379575184,-0.8939194983356678,-0.8887699188598711,-0.8833857499359293,-0.8777578243437878,-0.8718767662748752,-0.8657330022308358,-0.8593167739324635,-0.8526181533902703,-0.845627060289448,-0.8383332818417478,-0.830726495254754,-0.8227962929649019,-0.8145322107740737,-0.8059237590204151,-0.7969604569018495,-0.7876318700553027,-0.7779276514756244,-0.7678375858352923,-0.7573516372389905,-0.7464600004158386,-0.7351531553162534,-0.7234219250400582,-0.7112575369775177,-0.698651686995507,-0.6855966064472935,-0.6720851317266452,-0.658110776025736,-0.6436678028921661,-0.6287513011141324,-0.6133572603953826,-0.5974826472141539,-0.5811254801941418,-0.5642849042521919,-0.546961262728416,-0.5291561666515618,-0.510872560247601,-0.492114781764419,-0.4728886186621866,-0.4532013562092128,-0.43306181852851844,-0.41248040116261947,-0.39146909426416127,-0.3700414955791549,-0.3482128124680267,-0.32599985230754686,-0.3034210007335052,-0.2804961873186603,-0.2572468384313463,-0.23369581718506166,-0.20986735056553749,-0.1857869440059269,-0.1614812838692466,-0.13697812848582802,-0.11230618857816624,-0.08749499808169783,-0.06257477653334056,-0.03757628434603163,-0.012530672413007227,0.012530672413006339,0.03757628434603163,0.06257477653333969,0.08749499808169695,0.11230618857816624,0.13697812848582713,0.1614812838692466,0.18578694400592605,0.20986735056553663,0.23369581718506166,0.2572468384313455,0.2804961873186603,0.30342100073350436,0.325999852307546,0.3482128124680267,0.3700414955791541,0.39146909426416054,0.41248040116261947,0.4330618185285177,0.4532013562092121,0.4728886186621866,0.4921147817644183,0.510872560247601,0.5291561666515613,0.5469612627284154,0.5642849042521919,0.5811254801941412,0.5974826472141539,0.6133572603953821,0.6287513011141318,0.6436678028921661,0.6581107760257355,0.6720851317266446,0.6855966064472935,0.6986516869955064,0.7112575369775173,0.7234219250400582,0.735153155316253,0.7464600004158386,0.7573516372389901,0.767837585835292,0.7779276514756244,0.7876318700553023,0.7969604569018492,0.8059237590204148,0.8145322107740735,0.8227962929649018,0.8307264952547537,0.8383332818417477,0.845627060289448,0.85261815339027,0.8593167739324634,0.8657330022308358,0.871876766274875,0.8777578243437877,0.8833857499359291,0.888769918859871,0.8939194983356678,0.8988434379575182,0.9035504623727351,0.9080490655365331,0.9123475064074583,0.9164538059541101,0.9203757453500477,0.9241208652402676,0.9276964659692857,0.9311096086675776,0.9343671170998188,0.9374755801849691,0.9404413551047164,0.9432705709230678,0.9459691326459295,0.948542725655333,0.9509968204584933,0.9533366776971536,0.955567353367634,0.9576937042066788,0.9597203932025842,0.961651895195179,0.9634925025320468,0.9652463307519129,0.9669173242693933,0.9685092620383182,0.9700257631736225,0.9714702925143321,0.9728461661125113,0.97415655663515,0.9754044986679044,0.9765928939113505,0.9777245162619942,0.9788020167717046,0.9798279284805251,0.9808046711189556,0.9817345556768361,0.9826197888368629,0.9834624772715889,0.9842646318034644,0.9850281714281066,0.9857549272015366,0.9864466459925932,0.9871049941021544,0.9877315607511391,0.9883278614395746,0.9888953411792565,0.9894353776027437,0.9899492839516016,0.9904383119469413,0.9909036545454089,0.9913464485838605,0.991767777316011,0.9921686728443769,0.9925501184508541,0.9929130508292613,0.9932583622231747,0.9935869024723402,0.9938994809709213,0.9941968685407854,0.9944797992229802,0.9947489719904884,0.9950050523852831,0.9952486740826332,0.9954804403855342,0.9957009256520644,0.995910676658381,0.9961102138999985,0.9963000328339014,0.9964806050639681,0.9966523794720955,0.9968157832973388,0.9969712231652907,0.9971190860698556,0.9972597403094857,0.9973935363798765,0.9975208078250365,0.9976418720485791,0.9977570310870052,0.9978665723466811,0.9979707693061433,0.998069882185297,0.9981641585830109,0.9982538340845482,0.9983391328402118,0.9984202681165271,0.9984974428212235,0.9985708500032279,0.9986406733288249,0.9987070875350931,0.9987702588616745,0.9988303454618885,0.9988874977941592,0.9989418589946758,0.9989935652321724,0.9990427460456668,0.9990895246659622,0.9991340183216821,0.9991763385305674,0.9992165913767376,0.999254877774581,0.999291293719911,0.9993259305289952,0.9993588750660347,0.9993902099596468,0.9994200138088754,0.9994483613792318,0.9994753237892423,0.9995009686879595,0.9995253604238691,0.9995485602056082,0.9995706262548869,0.9995916139519898,0.9996115759742135,0.9996305624275826,0.999648620972167,0.9996657969413094,0.9996821334550579,0.9996976715280835,0.9997124501723494,0.9997265064947849,0.9997398757902081,0.9997525916297244,0.9997646859448236,0.9997761891073809,0.9997871300057618,0.9997975361172214,0.9998074335767748,0.9998168472427124,0.9998258007589227,0.9998343166141768,0.9998424161985231,0.9998501198569318,0.9998574469403246,0.9998644158541149,0.9998710441043804,0.9998773483417841,0.9998833444033518,0.9998890473522115,0.9998944715153939,0.9998996305197883,0.9999045373263423,0.9999092042625951],"type":"scatter"},{"mode":"lines","name":"Leaky ReLU","x":[-5.0,-4.974937343358396,-4.949874686716792,-4.924812030075188,-4.899749373433584,-4.87468671679198,-4.849624060150376,-4.824561403508772,-4.799498746867168,-4.774436090225564,-4.74937343358396,-4.724310776942356,-4.6992481203007515,-4.674185463659148,-4.649122807017544,-4.62406015037594,-4.598997493734336,-4.5739348370927315,-4.548872180451128,-4.523809523809524,-4.49874686716792,-4.473684210526316,-4.448621553884712,-4.423558897243108,-4.398496240601504,-4.3734335839598995,-4.348370927318296,-4.323308270676692,-4.298245614035087,-4.273182957393484,-4.2481203007518795,-4.223057644110276,-4.197994987468672,-4.172932330827067,-4.147869674185464,-4.12280701754386,-4.097744360902256,-4.072681704260652,-4.0476190476190474,-4.022556390977444,-3.9974937343358397,-3.9724310776942358,-3.947368421052632,-3.9223057644110275,-3.8972431077694236,-3.8721804511278197,-3.8471177944862154,-3.8220551378446115,-3.7969924812030076,-3.7719298245614037,-3.7468671679198,-3.7218045112781954,-3.6967418546365916,-3.6716791979949877,-3.6466165413533833,-3.6215538847117794,-3.5964912280701755,-3.5714285714285716,-3.5463659147869677,-3.5213032581453634,-3.4962406015037595,-3.4711779448621556,-3.4461152882205512,-3.4210526315789473,-3.3959899749373434,-3.3709273182957395,-3.3458646616541357,-3.3208020050125313,-3.2957393483709274,-3.2706766917293235,-3.245614035087719,-3.2205513784461153,-3.1954887218045114,-3.1704260651629075,-3.1453634085213036,-3.1203007518796992,-3.0952380952380953,-3.0701754385964914,-3.045112781954887,-3.020050125313283,-2.9949874686716793,-2.9699248120300754,-2.9448621553884715,-2.919799498746867,-2.8947368421052633,-2.8696741854636594,-2.844611528822055,-2.819548872180451,-2.7944862155388472,-2.7694235588972433,-2.7443609022556394,-2.719298245614035,-2.694235588972431,-2.6691729323308273,-2.644110275689223,-2.619047619047619,-2.593984962406015,-2.5689223057644113,-2.5438596491228074,-2.518796992481203,-2.493734335839599,-2.4686716791979952,-2.443609022556391,-2.418546365914787,-2.393483709273183,-2.368421052631579,-2.3433583959899753,-2.318295739348371,-2.293233082706767,-2.268170426065163,-2.243107769423559,-2.218045112781955,-2.192982456140351,-2.167919799498747,-2.1428571428571432,-2.117794486215539,-2.092731829573935,-2.067669172932331,-2.0426065162907268,-2.017543859649123,-1.992481203007519,-1.967418546365915,-1.9423558897243112,-1.9172932330827068,-1.892230576441103,-1.867167919799499,-1.8421052631578947,-1.8170426065162908,-1.791979949874687,-1.766917293233083,-1.741854636591479,-1.7167919799498748,-1.6917293233082709,-1.666666666666667,-1.6416040100250626,-1.6165413533834587,-1.5914786967418548,-1.566416040100251,-1.541353383458647,-1.5162907268170427,-1.4912280701754388,-1.466165413533835,-1.4411027568922306,-1.4160401002506267,-1.3909774436090228,-1.3659147869674189,-1.340852130325815,-1.3157894736842106,-1.2907268170426067,-1.2656641604010028,-1.2406015037593985,-1.2155388471177946,-1.1904761904761907,-1.1654135338345868,-1.140350877192983,-1.1152882205513786,-1.0902255639097747,-1.0651629072681708,-1.0401002506265664,-1.0150375939849625,-0.9899749373433586,-0.9649122807017543,-0.9398496240601508,-0.9147869674185465,-0.889724310776943,-0.8646616541353387,-0.8395989974937343,-0.8145363408521309,-0.7894736842105265,-0.7644110275689222,-0.7393483709273188,-0.7142857142857144,-0.6892230576441101,-0.6641604010025066,-0.6390977443609023,-0.6140350877192988,-0.5889724310776945,-0.5639097744360901,-0.5388471177944867,-0.5137844611528823,-0.4887218045112789,-0.46365914786967455,-0.4385964912280702,-0.41353383458646675,-0.3884711779448624,-0.36340852130325807,-0.3383458646616546,-0.3132832080200503,-0.28822055137844593,-0.2631578947368425,-0.23809523809523814,-0.21303258145363468,-0.18796992481203034,-0.162907268170426,-0.13784461152882255,-0.1127819548872182,-0.08771929824561475,-0.06265664160401041,-0.03759398496240607,-0.012531328320802615,0.012531328320801727,0.03759398496240607,0.06265664160400952,0.08771929824561386,0.1127819548872182,0.13784461152882166,0.162907268170426,0.18796992481202945,0.2130325814536338,0.23809523809523814,0.2631578947368416,0.28822055137844593,0.3132832080200494,0.33834586466165373,0.36340852130325807,0.3884711779448615,0.41353383458646586,0.4385964912280702,0.46365914786967366,0.488721804511278,0.5137844611528823,0.5388471177944858,0.5639097744360901,0.5889724310776936,0.6140350877192979,0.6390977443609023,0.6641604010025057,0.6892230576441101,0.7142857142857135,0.7393483709273179,0.7644110275689222,0.7894736842105257,0.81453634085213,0.8395989974937343,0.8646616541353378,0.8897243107769421,0.9147869674185465,0.9398496240601499,0.9649122807017543,0.9899749373433577,1.015037593984962,1.0401002506265664,1.0651629072681699,1.0902255639097742,1.1152882205513777,1.140350877192982,1.1654135338345863,1.1904761904761898,1.2155388471177941,1.2406015037593985,1.265664160401002,1.2907268170426063,1.3157894736842106,1.340852130325814,1.3659147869674184,1.3909774436090219,1.4160401002506262,1.4411027568922306,1.466165413533834,1.4912280701754383,1.5162907268170418,1.5413533834586461,1.5664160401002505,1.591478696741854,1.6165413533834583,1.6416040100250626,1.666666666666666,1.6917293233082704,1.7167919799498748,1.7418546365914782,1.7669172932330826,1.791979949874686,1.8170426065162903,1.8421052631578947,1.8671679197994981,1.8922305764411025,1.917293233082706,1.9423558897243103,1.9674185463659146,1.992481203007518,2.0175438596491224,2.0426065162907268,2.06766917293233,2.0927318295739346,2.117794486215539,2.1428571428571423,2.1679197994987467,2.19298245614035,2.2180451127819545,2.243107769423559,2.2681704260651623,2.2932330827067666,2.31829573934837,2.3433583959899744,2.3684210526315788,2.393483709273182,2.4185463659147866,2.443609022556391,2.4686716791979944,2.4937343358395987,2.518796992481203,2.5438596491228065,2.568922305764411,2.5939849624060143,2.6190476190476186,2.644110275689223,2.6691729323308264,2.6942355889724308,2.719298245614034,2.7443609022556386,2.769423558897243,2.7944862155388464,2.8195488721804507,2.844611528822055,2.8696741854636585,2.894736842105263,2.919799498746867,2.9448621553884706,2.969924812030075,2.9949874686716784,3.0200501253132828,3.045112781954886,3.0701754385964914,3.095238095238095,3.1203007518796984,3.1453634085213036,3.170426065162907,3.1954887218045105,3.220551378446114,3.245614035087719,3.2706766917293226,3.295739348370926,3.3208020050125313,3.3458646616541348,3.370927318295738,3.3959899749373434,3.421052631578947,3.4461152882205504,3.4711779448621556,3.496240601503759,3.5213032581453625,3.5463659147869677,3.571428571428571,3.5964912280701746,3.62155388471178,3.6466165413533833,3.6716791979949868,3.69674185463659,3.7218045112781954,3.746867167919799,3.7719298245614024,3.7969924812030076,3.822055137844611,3.8471177944862145,3.8721804511278197,3.897243107769423,3.9223057644110266,3.947368421052632,3.9724310776942353,3.9974937343358388,4.022556390977442,4.0476190476190474,4.072681704260651,4.097744360902254,4.12280701754386,4.147869674185463,4.1729323308270665,4.197994987468672,4.223057644110275,4.248120300751879,4.273182957393484,4.298245614035087,4.323308270676691,4.348370927318296,4.3734335839598995,4.398496240601503,4.423558897243108,4.448621553884712,4.473684210526315,4.4987468671679185,4.523809523809524,4.548872180451127,4.573934837092731,4.598997493734336,4.624060150375939,4.649122807017543,4.674185463659148,4.6992481203007515,4.724310776942355,4.74937343358396,4.774436090225564,4.799498746867167,4.8245614035087705,4.849624060150376,4.874686716791979,4.899749373433583,4.924812030075188,4.949874686716791,4.974937343358395,5.0],"y":[-0.05,-0.04974937343358396,-0.049498746867167924,-0.04924812030075188,-0.048997493734335845,-0.0487468671679198,-0.04849624060150376,-0.048245614035087724,-0.04799498746867168,-0.04774436090225564,-0.0474937343358396,-0.04724310776942356,-0.046992481203007516,-0.04674185463659148,-0.04649122807017544,-0.0462406015037594,-0.04598997493734336,-0.045739348370927316,-0.04548872180451128,-0.04523809523809524,-0.0449874686716792,-0.04473684210526316,-0.044486215538847115,-0.04423558897243108,-0.04398496240601504,-0.043734335839598994,-0.04348370927318296,-0.043233082706766915,-0.04298245614035087,-0.042731829573934836,-0.04248120300751879,-0.042230576441102764,-0.04197994987468672,-0.04172932330827067,-0.04147869674185464,-0.0412280701754386,-0.040977443609022564,-0.04072681704260652,-0.04047619047619048,-0.04022556390977444,-0.0399749373433584,-0.039724310776942356,-0.03947368421052632,-0.03922305764411028,-0.038972431077694235,-0.0387218045112782,-0.038471177944862156,-0.03822055137844611,-0.03796992481203008,-0.037719298245614034,-0.037468671679198,-0.037218045112781956,-0.03696741854636592,-0.03671679197994988,-0.036466165413533834,-0.0362155388471178,-0.035964912280701755,-0.03571428571428572,-0.035463659147869676,-0.035213032581453634,-0.0349624060150376,-0.034711779448621555,-0.03446115288220551,-0.034210526315789476,-0.03395989974937343,-0.0337092731829574,-0.033458646616541354,-0.03320802005012531,-0.032957393483709276,-0.03270676691729323,-0.03245614035087719,-0.032205513784461154,-0.03195488721804511,-0.031704260651629075,-0.03145363408521304,-0.031203007518796993,-0.030952380952380953,-0.030701754385964914,-0.03045112781954887,-0.030200501253132832,-0.029949874686716792,-0.029699248120300753,-0.029448621553884717,-0.02919799498746867,-0.028947368421052635,-0.028696741854636595,-0.028446115288220553,-0.028195488721804513,-0.027944862155388474,-0.027694235588972434,-0.027443609022556395,-0.027192982456140352,-0.026942355889724313,-0.026691729323308273,-0.02644110275689223,-0.02619047619047619,-0.02593984962406015,-0.025689223057644112,-0.025438596491228073,-0.02518796992481203,-0.02493734335839599,-0.02468671679197995,-0.02443609022556391,-0.02418546365914787,-0.023934837092731833,-0.023684210526315794,-0.023433583959899754,-0.02318295739348371,-0.022932330827067672,-0.022681704260651633,-0.02243107769423559,-0.02218045112781955,-0.02192982456140351,-0.02167919799498747,-0.021428571428571432,-0.02117794486215539,-0.02092731829573935,-0.02067669172932331,-0.020426065162907268,-0.02017543859649123,-0.01992481203007519,-0.01967418546365915,-0.019423558897243114,-0.019172932330827067,-0.01892230576441103,-0.018671679197994992,-0.018421052631578946,-0.01817042606516291,-0.01791979949874687,-0.01766917293233083,-0.01741854636591479,-0.01716791979949875,-0.01691729323308271,-0.01666666666666667,-0.016416040100250627,-0.016165413533834588,-0.015914786967418548,-0.01566416040100251,-0.015413533834586471,-0.015162907268170427,-0.014912280701754389,-0.01466165413533835,-0.014411027568922305,-0.014160401002506267,-0.013909774436090228,-0.013659147869674189,-0.013408521303258149,-0.013157894736842106,-0.012907268170426067,-0.01265664160401003,-0.012406015037593985,-0.012155388471177947,-0.011904761904761908,-0.011654135338345868,-0.011403508771929829,-0.011152882205513786,-0.010902255639097747,-0.010651629072681707,-0.010401002506265664,-0.010150375939849625,-0.009899749373433587,-0.009649122807017543,-0.009398496240601508,-0.009147869674185466,-0.00889724310776943,-0.008646616541353387,-0.008395989974937344,-0.00814536340852131,-0.007894736842105265,-0.007644110275689222,-0.007393483709273188,-0.007142857142857144,-0.006892230576441101,-0.0066416040100250665,-0.006390977443609023,-0.0061403508771929885,-0.005889724310776945,-0.005639097744360901,-0.005388471177944867,-0.005137844611528823,-0.004887218045112789,-0.004636591478696745,-0.004385964912280702,-0.004135338345864667,-0.003884711779448624,-0.0036340852130325807,-0.003383458646616546,-0.003132832080200503,-0.0028822055137844595,-0.002631578947368425,-0.0023809523809523816,-0.002130325814536347,-0.0018796992481203035,-0.00162907268170426,-0.0013784461152882255,-0.001127819548872182,-0.0008771929824561475,-0.0006265664160401041,-0.0003759398496240607,-0.00012531328320802615,0.012531328320801727,0.03759398496240607,0.06265664160400952,0.08771929824561386,0.1127819548872182,0.13784461152882166,0.162907268170426,0.18796992481202945,0.2130325814536338,0.23809523809523814,0.2631578947368416,0.28822055137844593,0.3132832080200494,0.33834586466165373,0.36340852130325807,0.3884711779448615,0.41353383458646586,0.4385964912280702,0.46365914786967366,0.488721804511278,0.5137844611528823,0.5388471177944858,0.5639097744360901,0.5889724310776936,0.6140350877192979,0.6390977443609023,0.6641604010025057,0.6892230576441101,0.7142857142857135,0.7393483709273179,0.7644110275689222,0.7894736842105257,0.81453634085213,0.8395989974937343,0.8646616541353378,0.8897243107769421,0.9147869674185465,0.9398496240601499,0.9649122807017543,0.9899749373433577,1.015037593984962,1.0401002506265664,1.0651629072681699,1.0902255639097742,1.1152882205513777,1.140350877192982,1.1654135338345863,1.1904761904761898,1.2155388471177941,1.2406015037593985,1.265664160401002,1.2907268170426063,1.3157894736842106,1.340852130325814,1.3659147869674184,1.3909774436090219,1.4160401002506262,1.4411027568922306,1.466165413533834,1.4912280701754383,1.5162907268170418,1.5413533834586461,1.5664160401002505,1.591478696741854,1.6165413533834583,1.6416040100250626,1.666666666666666,1.6917293233082704,1.7167919799498748,1.7418546365914782,1.7669172932330826,1.791979949874686,1.8170426065162903,1.8421052631578947,1.8671679197994981,1.8922305764411025,1.917293233082706,1.9423558897243103,1.9674185463659146,1.992481203007518,2.0175438596491224,2.0426065162907268,2.06766917293233,2.0927318295739346,2.117794486215539,2.1428571428571423,2.1679197994987467,2.19298245614035,2.2180451127819545,2.243107769423559,2.2681704260651623,2.2932330827067666,2.31829573934837,2.3433583959899744,2.3684210526315788,2.393483709273182,2.4185463659147866,2.443609022556391,2.4686716791979944,2.4937343358395987,2.518796992481203,2.5438596491228065,2.568922305764411,2.5939849624060143,2.6190476190476186,2.644110275689223,2.6691729323308264,2.6942355889724308,2.719298245614034,2.7443609022556386,2.769423558897243,2.7944862155388464,2.8195488721804507,2.844611528822055,2.8696741854636585,2.894736842105263,2.919799498746867,2.9448621553884706,2.969924812030075,2.9949874686716784,3.0200501253132828,3.045112781954886,3.0701754385964914,3.095238095238095,3.1203007518796984,3.1453634085213036,3.170426065162907,3.1954887218045105,3.220551378446114,3.245614035087719,3.2706766917293226,3.295739348370926,3.3208020050125313,3.3458646616541348,3.370927318295738,3.3959899749373434,3.421052631578947,3.4461152882205504,3.4711779448621556,3.496240601503759,3.5213032581453625,3.5463659147869677,3.571428571428571,3.5964912280701746,3.62155388471178,3.6466165413533833,3.6716791979949868,3.69674185463659,3.7218045112781954,3.746867167919799,3.7719298245614024,3.7969924812030076,3.822055137844611,3.8471177944862145,3.8721804511278197,3.897243107769423,3.9223057644110266,3.947368421052632,3.9724310776942353,3.9974937343358388,4.022556390977442,4.0476190476190474,4.072681704260651,4.097744360902254,4.12280701754386,4.147869674185463,4.1729323308270665,4.197994987468672,4.223057644110275,4.248120300751879,4.273182957393484,4.298245614035087,4.323308270676691,4.348370927318296,4.3734335839598995,4.398496240601503,4.423558897243108,4.448621553884712,4.473684210526315,4.4987468671679185,4.523809523809524,4.548872180451127,4.573934837092731,4.598997493734336,4.624060150375939,4.649122807017543,4.674185463659148,4.6992481203007515,4.724310776942355,4.74937343358396,4.774436090225564,4.799498746867167,4.8245614035087705,4.849624060150376,4.874686716791979,4.899749373433583,4.924812030075188,4.949874686716791,4.974937343358395,5.0],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"width":400,"height":300,"title":{"text":"Activation Functions in Neural Networks"},"xaxis":{"title":{"text":"Neuron"}},"yaxis":{"title":{"text":"Output"}},"legend":{"title":{"text":"Activation Functions"}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('d8758cb9-7c27-4678-827a-054d39836bd3');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<h4 id="ex-multiple-logistic-regression">Ex: Multiple Logistic Regression:</h4>
<div class="r-stack">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://github.com/hassothea/hassothea.github.io/blob/master/files/teaching/img/logRM.png?raw=true" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/multi_logit.gif" class="fragment quarto-figure quarto-figure-center" style="width:95.0%"></p>
</figure>
</div>
</div>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-9" class="slide level2" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Model: Multilayer perceptron</h2>
<h3 id="loss-function-true-y-vs-prediction-colorbluehaty">Loss function: true <span class="math inline">\(y\)</span> vs prediction <span class="math inline">\(\color{blue}{\hat{y}}\)</span></h3>
<div style="font-size: 57%">
<div class="columns">
<div class="column" style="width:65%;">
<ul>
<li>Given weights <span class="math inline">\(\color{blue}{W_j}\)</span>’s and biases <span class="math inline">\(\color{blue}{b_j}\)</span>’s of the network, the feedforward network can produce prediction <span class="math inline">\(\hat{y}\)</span>.</li>
<li>To measure how good the network is, we compare the prediction <span class="math inline">\(\color{blue}{\hat{y}}\)</span> to the real target <span class="math inline">\(y\)</span>.</li>
<li><strong>Loss function</strong> quantifies the difference between the predicted output and the actual target.</li>
<li><strong>Regression</strong> losses:
<ul>
<li><span class="math inline">\(\ell_2(y_i,\color{blue}{\hat{y}_i})=(y_i-\color{blue}{\hat{y}_i})^2\)</span>: Squared loss.</li>
<li><span class="math inline">\(\ell_1(y_i,\color{blue}{\hat{y}_i})=|y_i-\color{blue}{\hat{y}_i}|\)</span>: Absolute loss.</li>
<li><span class="math inline">\(\ell_{\text{rel}}(y_i,\color{blue}{\hat{y}_i})=|\frac{y_i-\color{blue}{\hat{y}_i}}{y_i}|\)</span>: Relative loss.</li>
</ul></li>
<li><strong>Classification</strong> losses:
<ul>
<li><span class="math inline">\(\text{CEn}(y_i,\color{blue}{\hat{y}_i})=-\sum_{j=1}^My_{ij}\log(\color{blue}{\hat{y}_{ij}})\)</span>: Cross-Entropy.</li>
<li><span class="math inline">\(\text{Hinge}(y_i,\color{blue}{\hat{y}_i})=\max\{0,1-\sum_{j=1}^My_{ij}\color{blue}{\hat{y}_{ij}}\}\)</span>: Hinge loss.</li>
<li><span class="math inline">\(\text{KL}(y_i,\color{blue}{\hat{y}_i})=\sum_{j=1}^My_{ij}\log(y_{ij}/\color{blue}{\hat{y}_{ij}})\)</span>: Kullback-Leibler (KL) Divergence.</li>
</ul></li>
</ul>
</div><div class="column" style="width:35%;">
<ul>
<li class="fragment"><span class="light-red"><strong>Q1</strong></span>: What are the <span class="light-blue">key parameters</span> of the network? <img data-src="./img/answer.jpg"></li>
<li class="fragment"><span class="green"><strong>A1</strong></span>: All weights <span class="math inline">\(\color{blue}{W_j}\)</span>’s and biases <span class="math inline">\(\color{blue}{b_j}\)</span>’s.</li>
<li class="fragment"><span class="light-red"><strong>Q2</strong></span>: How to find the suitable values of these <span class="light-blue">parameters</span>?</li>
<li class="fragment"><span class="green"><strong>A2</strong></span>: <strong>Loss function</strong> can guide the network to its better and better state! In other words, we can use the <span class="light-red"><strong>loss/mistake</strong></span> to adjust all <span class="light-blue">key parameters</span>, leading to a better state of the network.</li>
</ul>
</div></div>
</div>
</section>
<section id="model-multilayer-perceptron-10" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="feedforward-neural-networks-by-hand">Feedforward Neural Networks By Hand</h3>
<p>👉 <a href="https://colab.research.google.com/drive/1UlLjXnFHUKZbs2jH31eVLe5CuoQd5kXR?usp=sharing" target="_blank">Jupyter notebook: Feedforward NN by hand</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/ffNN.gif" class="quarto-figure quarto-figure-center" height="450"></p>
</figure>
</div>
</section>
<section id="model-multilayer-perceptron-11" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="why-is-it-powerful">Why is it powerful?</h3>
<iframe src="https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/" width="1000" height="450" frameborder="0" marginwidth="0" marginheight="0" style="border: none">
</iframe>
<div class="fragment" style="font-size: 70%">
<ul>
<li><span class="blue">Roughly speaking, it can approximate any <strong>reasonably complex input-output relationship</strong> to any desired <strong>level of precision</strong>! (For more, read <a href="https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/" target="_blank">UAT, Deepmind</a>)</span>.</li>
</ul>
</div>
</section>
<section id="model-multilayer-perceptron-12" class="slide level2">
<h2>Model: Multilayer perceptron</h2>
<h3 id="why-is-it-powerful-1">Why is it powerful?</h3>
<iframe src="https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/" width="1000" height="450" frameborder="0" marginwidth="0" marginheight="0" style="border: none">
</iframe>
<div style="font-size: 70%">
<p>Let’s see what it means: 👉 <a href="https://colab.research.google.com/drive/1oydA7p62mXIfQab5MYgAj2VhHRsPp7Ds?usp=sharing" target="_blank">Jupyter notebook: Universal Approximation Theorem</a>.</p>
</div>
</section>
<section id="backpropagation-gradient-based" class="slide level2">
<h2>Backpropagation: <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/courses/LogisticReg.html#/binary-logistic-regression-14" data-taret="_blank">Gradient-based</a></h2>
<div style="font-size: 70%">
<ul>
<li><a href="https://www.3blue1brown.com/about">Grant Sanderson</a> of <a href="https://www.3blue1brown.com/">3B1B</a> did a really amazing job on this 👇</li>
<li>Source: <a href="https://www.3blue1brown.com/lessons/backpropagation">Backpropagation, 3Blue1Brown</a>.</li>
<li>More readings:
<ul>
<li><a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/courses/LogisticReg.html#/binary-logistic-regression-14" target="_blank">Stochastic Gradient Descent for Logistic Regression, AML course by Sothea HAS</a></li>
<li><a href="https://colah.github.io/posts/2015-08-Backprop/" target="_blank">Graphs: Backpropagation by Colah</a></li>
<li><a href="https://www.cs.columbia.edu/~mcollins/ff2.pdf" target="_blank">Computational Graphs, and Backpropagation by Michael Collins</a>.</li>
</ul></li>
</ul>
</div>
<iframe src="https://www.3blue1brown.com/lessons/backpropagation" width="100%" height="380px" frameborder="0">
</iframe>
</section>
<section id="optimization-in-keras" class="slide level2">
<h2>Optimization in Keras</h2>
<div style="font-size: 75%">
<ul>
<li>We set up optimization method for our existing network as follow:</li>
</ul>
<div id="3e47ab63" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href=""></a><span class="co"># We use Adam optimizer</span></span>
<span id="cb6-2"><a href=""></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam, SGD</span>
<span id="cb6-3"><a href=""></a><span class="co"># Set up optimizer for our model</span></span>
<span id="cb6-4"><a href=""></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>, loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Let’s have a look at your model:</li>
</ul>
<div id="f0513181" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href=""></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_1"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ dense_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)            │       <span style="color: #00af00; text-decoration-color: #00af00">100,480</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)            │        <span style="color: #00af00; text-decoration-color: #00af00">16,512</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">1,290</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">118,282</span> (462.04 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">118,282</span> (462.04 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</div>
</section>
<section id="training-learning-curves" class="slide level2">
<h2>Training &amp; Learning Curves</h2>
<div style="font-size: 70%">
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>A few important hyperparameters:
<ul>
<li><code>batch_size</code>: number of minibatch <span class="math inline">\(b\)</span>.</li>
<li><code>epochs</code>: number of times that the network passes through the entire training dataset.</li>
<li><code>validation_split</code>: a fraction of the training data for validation during model training. We can keep track of the model state during training by measuring the loss on this validation data, especially for preventing overfitting.</li>
</ul></li>
<li>Choosing the right architecture requires experiences and tuning.</li>
<li>In this case, the network yields <span class="light-blue"><strong>Test Accuracy <span class="math inline">\(=\)</span> 0.954</strong></span> (correctly predicted).</li>
<li>Tuning the hyperparameters would push its performance even further.</li>
</ul>
</div><div class="column" style="width:50%;">
<div id="fa220d28" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href=""></a><span class="co"># Training the network</span></span>
<span id="cb8-2"><a href=""></a>history <span class="op">=</span> model.fit(X_train[:<span class="dv">10000</span>,:], train_labels[:<span class="dv">10000</span>], epochs<span class="op">=</span><span class="dv">50</span>, batch_size<span class="op">=</span><span class="dv">64</span>, validation_split<span class="op">=</span><span class="fl">0.1</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-3"><a href=""></a><span class="co"># evaluation</span></span>
<span id="cb8-4"><a href=""></a>loss, accuracy <span class="op">=</span> model.evaluate(X_test, test_labels, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-5"><a href=""></a><span class="co"># Extract loss values </span></span>
<span id="cb8-6"><a href=""></a>train_loss <span class="op">=</span> history.history[<span class="st">'loss'</span>]</span>
<span id="cb8-7"><a href=""></a>val_loss <span class="op">=</span> history.history[<span class="st">'val_loss'</span>] </span>
<span id="cb8-8"><a href=""></a><span class="co"># Plot the learning curves </span></span>
<span id="cb8-9"><a href=""></a>epochs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(train_loss) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb8-10"><a href=""></a>fig1 <span class="op">=</span> go.Figure(go.Scatter(x<span class="op">=</span>epochs, y<span class="op">=</span>train_loss, name<span class="op">=</span><span class="st">"Training loss"</span>))</span>
<span id="cb8-11"><a href=""></a>fig1.add_trace(go.Scatter(x<span class="op">=</span>epochs, y<span class="op">=</span>val_loss, name<span class="op">=</span><span class="st">"Training loss"</span>))</span>
<span id="cb8-12"><a href=""></a>fig1.update_layout(title<span class="op">=</span><span class="st">"Training and Validation Loss"</span>, </span>
<span id="cb8-13"><a href=""></a>                   width<span class="op">=</span><span class="dv">510</span>, height<span class="op">=</span><span class="dv">250</span>,</span>
<span id="cb8-14"><a href=""></a>                   xaxis<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Epoch"</span>, <span class="bu">type</span><span class="op">=</span><span class="st">"log"</span>),</span>
<span id="cb8-15"><a href=""></a>                   yaxis<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Loss"</span>))</span>
<span id="cb8-16"><a href=""></a>fig1.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="3b5bddf1" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>                            <div id="8a266b4e-6588-4862-8e03-85b807e5a940" class="plotly-graph-div" style="height:250px; width:510px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("8a266b4e-6588-4862-8e03-85b807e5a940")) {                    Plotly.newPlot(                        "8a266b4e-6588-4862-8e03-85b807e5a940",                        [{"name":"Training loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],"y":[0.6126126050949097,0.25271502137184143,0.18029044568538666,0.1358240842819214,0.10117750614881516,0.0773884505033493,0.0587075799703598,0.04289601370692253,0.03467782214283943,0.02535671927034855,0.02010512910783291,0.012665526941418648,0.008521420881152153,0.005344196688383818,0.0042855748906731606,0.0030843522399663925,0.0027635935693979263,0.0027832374908030033,0.0018850605702027678,0.0018685885006561875,0.0011636700946837664,0.0009435686515644193,0.0008105469169095159,0.0007126813288778067,0.0006557926535606384,0.0005769400740973651,0.000511402846314013,0.00045084228622727096,0.00040371104842051864,0.0003553800634108484,0.0003239020297769457,0.00029433632153086364,0.00026749118114821613,0.00023994382354430854,0.00021874147932976484,0.0001985759736271575,0.000178189788130112,0.0001612774358363822,0.00014775113959331065,0.00013500943896360695,0.00012221306678839028,0.00011451783211668953,0.00010240398842142895,0.00009183993097394705,0.00008458924276055768,0.00007749994256300852,0.00007011812704149634,0.00006480517913587391,0.000059094501921208575,0.000053618357924278826],"type":"scatter"},{"name":"Validation loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],"y":[0.3067667782306671,0.2502177059650421,0.2144790142774582,0.19920971989631653,0.1766004115343094,0.1801649034023285,0.1746029406785965,0.16466842591762543,0.18278399109840393,0.1753142923116684,0.20862504839897156,0.20665018260478973,0.19183096289634705,0.20608948171138763,0.21156011521816254,0.20298635959625244,0.21847161650657654,0.22689786553382874,0.24120639264583588,0.23031719028949738,0.23347900807857513,0.2381669580936432,0.24468806385993958,0.24715514481067657,0.2472912073135376,0.248091921210289,0.25422221422195435,0.25680020451545715,0.25953176617622375,0.26211923360824585,0.2678789794445038,0.2679581940174103,0.27112191915512085,0.27576664090156555,0.27829572558403015,0.27964654564857483,0.2838960587978363,0.2835957407951355,0.28628644347190857,0.2855916917324066,0.2947652339935303,0.2930474281311035,0.29774829745292664,0.3017541766166687,0.3010528087615967,0.3029247522354126,0.31282320618629456,0.31585386395454407,0.31332042813301086,0.318635493516922],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"title":{"text":"Epoch"},"type":"log"},"title":{"text":"Training and Validation Loss"},"width":510,"height":250,"yaxis":{"title":{"text":"Loss"}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('8a266b4e-6588-4862-8e03-85b807e5a940');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</div></div>
</div>
</section>
<section id="diagnostics-with-learning-curves" class="slide level2">
<h2>Diagnostics with Learning Curves</h2>
<div style="font-size: 70%">
<div class="columns">
<div class="column" style="width:50%;">
<div id="4d7166ef" class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<div>                            <div id="86dc834f-6600-4d62-813b-83c6de8ea123" class="plotly-graph-div" style="height:250px; width:510px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("86dc834f-6600-4d62-813b-83c6de8ea123")) {                    Plotly.newPlot(                        "86dc834f-6600-4d62-813b-83c6de8ea123",                        [{"name":"Training loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],"y":[0.6126126050949097,0.25271502137184143,0.18029044568538666,0.1358240842819214,0.10117750614881516,0.0773884505033493,0.0587075799703598,0.04289601370692253,0.03467782214283943,0.02535671927034855,0.02010512910783291,0.012665526941418648,0.008521420881152153,0.005344196688383818,0.0042855748906731606,0.0030843522399663925,0.0027635935693979263,0.0027832374908030033,0.0018850605702027678,0.0018685885006561875,0.0011636700946837664,0.0009435686515644193,0.0008105469169095159,0.0007126813288778067,0.0006557926535606384,0.0005769400740973651,0.000511402846314013,0.00045084228622727096,0.00040371104842051864,0.0003553800634108484,0.0003239020297769457,0.00029433632153086364,0.00026749118114821613,0.00023994382354430854,0.00021874147932976484,0.0001985759736271575,0.000178189788130112,0.0001612774358363822,0.00014775113959331065,0.00013500943896360695,0.00012221306678839028,0.00011451783211668953,0.00010240398842142895,0.00009183993097394705,0.00008458924276055768,0.00007749994256300852,0.00007011812704149634,0.00006480517913587391,0.000059094501921208575,0.000053618357924278826],"type":"scatter"},{"name":"Validation loss","x":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50],"y":[0.3067667782306671,0.2502177059650421,0.2144790142774582,0.19920971989631653,0.1766004115343094,0.1801649034023285,0.1746029406785965,0.16466842591762543,0.18278399109840393,0.1753142923116684,0.20862504839897156,0.20665018260478973,0.19183096289634705,0.20608948171138763,0.21156011521816254,0.20298635959625244,0.21847161650657654,0.22689786553382874,0.24120639264583588,0.23031719028949738,0.23347900807857513,0.2381669580936432,0.24468806385993958,0.24715514481067657,0.2472912073135376,0.248091921210289,0.25422221422195435,0.25680020451545715,0.25953176617622375,0.26211923360824585,0.2678789794445038,0.2679581940174103,0.27112191915512085,0.27576664090156555,0.27829572558403015,0.27964654564857483,0.2838960587978363,0.2835957407951355,0.28628644347190857,0.2855916917324066,0.2947652339935303,0.2930474281311035,0.29774829745292664,0.3017541766166687,0.3010528087615967,0.3029247522354126,0.31282320618629456,0.31585386395454407,0.31332042813301086,0.318635493516922],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"title":{"text":"Epoch"},"type":"log"},"title":{"text":"Training and Validation Loss"},"width":510,"height":250,"yaxis":{"title":{"text":"Loss"}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('86dc834f-6600-4d62-813b-83c6de8ea123');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<div style="font-size: 85%">
<ul>
<li>The above learning curve can be used to access the state of our model during and after training.
<ul>
<li>The <span class="light-blue"><strong>training loss</strong></span> always decreases as it’s measured using the training data.</li>
<li>The drop of <span class="light-red"><strong>validation loss</strong></span> indicates the generalization capability of the model at that state.</li>
<li>The model starts to overfit the training data when the validation curve starts to increase.</li>
<li>We should stop the training process when we observe this change in validation curve.</li>
</ul></li>
</ul>
</div>
</div><div class="column" style="font-size: 90%">
<ul>
<li>The learning curves can also reveal other aspects of the network and the data including:
<ul>
<li>When the model underfit the data or requires more training epochs</li>
<li>When the learning rate (<span class="math inline">\(\eta\)</span>) is too large</li>
<li>When the model cannot generalize well to validation set</li>
<li>When it converges properly</li>
<li>When the validation data is not representative enough</li>
<li>When the validation data is too easy too predict…</li>
</ul></li>
<li>These are helpful resources for understanding the above properties:
<ul>
<li><a href="https://wandb.ai/mostafaibrahim17/ml-articles/reports/A-Deep-Dive-Into-Learning-Curves-in-Machine-Learning--Vmlldzo0NjA1ODY0" target="_blank">A deep Dive Into Learning Curves in ML, Mostafa Ibrahim</a></li>
<li><a href="https://rstudio-conf-2020.github.io/dl-keras-tf/notebooks/learning-curve-diagnostics.nb.html#:~:text=Learning%20curves%20can%20also%20be,train%20and%20a%20validation%20dataset." target="_blank">Diagnosing Model Performance with Learning Curves</a>.</li>
</ul></li>
</ul>
</div></div>
</div>
</section>
<section id="neural-network-playground" class="slide level2">
<h2><a href="https://playground.tensorflow.org/" target="_blank">Neural Network Playground</a></h2>
<iframe src="https://playground.tensorflow.org/" width="100%" height="630px" frameborder="0">
</iframe>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<div style="font-size:80%">
<div class="columns">
<div class="column" style="width:50%;">
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Pros</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Versatility</strong>: DNNs can be used for a wide range of tasks including classification, regression, and even function approximation.</li>
<li><strong>Non-linear Problem Solving</strong>: They can model complex relationships and capture non-linear patterns in data, thanks to their non-linear activation functions.</li>
<li><strong>Flexibility</strong>: MLPs can have multiple layers and neurons, making them highly adaptable to various problem complexities.</li>
<li><strong>Training Efficiency</strong>: With advancements like backpropagation, training MLPs has become efficient and effective.</li>
<li><strong>Feature Learning</strong>: MLPs can automatically learn features from raw data, reducing the need for manual feature extraction.</li>
</ul>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/neural.png" class="quarto-figure quarto-figure-center" style="position: relative; bottom: 10px" width="100"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="callout callout-caution no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Cons</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>Computational Complexity</strong>: They can be computationally intensive, especially with large datasets and complex architectures, requiring significant processing power and memory.</li>
<li><strong>Overfitting</strong>: MLPs can easily overfit to training data, especially if they have too many parameters relative to the amount of training data.</li>
<li><strong>Black Box Nature</strong>: The internal workings of an MLP are not easily interpretable, making it difficult to understand how specific decisions are made.</li>
<li><strong>Requires Large Datasets</strong>: Effective training of MLPs often requires large amounts of data, which might not always be available.</li>
<li><strong>Hyperparameter Tuning</strong>: MLPs have several hyperparameters (e.g., learning rate, number of hidden layers, number of neurons per layer) that need careful tuning, which can be time-consuming and challenging.</li>
<li><strong>Architecture</strong>: Designing right architecture can be challenging as well.</li>
</ul>
</div>
</div>
</div>
</div></div>
</div>
</section>
<section id="its-party-time" class="slide level2 center" data-background-image="./img/end_page.jpg" data-background-opacity="0.3">
<h2>🥳 It’s party time 🥂</h2>
<p><br> <br> <br> <br> <br> <br> <br> <br></p>
<h4 id="view-party-menu-here-party-menu.">📋 View party menu here: <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/TP_Neural_Network.html" target="_blank">Party Menu</a>.</h4>
<h4 id="download-party-invitation-here-party-invitation-letter.">🫠 Download party invitation here: <a href="https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/TP_Neural_Network.ipynb" target="_blank">Party Invitation Letter</a>.</h4>
</section>
<div class="quarto-auto-generated-content">
<p><img src="./img/AMS_logo.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="Neural_Network_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="Neural_Network_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="Neural_Network_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="Neural_Network_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="Neural_Network_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="Neural_Network_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="Neural_Network_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="Neural_Network_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="Neural_Network_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="Neural_Network_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>