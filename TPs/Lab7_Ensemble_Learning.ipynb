{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab7 - Ensemble Learning**\n",
    "\n",
    "**Course: Advanced Machine Learning** <br>\n",
    "**Lecturer: Dr. Sothea HAS**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**  Ensemble Learning Methods are about combining several base learners to enhance its performance. In this lab, you will apply each ensemble learning method on real datasets and analyze its sensitivity in terms of the key hyperparameters of the method. Moreover, the feature importances will also be computed from each model.\n",
    "\n",
    "\n",
    "> **The `Jupyter Notebook` for this TP can be downloaded here: [TP7_Ensemble_Learning](https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/TP7_Ensemble_Learning.ipynb)**.\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Auto-MPG Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is downloaded from [`UCI Machine Learning Repository`](https://archive.ics.uci.edu/dataset/9/auto+mpg){target='_blank'}. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes, (Quinlan, 1993). \n",
    "\n",
    "Load the dataset from kaggle using the following link: [`Auto-MPG` dataset](https://www.kaggle.com/datasets/uciml/autompg-dataset){target='_blank'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Overview and Univariate Analysis:**\n",
    "\n",
    "- Check the dimension of the dataset and modify columns with wrong data type.\n",
    "- Whatâ€™s wrong with the column `horsepower`? Properly solve it.\n",
    "- Perform univariate analysis aiming at understanding individual columns of the data and dectecting the following problems:\n",
    "    - Outliers\n",
    "    - Duplications\n",
    "    - Missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Bivariate Analysis**\n",
    "\n",
    "- Plot pairplot between quantitative columns. Take note on the most potential predictors for the target `MPG`.\n",
    "- Is `origin` useful for predicting the target `MPG`?\n",
    "- Preprocess the inputs for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Random Forest: OOB vs Cross Validation** \n",
    "\n",
    "- Split the dataset into $80\\%-20\\%$ training-testing data using `random_state = 42`.\n",
    "\n",
    "- Build a random forest model with its default setting. Then compute the free **Out-Of-Bag Errors** or Score obtained by the forest (see `model.oob_score_`). Compute suitable metrics on the test data and store them in a data frame.\n",
    "\n",
    "- Fine-tune the key hyperparameters of the random forest (`max_depth`, `max_features`, `n_estimators`...), then evaluate its CV error or Score. Compare it to the corresponding **OOB** criterion for each combination of the hyperparameters. In order to achieve this, you should follow the following steps:\n",
    "    1. **Initialize:** Ensure you set `oob_score=True` when initializing your `RandomForestRegressor`.\n",
    "    - **OOB Error:** After fitting the model, access the **OOB** score using the `.oob_score_` attribute of the fitted model object. Convert this to an error rate (e.g., 1âˆ’score).\n",
    "    2. **CV Error:** For the same model configuration, calculate the mean cross-validation score using the `cross_val_score` function (or `cross_validate`) with `k=5` folds. Convert the mean score to an error rate.\n",
    "    3. **Visualize:** Plot the **OOB Error** and the **CV Error** on the same graph for each hyperparameter. Analyze the gap between the two lines.\n",
    "\n",
    "- Compare the test metrics of the three models:\n",
    "    - The default random forest (second point).\n",
    "    - The model with the best `OOB` performance.\n",
    "    - The model with the best `CV` performance.\n",
    "\n",
    "- Compote and plot the figure of `mean decrease impurity measure` and `permutation feature importances` of the last two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Extra-trees: OOB vs Cross Validation** \n",
    "\n",
    "- Repeat the previous questions of part **(C)** from the second point using ExtraTrees model from the same module. Compare the result to Random Forest.\n",
    "\n",
    "- Compote and plot the figure of `mean decrease impurity measure` and `permutation feature importances` of the last two Extra-tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E. XGBoost: OOB vs Cross Validation**\n",
    "\n",
    "- Repeat the previous quesions with [`XGboost`](https://xgboost.readthedocs.io/en/stable/python/python_intro.html#install-xgboost). \n",
    "\n",
    "- Compute and draw both feature importances for the last two XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F. GradientCOBRA: Find-tune the hyperparameter $h$**\n",
    "\n",
    "- Build `GradientCOBRA` model by fine-tune the most suitable smoothing hyperparameter $h>0$. \n",
    "- Compute its test performance and compare the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. [Kaggle Cybersecurity Intrusion Detection Dataset](https://www.kaggle.com/datasets/dnkumars/cybersecurity-intrusion-detection-dataset/data){target='_blank'}**\n",
    "\n",
    "This [Kaggle Cybersecurity Intrusion Detection Dataset](https://www.kaggle.com/datasets/dnkumars/cybersecurity-intrusion-detection-dataset/data){target='_blank'} is designed for detecting cyber intrusions based on network traffic and user behavior. You may find the explanation of the dataset in detail, including the dataset structure, feature importance, possible analysis approaches, and how it can be used for machine learning in the the provided link. \n",
    "\n",
    "**Question:** import the dataset, analyze the data and preprocess it properly. Build your best model to detect intrusion activities in the dataset. Report the test performance and features the seem to influence the performance of the model the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "$^{\\text{ðŸ“š}}$ [Bagging predictors, Breiman (1996)](https://link.springer.com/article/10.1007/BF00058655){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [The strength of weak learnability, Robert E. Schapire (1990).](https://link.springer.com/article/10.1007/BF00116037){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [COBRA: A combined regression strategy, Beau et al. (2016)](https://www.sciencedirect.com/science/article/pii/S0047259X15000950){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [Gradient COBRA: A kernel-based consensual aggregation for regression, Has (2023)](https://doi.org/10.52933/jdssv.v3i2.70){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [Aggregation using inputâ€“output trade-off, Fischer & Mougeot (2019)](https://www.sciencedirect.com/science/article/abs/pii/S0378375818302349){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [Super Learner, M. J. Van der Laan (2007)](https://www.degruyter.com/document/doi/10.2202/1544-6115.1309/html){target=\"_blank\"}. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
