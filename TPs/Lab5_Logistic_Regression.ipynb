{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab5: Logistic Regression**\n",
    "\n",
    "**Course**: **M2-DAS: Advanced Machine Learning** <br>\n",
    "**Lecturer**: **Dr. Sothea HAS**\n",
    "\n",
    "-----\n",
    "\n",
    "**Objective:** In this lab, you will learn how to build Binary Logistic Regression model to predict `heart failure` patients. Not only that, you will learn to detect informative features for maximizing the potential of the constructed models. You will also see that **quantitative features** are not always the most important ones in building a good predictive model. You have to treat all types of data carefully.\n",
    "\n",
    "- The `notebook` of this `TP` can be downloaded here: [Lab5_Logistic_Regression.ipynb](https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/Lab5_Logistic_Regression.ipynb){target=\"_blank\"}.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Heart Failure Prediction**\n",
    "\n",
    "We will work with [Kaggle Heart Failure Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction) as introduced in [TP1](https://hassothea.github.io/Advanced-Machine-Learning-ITC/TPs/Lab4_NBC.html){target='_blank'}. You may use the preprocessing step done in the previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"fedesoriano/heart-failure-prediction\")\n",
    "data = pd.read_csv(path + \"/heart.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. Binary Logistic Regression**\n",
    "- Split the data into $80\\%$-training and $20\\%$-testing data.\n",
    "- We start from **feature transformation:**\n",
    "    - Perform one-hot encoding for all the qualitative variables.\n",
    "    - Standardize all the inputs.\n",
    "- Construct 4 Binary Logistic Regression models on the 80%-Training using different options of inputs:\n",
    "    - `lg_quan`: logistic regression using only quantitative inputs.\n",
    "    - `lg_qual`: logistic regression using only qualitative inputs (use one-hot encoding: `pd.get_dummies()`).\n",
    "    - `lg_eda`: logistic regression using your selected inputs.\n",
    "    - `lg_full`: logistic regression using all inputs.\n",
    "- Measure their performance on the corresponding testing data. Compare the results to the result of NBC from the previous **TP**.\n",
    "- Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2. Polynomial Features**\n",
    "\n",
    "Based on the result of EDA, one may try to further elevate the performance of the model using feature engineering or handle problems stored in your problem list. Here, we will try feature engineering.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "- **Quadratic features:** Build a model by introducing quadratic features of some selected variable i.e., $X_1, X_2, X_3\\to X_1^2, X_2^2, X_3^3, X_1X_2, X_1X_3, X_2X_3$. Test its performance on the test data.\n",
    "- **Penalty parameter C:** When more features are created, the model will naturally become too flexible, it's recommended to fine-tune penalty parameter $C$ in this case as well.\n",
    "    - **Random choice:** Try varying parameter $C$, for example, $C=0.01$ as follow `LogisticRegression(C=0.01)`. Fit the model to the training data then test its performance on the testing data. Measure its test performance.\n",
    "    - **Search for the best $C$:** Now, try to search for the best $C$ and report the performance on the test data of the model built with the optimal value of $C$. Measure the performance of the model on the test data.\n",
    "- Compare the performance of all models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Logistic Regression on Email Spam Dataset**\n",
    "\n",
    "The `spam` dataset contains frequency of some common words and its class  ('spam' or 'nonspam'). The following code allows you to import this data into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"https://raw.githubusercontent.com/hassothea/MLcourses/main/data/spam.txt\"\n",
    "data = pd.read_csv(path, sep=\" \")\n",
    "data = data.drop(columns=['Id'])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inspect the dataset to find missing values and proportion of spam and nonspam emails.\n",
    "- Split the data into training and testing parts.\n",
    "- Apply techniques you had done in the previous part to identify email spams.\n",
    "- Evaluate model performance on test data using suitable metrics: accuracy, recall, precision, f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Further Reading**\n",
    "\n",
    "$^{\\text{ðŸ“š}}$  `Pandas` python library: [https://pandas.pydata.org/docs/getting_started/index.html#getting-started](https://pandas.pydata.org/docs/getting_started/index.html#getting-started) <br>\n",
    "$^{\\text{ðŸ“š}}$  `Pandas Cheatsheet`: [https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) <br>\n",
    "$^{\\text{ðŸ“š}}$  `10 Minute to Pandas`: [https://pandas.pydata.org/docs/user_guide/10min.html](https://pandas.pydata.org/docs/user_guide/10min.html) <br>\n",
    "$^{\\text{ðŸ“š}}$  `Some Pandas Lession`: [https://www.kaggle.com/learn/pandas](https://pandas.pydata.org/docs/user_guide/10min.html) <br>\n",
    "$^{\\text{ðŸ“š}}$ [Chapter 4, *Introduction to Statistical Learning with R*, James et al. (2021).](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [Chapter 2, *The Elements of Statistical Learning*, Hastie et al. (2008).](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [Friedman (1989)](http://www.leg.ufpr.br/~eferreira/CE064/Regularized%20Discriminant%20Analysis.pdf){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset){target=\"_blank\"}. <br>\n",
    "$^{\\text{ðŸ“š}}$ [Different Type of Correlation Metrics Used by Data Scientists, Ashray](https://www.analyticsvidhya.com/blog/2021/09/different-type-of-correlation-metrics-used-by-data-scientist/){target=\"_blank\"}. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
