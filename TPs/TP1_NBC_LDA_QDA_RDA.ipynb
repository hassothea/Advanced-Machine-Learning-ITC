{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TP1 - Naive Bayes Classifier / Linear, Quadratic and Regularized Discriminant Analysis**\n",
    "\n",
    "-----\n",
    "<br>\n",
    "\n",
    "<center>Course: Advanced Machine Learning <br>\n",
    "Lecturer: Sothea HAS, PhD\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Objective:</b> This practical session (TP) aims to familiarize you with the key assumptions of each introduced model. The first section is designed to test your understanding of the data and models, while the second section focuses on applying your knowledge to real-world datasets. Please read the instructions carefully and try to complete the tasks independently. If you get stuck, don’t hesitate to ask for help. Good luck!\n",
    "\n",
    "1. Model Assumptions and Data Simulation: It's important to verify that any models should work well on the data that respect their assumptions. \n",
    "- *Data Simulation*: Create datasets that either respect or violate the assumptions of each model, including addressing imbalance problems.\n",
    "- *Model Implementation*: Put the models into action.\n",
    "- *Model Evaluation*: Report the performance of the models using appropriate metrics.\n",
    "\n",
    "2. Real Data Implementation: In real-world problems, things are more complicated because very often the assumptions of the model are often violated. We shall see this by exploring the following real datasets.\n",
    "- *Real Datasets*: you may start with `Spam` dataset to reproduce the experimental results shown in the course, or explore [Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset).\n",
    "- *Preprocessing/Descriptive Analysis*: Understand the features and verify the assumptions (use correlation metrics, for example).\n",
    "- *Implementation*: Apply the models to the datasets.\n",
    "- *Evaluation*: Assess the performance of the models.\n",
    "\n",
    "</div>\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Simulation**\n",
    "- Write a function `simulateClassificationData(n=200, d=2, M = 2, method = \"nbc\")` that returns input-output observations with \n",
    "    - observation size `n` ($200$ by default)\n",
    "    - input $x_i$ are of dimension `d` ($2$ by default)\n",
    "    - the target $y$ contains `M` classes taking values in $\\{1,2,\\dots,M\\}$\n",
    "    - and lastly, `method` defines the prefered method that is supposed to work well on this dataset. It should be an element of [`nbc`, `LDA`, `QDA`, `RDA`] (`nbc` by default).\n",
    "\n",
    "*I set an example below, you can do it differently.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "\n",
    "def simulateClassificationData(n=200, d=2, M=2, method=\"nbc\", class_weights = None):\n",
    "    \"\"\"\n",
    "    Generates a design matrix for classification that works well with Naive Bayes.\n",
    "    \n",
    "    Parameters:\n",
    "    n (int): Number of samples.\n",
    "    d (int): Number of features.\n",
    "    M (int): Number of classes.\n",
    "    method (str): Method name, default is \"nbc\" (Naive Bayes Classification).\n",
    "    class_weights (arr): The proportion of each class. If `None`, the data is balanced.\n",
    "    random_state (int): Random seed for repreoducing the result in random simulation.\n",
    "    \n",
    "    Returns:\n",
    "    X (numpy.ndarray): Feature matrix.\n",
    "    y (numpy.ndarray): Labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the class weight is given. If it's None, it's a balanced case.\n",
    "    if class_weights is None:\n",
    "        class_weights = np.ones(M)/M\n",
    "    idx = np.random.multinomial(1, class_weights, size=n)\n",
    "    n_class = [np.sum(idx[:,i]) for i in range(M)]\n",
    "\n",
    "    # Generate data that prefers NBC model.\n",
    "    if method == \"nbc\":\n",
    "        b = np.random.randint(1,10, 1)\n",
    "        x1 = np.random.poisson(b, size=(n_class[0], d//2))\n",
    "        b = np.random.uniform(-2, 2, 2)\n",
    "        x2 = np.random.normal(np.min(b), np.abs(np.max(b)), size=(n_class[0], d//2))\n",
    "        X = np.column_stack([x1,x2])\n",
    "        for i in range(1,M):\n",
    "            b = np.random.randint(1,10, 1)\n",
    "            x1 = np.random.poisson(b, size=(n_class[i], d//2))\n",
    "            b = np.random.uniform(-2,2, 2)\n",
    "            x2 = np.random.normal(np.min(b), np.abs(np.max(b)), size=(n_class[i], d//2))\n",
    "            X = np.row_stack([X, np.column_stack([x1, x2])])\n",
    "        y = np.repeat([str(i) for i in range(1,M+1)], n_class)\n",
    "\n",
    "    # Data that prefers LDA\n",
    "    elif method == \"lda\":\n",
    "        # generate parameters (means & covariances)\n",
    "        mu = np.random.uniform(-5, 5, size=d)\n",
    "        sigma0 = make_spd_matrix(n_dim=d)\n",
    "        for i in range(M-1):\n",
    "            mu = np.row_stack([mu, np.random.uniform(-5, 5, size=d)])\n",
    "            \n",
    "        # generate observations\n",
    "        X = np.row_stack([np.random.multivariate_normal(mean=mu[i,:], cov=sigma0, size=int(n*class_weights[i])) for i in range(M)])\n",
    "        y = np.concatenate([np.repeat(str(i), int(n*class_weights[i-1])) for i in range(1,M+1)])\n",
    "\n",
    "    # Data that prefers QDA or RDA\n",
    "    elif method in [\"qda\", \"rda\"]:\n",
    "        # generate parameters (means & covariances)\n",
    "        mu = np.random.uniform(-5, 5, size=d)\n",
    "        sigma = [make_spd_matrix(n_dim=d)]\n",
    "        for i in range(M-1):\n",
    "            mu = np.row_stack([mu, np.random.uniform(-5, 5, size=d)])\n",
    "            sigma.append(make_spd_matrix(n_dim=d))\n",
    "        # generate observations\n",
    "        X = np.row_stack([np.random.multivariate_normal(mean=mu[i,:], cov=sigma[i], size=int(n*class_weights[i])) for i in range(M)])\n",
    "        y = np.concatenate([np.repeat(str(i), int(n*class_weights[i-1])) for i in range(1,M+1)])\n",
    "    # If method is other than above, return value error.\n",
    "    else:\n",
    "        ValueError(\"method is either 'nbc', 'lda', 'qda' or 'rda'!\")\n",
    "    # Randomly shuffle the data\n",
    "    id_shuffle = np.random.permutation(range(len(y)))\n",
    "    return X[id_shuffle,:], y[id_shuffle].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### 1.1. Balanced datasets\n",
    "**A.** With $(n,d,M)=(700,2,3)$ fixed and three different options for method in the list `['nbc', 'lda', 'qda']`, use the function above to generate three different datasets: `(X_nbc, y_nbc)`, `(X_lda, y_lda)`, and `(X_qda, y_qda)`, where `X` and `y` are different input-output pairs. An example is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "n, d, M = 700, 2, 3\n",
    "\n",
    "# Set random seed for our data generation\n",
    "np.random.seed(42)\n",
    "X_nbc, y_nbc = simulateClassificationData(n, d, M, method=\"nbc\")\n",
    "X_lda, y_lda = simulateClassificationData(n, d, M, method=\"lda\")\n",
    "X_qda, y_qda = simulateClassificationData(n, d, M, method=\"qda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "**B.** Write the code to visualize the first $500$ observations from each dataset using scatterplots colored according to the classes of the target `y`. The remaining $200$ observations are treated as the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "**C.** For now, work with `(X_nbc, y_nbc)` dataset.\n",
    "\n",
    "**a.** Train `NBC`, `LDA` and `QDA` on the first $500$ observations. Report the *accuracies*, *precision*, *recall* and *f1-scaore* of the three models on the remaining $200$ testing points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Draw the decision boundary with the testing data of the three models on the `(X_nbc, y_nbc)` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**D.** Do the same with the other two datasets. Make some comments on the results: Are the results reasonable? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "### 1.2. Imbalanced datasets\n",
    "\n",
    "Now, we will work with imbalanced simulated datasets. The goal is to identify problems associated with imbalanced datasets and to propose possible solutions we have studied so far.\n",
    "\n",
    "**A.** With the same options as in the balanced case, but adding an additional option of `class_weights = [0.2, 0.15, 0.65]`, generate other three imbalanced datasets. An example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.2, 0.15, 0.65] #[0.2, 0.15, 0.65]\n",
    "np.random.seed(42) \n",
    "X_nbc, y_nbc = simulateClassificationData(n, d, M, method=\"nbc\", class_weights=weights)\n",
    "X_lda, y_lda = simulateClassificationData(n, d, M, method=\"lda\", class_weights=weights)\n",
    "X_qda, y_qda = simulateClassificationData(n, d, M, method=\"qda\", class_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Create the countplots and scatterplots for the three simulated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Compute the performance metrics (recall, precision, f1-score,accuract) on imbalanced `nbc` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** How does this result compare to the balanced case? Does this surprise you?\n",
    "\n",
    "> Your opinion..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "**B.** Do the same (compute these matrices) for the other two imbalanced datasets: `lda` and `qda`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "**C.** In `NBC`, `LDA`, and `QDA`, all parameters (such as means and variances) are directly estimated using data. However, in `RDA`, we can tune the trade-off parameter $\\alpha$ that combines the `QDA` covariance matrices with the `LDA` covariance matrix. This allows for a balance between the flexibility of `QDA` and the simplicity of `LDA`:\n",
    "\n",
    "$$\\hat{\\Sigma}_k(\\alpha)=\\alpha\\hat{\\Sigma}_k+(1-\\alpha)\\hat{\\Sigma}.$$\n",
    "\n",
    "We are now searching for an optimal $\\alpha^*$ that yields better or even the best scores (recall, precision and F$_1$-score). This can be achieved using [Cross-Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#:~:text=Cross%2Dvalidation%20includes%20resampling%20and,model%20will%20perform%20in%20practice.){target=\"_blank\"} technique. The steps we follow are:\n",
    "\n",
    "**a.** First, let's fix $\\alpha_0=0.5$, write a `python` function called `deltaRDA(X, y, X_test, alpha = 0.5, lda_cov = None, qda_cov = None, means = None)` where\n",
    "\n",
    "- `X,y`: input-output training data\n",
    "- `X_test`: an array containing testing inputs $x_i$ for computing $\\delta(x)$.\n",
    "- `alpha`: regularized strength (be default is $0.5$).\n",
    "- `lda_cov`: common covariance in `LDA` ($\\hat{\\Sigma}$). It should be estimated using data `(X,y)` if it's `None`. \n",
    "- `qda_cov`: list of per-class covariances in `QDA` ($\\hat{\\Sigma}_k$). It should be estimated using data.  \n",
    "- `means`: list of per-class means. It should also be estimated if it's `None`. <br>\n",
    "\n",
    "^[This may be the most difficult question.]This function computes the prediction of `X_test` using `RDA` at regularized value $\\alpha_0$. Hint: using the implementation on [slide 37](https://hassothea.github.io/Advanced-Machine-Learning-ITC/courses/Intro_NBC_LDA_QDA.html#/implementation-of-qda){target=\"_blank\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltaRDA(X, y, X_test, alpha = 0.5, lda_cov = None, qda_cov = None, means = None):\n",
    "    # To do\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Compute the confusion matrix and all the performance matrices on the test data using `deltaRDA` at $\\alpha=0.5$ as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark question:** What do you think about the results of `RDA` compared to the previous results from `NBC`, `LDA`, and `QDA`? Remember, we initially chose $\\alpha_0=0.5$. Now, let’s find the optimal value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your response:\n",
    "\n",
    "\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Split your training data into $K$ folds namely: $F_1,F_2,\\dots,F_K$. Let $F_{-k}$ represent all the training data except for the $k$th fold. For any fixed $\\alpha$, write a function `cvRDA(alpha)` that returns the average F$_1$-score over each fold $F_k$ using model built on $F_{-k}$. One way to do this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5          # choose K = 5\n",
    "np.random.seed(42)\n",
    "rand_id = np.random.choice(range(K), replace=True, size=500) # Shuffle index\n",
    "X_train = X_nbc[:500,:]\n",
    "y_train = y_nbc[:500]\n",
    "X_test = X_nbc[500:,:]\n",
    "y_test = y_nbc[500:]\n",
    "\n",
    "def cvRDA(alpha):\n",
    "    res = []\n",
    "    for k in range(K):\n",
    "        y_hat = deltaRDA(X=X_train[rand_id != k,:],\n",
    "                         y=y_train[rand_id != k],\n",
    "                         X_test=X_train[rand_id == k,:],\n",
    "                         alpha=alpha)\n",
    "        res.append(f1_score(y_train[rand_id == k], y_hat, average=\"macro\"))\n",
    "    return np.mean(res).round(3)\n",
    "\n",
    "print(f\"* Cross-validation F1-score for alpha = 0.5: {cvRDA(0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**d.** Choose a grid for $\\alpha$, for example, `G=np.linspace(0,1,100)` which generates uniform grid of $100$ values on interval $[0,1]$. Now, write `Python` code that searches for $\\alpha^*$ on the grid $G$ with largest cross-validation F$_1$-score. Plot the the following graph of $(x,y)=(\\alpha, \\text{CVError}(\\alpha))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.linspace(0,1,100)\n",
    "cvError = [cvRDA(alpha) for alpha in G]\n",
    "\n",
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**e.** Build the `RDA` model with the observed optimal $\\alpha^*$ using all $500$ training data points. Report the performance metrics on the testing data. Conclude your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You conclusion: \n",
    "\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Real datasets**\n",
    "\n",
    "> In this section, you will reproduce the results shown in the course with `Spam` dataset and explore the [Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset). We can download `Spam` data using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>address</th>\n",
       "      <th>all</th>\n",
       "      <th>num3d</th>\n",
       "      <th>our</th>\n",
       "      <th>over</th>\n",
       "      <th>remove</th>\n",
       "      <th>internet</th>\n",
       "      <th>order</th>\n",
       "      <th>mail</th>\n",
       "      <th>...</th>\n",
       "      <th>charSemicolon</th>\n",
       "      <th>charRoundbracket</th>\n",
       "      <th>charSquarebracket</th>\n",
       "      <th>charExclamation</th>\n",
       "      <th>charDollar</th>\n",
       "      <th>charHash</th>\n",
       "      <th>capitalAve</th>\n",
       "      <th>capitalLong</th>\n",
       "      <th>capitalTotal</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.013</td>\n",
       "      <td>7.484</td>\n",
       "      <td>669</td>\n",
       "      <td>1407</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.972</td>\n",
       "      <td>18</td>\n",
       "      <td>110</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3263</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.912</td>\n",
       "      <td>22</td>\n",
       "      <td>568</td>\n",
       "      <td>nonspam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.784</td>\n",
       "      <td>29</td>\n",
       "      <td>1192</td>\n",
       "      <td>nonspam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.064</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      make  address   all  num3d   our  over  remove  internet  order  mail  \\\n",
       "Id                                                                            \n",
       "1361  0.08     0.08  0.76    0.0  0.85  1.02    0.25      0.17   0.59  0.08   \n",
       "520   0.31     0.00  0.00    0.0  0.00  0.00    0.00      0.00   0.00  0.00   \n",
       "3263  0.22     0.00  0.07    0.0  0.07  0.07    0.00      0.14   0.00  0.36   \n",
       "2735  0.04     0.09  0.31    0.0  0.04  0.22    0.04      0.00   0.00  0.58   \n",
       "153   0.67     0.00  0.67    0.0  2.70  0.00    0.00      0.00   0.00  0.00   \n",
       "\n",
       "      ...  charSemicolon  charRoundbracket  charSquarebracket  \\\n",
       "Id    ...                                                       \n",
       "1361  ...          0.000             0.065                0.0   \n",
       "520   ...          0.114             0.000                0.0   \n",
       "3263  ...          0.041             0.031                0.0   \n",
       "2735  ...          0.013             0.224                0.0   \n",
       "153   ...          0.000             0.000                0.0   \n",
       "\n",
       "      charExclamation  charDollar  charHash  capitalAve  capitalLong  \\\n",
       "Id                                                                     \n",
       "1361            0.403       0.117     0.013       7.484          669   \n",
       "520             0.057       0.000     0.000       2.972           18   \n",
       "3263            0.031       0.000     0.000       1.912           22   \n",
       "2735            0.027       0.006     0.000       1.784           29   \n",
       "153             0.200       0.000     0.000       1.064            3   \n",
       "\n",
       "      capitalTotal     type  \n",
       "Id                           \n",
       "1361          1407     spam  \n",
       "520            110     spam  \n",
       "3263           568  nonspam  \n",
       "2735          1192  nonspam  \n",
       "153             33     spam  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path_spam = \"https://raw.githubusercontent.com/hassothea/MLcourses/refs/heads/main/data/spam.txt\"\n",
    "spam = pd.read_csv(path_spam, index_col=0, sep=\" \")\n",
    "spam.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.** Perform descriptive analysis on the datasets: aim for connection between inputs and the target.\n",
    "\n",
    "**B.** Split and built model based on your analysis in the previous step.\n",
    "\n",
    "**C.** Report the results on the testing data.\n",
    "\n",
    "-------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
